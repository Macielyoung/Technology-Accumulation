## 机器学习总结

1. 如何解决过拟合

   **Dropout（随机失活），regularization（正则化），batch normalization（逐层归一化），early stop（提前终止）**

   **Dropout**：在训练神经网络的时候让神经元以超参数p的概率被激活；

   **Regularization**：

   * L2正则。目标函数中增加所有权重w参数的平方和，逼迫所有权重尽可能趋向于0但不为0.

   * L1正则(稀疏规则算子)。目标函数中增加所有权重w参数的绝对值之和，逼迫更多w为0.（变稀疏，L2因为导数也为0，所以速度较慢于L1）。**L1使权值变稀疏，方便提取特征，L2可以防止过拟合，提升模型泛化能力**。L1和L2的正则先验分布分别是拉普拉斯分布和高斯分布。

   * 稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些无用的特征，也就是把这些特征对应的权重置为 0 ；

   **Batch Normalization**：给每层输出都做一个归一化（网络上相当于加一个线性变化层），使下一层输入接近高斯分布，效果十分好；

   **Early Stop**：追求细粒度极小值具有较大的泛化误差，所以许多训练方法提出提前终止策略。典型方法是根据交叉验证提前终止，交叉验证错误率最小时可以认为泛化性能最好，这时即使训练误差仍然在下降，也需要终止继续训练。


2. LR和SVM的联系和区别。

   联系：

   （1）都可以处理分类问题，且一般都用于处理二分类问题，也可拓展到处理多分类；

   （2）都可以增加不同的正则化，如L1，L2正则。

   区别：

   （1）LR是参数模型，SVM是非参数模型；

   （2）目标函数来看，LR是logistical loss，SVM是hinge loss，这两个损失函数的目的都是**增加对分类影响较大的数据点的权重，减小对分类影响较小的数据点的权重**；
   （3）SVM只考虑支持向量，即和分类相关的少数数据点去学习分类器，而LR通过非线性映射，大大增加离分类平面较远点的权重，减小近点权重；
   （4）LR能做的SVM都可以实现，准确率上可能有所差距，而SVM可以做的LR不一定可以实现。

3. GBDT和GXGBoost的区别是什么，讲述Boosting和Bagging之间的差别。

   Bagging和Boosting属于集成学习的两类方法。Bagging是有放回的采样同数量样本训练每个学习器，然后再一起集成（简单投票）；Boosting是使用全部样本（可调权重）依次训练每个学习器，最后迭代集成（平滑加权）。

   Bagging学习器之间不存在强依赖关系，学习器之间可以并行训练，集成方式一般为投票。Random Forest属于Bagging的代表，放回抽样，**每个学习器随机选择部分特征去优化**；

   Boosting方法学习器之间存在强依赖关系，必须串行生成，集成方式为加权和。AdaBoost属于Boosting，采用指数损失函数来代替0/1损失函数。GBDT是Boosting的优秀代表，对函数残差近似值进行拟合，用CART回归树做学习器，集成为回归模型。XGBoost是Boosting的集大成者，对函数残差近似值进行梯度下降，迭代时利用二阶梯度信息，使用二阶泰勒展开系数计算最优分裂，集成模型可分类可回归，XGBoost类似于GBDT的优化版，泛化，性能和扩展性比GBDT好。

4. 谈谈判别模型和生成模型。

   判别模型：右数据直接学习决策函数`Y = f(x)`，或者有条件分布概率`P(Y|X)`作为预测模型；

   生成模型：由数据学习联合概率密度分布函数`P(X,Y)`，然后求出条件分布概率`P(Y|X)`作为预测模型。

   常见的判别模型：KNN，LR，SVM，决策树，感知机，线性判别分析（LDA），传统神经网络，线性回归，Boosting，条件随机场；

   常见的生成模型：朴素贝叶斯，隐马尔可夫模型，高斯混合模型，文档主题生成模型（LDA），RBF

5. 简要说说EM算法。

   有时候因为样本的产生和隐变量（无法观察）有关，而求模型的参数时一般采用最大似然估计，由于含有隐含变量，所以对似然函数参数无法求导，这时使用EM算法来求模型的参数（模型参数可能有多个）。

   EM算法一般分为两步：

   （1）E步：选取一组参数，求出在该参数下隐含变量的条件概率值；

   （2）M步：结合E步求出的隐含变量条件概率，求出似然函数下届函数（本质上是某个期望函数）的最大值。

   重复上两步直至收敛。

   ![1534499410306](https://github.com/Macielyoung/Technology-Accumulation/blob/master/Machine%20Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/pic/EM.png)

   EM算法一个常见例子就是GMM模型，每个样本都有可能由k个高斯分布产生，只是每个高斯产生的概率不同，因此每个样本都有对应的高斯分布（k个中的某一个），此时的隐含变量就是每个样本对应的某个高斯分布。


6. KNN中的K值如何选取。

   KNN中的K值选取对结果会产生重大影响。

   （1）如果选择较小的k值，相当于用较少的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，于此同时“学习”的估计误差会增大，意味整体模型变得复杂，容易发生过拟合；

   （2）如果选择较大的k值，相当于用较多的训练实例进行预测，其优点是可以减小学习的估计误差，但缺点是学习的近似误差会增大。与输入实例较远（不相似）的训练实例也会对学习器作用，使预测发生错误，k值的增大就意味着整体的模型变得简单；

   （3）k=N时，表示无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的类，模型过于简单，忽略了训练实例中大量有用消息。

   在实际应用中，k值一般取一个较小的数值，例如采用**交叉验证法**（即一部分样本做训练集，一部分做测试集）来选择最优的k值。

7. 机器学习中为何要经常对数据做归一化。

   （1）归一化后加快了梯度下降求最优解的速度。不归一化使用梯度下降法寻找最优解时，很有可能走“之字型”路线（垂直等高线），从而导致需要迭代很多次才能收敛

   （2）归一化有可能提高精度。一些分类器需要计算样本之间的距离（如KNN计算欧式距离），如果一个特征值范围非常大，那么距离计算就主要取决于这个特征，从而可能与实际情况相悖（比如可能和值域小的特征更相关）。

   **归一化**：线性归一化，比较适用在数值比较集中的情况；标准差标准化，处理数据符合标准正太分布；非线性归一化，经常用在数据分化比较大的场景，通过一些数学函数，将原始值进行映射。

   概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。

8. LR为什么要对特征进行离散化。

   （1）离散特征的增加和减少都相对容易，易于模型的快速迭代；

   （2）稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；

   （3）离散化后的特征对异常数据有很强的鲁棒性；

   （4）LR属于广义线性模型，表达能力受限，离散化后相当于引入了非线性，每个变量有单独权重，能够提升模型表达能力，加大拟合；

   （5）离散化后可以进行特征交叉，有M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；

   （6）特征离散化后，模型更加稳定；

   （7）特征离散化后，起到了简化LR模型的作用，降低了模型过拟合的风险。

9. 什么是熵。

   熵的定义很简单，用来表示随机变量的不确定性。

   香农引入信息熵，将其定义为离散随机事件的出现概率。一个系统越是有序，其信息熵就越低；反之，越是混乱，则信息熵就越高。所以说，信息熵可以被认为是系统有序化程度的一个度量。

   **为了准确估计随机变量的状态，我们一般习惯性最大熵，认为在所有可能的概率模型（分布）的集合中，熵最大的模型是最好的模型**。

10. Python是如何进行内存管理的。

    从三个方面来看，一对象的引用计数机制，二垃圾回收机制，三内存池机制。

    一、对象的引用计数机制

    Python内部使用引用计数，来保持追踪内存中的对象，所有对象都有引用计数。引用计数增肌有以下几种情况：（1）一个对象分配一个新名称； （2）将其放入一个容器中（如list，dict或元组等）。引用计数减少的情况有以下两种：（1）使用del语句对对象别名显示的销毁； （2）引用超出作用域或被重新赋值。

    二、垃圾回收

      （1）当一个对象的引用计数归零时，它将被垃圾收集机制处理掉；

      （2）当两个对象a和b相互引用时，del语句可以减少a和b的引用计数，并销毁用于引用底层对象的名称。然而由于每个对象都包含一个对其他对象的引用，因此引用计数不会归零，对象也不会销毁。为解决这个问题，解释器会定期执行一个**循环检测器**，检索不可访问对象的玄幻并删除它们。

     三、内存池机制

      Python提供了对内存的垃圾回收机制，但是它将不用的内存放到内存池而不是返回给操作系统。

      （1）Pymalloc机制。为了加速Python的执行效率，Python引入了一个内存池机制，用于管理对小块内存的申请和释放；

      （2）Python中所有小于256个字节的对象都是使用pymalloc实现的分配器，而大的对象则使用系统的malloc。

      （3）对于Python对象，如整数，浮点数和list，都有其独立的私有内存池，对象间不共享他们的内存池。也就是说如果你分配又释放了大量的整数，用于缓存这些整数的内存就不能再分配给浮点数了。

11. 标准化与归一化的区别。

     归一化：（1）把数变为（0,1）之间的小数主要是为了数据处理方便，映射到（0,1）范围内，处理更加便捷快速； （2）把有量纲表达式变为无量纲表达式，归一化是一种简化计算的方式，把有量纲表达式经过变换化为无量纲表达式，成为纯量。

    标准化： 数据的标准化是将数据按比例缩放，使之落入一个小的特定区间。由于信用指示体系的各个指标度量单位不同，为了能够将指标参与评价计算，需要对指标进行规范化处理，通过函数变换将其数值映射到某个数值区间。

12. 随机森林如何处理缺失值。

    （1）对于训练集，同一个类别下的数据，如果分类变量缺失，用众数补上，如果是连续型变量缺失，用中位数补上；

    （2）先用na.roughfix补上缺失值，然后构建森林并计算proximity matrix，再回头看缺失值，如果是分类变量，则用没有缺失的观测实例的proximity中的权重进行投票；如果是连续性变量，则用proximity矩阵进行加权平均的方法补缺失值。然后迭代4-6次，这个缺失值填补的思想和KNN有些类似。

13. 解释对偶的概念。

     一个优化问题可以从两个角度进行考察，一个是primal问题，一个是dual问题，就是对偶问题。一般情况下对偶问题给出主问题最优值的下届，在强对偶性成立的情况下由对偶问题可以得到主问题的最优下届，对偶问题是凸优化问题，可以进行较好的求解，SVM中就是将primal问题转换为dual问题进行求解，从而进一步引入核函数的思想。

14. 如何进行特征选择。

     特征选择是一个重要的数据预处理过程，主要有两个原因：(1)减少特征数量、降维，使模型泛化能力更强，减少过拟合； (2)增强对特征和特征值之间的理解

     常见的特征选择方式：

       (1)去除方差较小的特征；

       (2)正则化。L1正则化能够生成稀疏的模型，L2正则化的表现更加稳定，由于有用的特征往往对应系数非零；

       (3)随机森林。对于分类问题，通常采用Gini不纯度或者信息增益；对于回归问题，通常采用方差或者最小二乘拟合。一般不需要feature engineering、调参等繁琐的步骤。然而它存在两个主要问题：一是重要的特征有可能得分低（关联特征问题），二是这种方法对特征变量类别多的特征越有利（偏向问题）。

       (4)稳定性选择。一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被认为重要特征的次数除以它所有的子集被测试的次数）。理想情况下，重要特征的得分会接近100%，稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近0。

15. 数据预处理。

    （1）填补缺失值。离散：None；连续：均值；缺失值若太多，则去除该列

    （2）有的模型（决策树）需要离散值，特征是连续值则对其离散化。

    （3）对定量特征二值化。核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的为0.

    （4）Pearson相关系数，去除高度相关的列。

16. 如何解决梯度消失和梯度膨胀。

    （1）梯度消失：

    根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话，那么即使这个结果是0.99，经过足够多层传播后，误差对输入层的偏导也会趋向于0，可以采用ReLU激活函数有效的解决梯度消失的情况，也可以用Batch Normalization解决这个问题。

    （2）梯度膨胀：

    根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播后，误差对输入层的偏导会趋于无穷大，可以通过激活函数来解决，也可以用Batch Normalization解决这个问题。

17. 数据不平衡问题及解决办法。

    数据不平衡问题是指数据的分布不均匀，这是由数据集本身引起的。解决方法以下几种：

    * 采样，对小样本加噪声采样，对大样本进行下采样
    * 数据生成，利用已知样本生成新的样本
    * 进行特殊的加权，如在AdaBoost中或者SVM中
    * 采用对不平衡数据集不敏感的算法
    * 改变评价标准，用AUC/ROC来进行评价
    * 采用Bagging、Boosting等集成学习方法
    * 在设计模型到时候考虑数据的先验分布

18. 比较一下EM算法、HMM模型和CRF。

    （1）EM算法。

    EM算法是用于含有隐变量模型的极大似然估计或者极大后验估计，由两步组成：E步，求期望；M步，求极大。本质上EM算法还是一个迭代算法，通过不断用上一代参数对隐变量的估计来对当前变量进行计算，直到收敛。

    **注意**：EM算法对初值敏感，而且EM算法是不断求解下届的极大化逼近求解对数似然函数的极大化算法，也就是说EM算法不能保证找到全局最优值。

    （2）HMM模型

    隐马尔科夫模型是用于标注问题的生成模型。有几个参数：初始状态概率向量π ，状态转移矩阵A，观测概率矩阵B，称为马尔科夫模型的三要素。

    马尔科夫模型三个基本问题：

    - 概率计算问题：给定模型和观测序列，计算模型下观测序列输出的概率。（前向后向算法）
    - 学习问题：已知观测序列，估计模型参数，即用极大似然估计来估计参数。（EM算法和极大似然估计）
    - 预测问题：已知模型和观测序列，求解对应的状态序列。（近似算法[贪心]和维比特算法[动态规划求最优路径]）

    （3）条件随机场CRF

    给定一组输入随机变量的条件下另一组输出随机变量的条件概率分布密度。条件随机场假设输出变量构成马尔科夫随机场，而我们平时看到的大多是线性链随机场，也就是由输入对输出进行预测的判别模型。求解方法为极大似然估计或正则化的极大似然估计。

    HMM和CRF进行比较，它们都用了图的知识，CRF用了马尔科夫随机场（无向图），HMM的基础是贝叶斯网络（有向图）。不过其根本不同在于CRF是判别模型，HMM是生成模型。

19. 优化算法及其优缺点。

    （1）随机梯度下降

    优点：可以一定程度上解决局部最优解的问题

    缺点：收敛速度较慢，选择合适的learning rate比较困难

    （2）批量梯度下降

    优点：容易陷入局部最优解

    缺点：收敛速度较快

    （3）mini_batch梯度下降

    综合了随机梯度下降和批量梯度下降的优缺点，提取的中和方法。

    （4）牛顿法

    求解时需要计算Hessian矩阵，当维度较高的时候，计算Hessian矩阵比较困难。

    （5）拟牛顿法

    为了改进牛顿法在迭代过程中计算Hessian矩阵而提取的算法，采用的方式是逼近Hessian矩阵的方式来求解。

20. RF与GBDT间的区别和联系。

    （1）相同点：都是由多棵树组成，最终的结果都是由多棵树一起决定的。

    （2）不同点：

    * 组成RF的树可以是分类树也可以是回归树，而GBDT只能是回归树
    * 组成RF的树可以并行生成，而GBDT是串行生成
    * RF的结果是多数表决，GBDT是多棵树累加之和
    * RF对异常值不敏感，GBDT对异常值敏感
    * RF是减少模型方差，GBDT是减少模型偏差
    * RF不需要进行特征归一化，GBDT需要进行特征归一化

21. 简述LR的原理与推导。

    参考链接：[LR推导](https://zhuanlan.zhihu.com/p/29691293)

    ![LR](https://github.com/Macielyoung/Technology-Accumulation/blob/master/Machine%20Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/pic/LR.jpeg)

22. 如何理解偏差方差之间的平衡的。

    ![Bias vs Variance](D:\GitProject\Technology-Accumulation\Machine Learning\机器学习总结\pic\bias&variance.png)

    从数学的角度来看，任何模型出现的误差可以分为三个部分。以下是三个部分：

    ![](http://chart.googleapis.com/chart?cht=tx&chl=\Large x=\frac{-b\pm\sqrt{b^2-4ac}}{2a})

23. 

