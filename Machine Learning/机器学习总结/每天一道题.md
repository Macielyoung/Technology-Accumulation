## 机器学习每日一题

**梯度下降法**

梯度下降法的正确步骤是什么？   （D）

a. 计算预测值和真实值之间的误差

b. 重复迭代，直至得到网络权重的最佳值

c. 把输入传入网络，得到输出值

d. 用随机值初始化权重和偏差

e. 对每一个产生误差的神经元，调整相应的权重值以减小误差

A. abcde               B. edcba               C. cbaed                D. dcaeb



**逻辑回归（LR）**

关于逻辑回归和SVM，下面说法错误的是？         （B）

A. Logistic回归可用于预测事件发生概率的大小 

B. Logistic回归的目标函数是最小化后验概率 

C. SVM的目标的结构风险最小化 

D. SVM可以有效避免模型过拟合

解析：Logit回归本质上是一种根据样本对权值进行**极大似然估计**的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率，B错误。A. Logit回归的输出就是样本属于正类别的几率，可以计算出概率。C. SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化. D. SVM可以通过正则化系数控制模型的复杂度，避免过拟合。 



**SVM**

下列选项不是SVM核函数的是                   （B）

A. 多项式核函数

B. logistic核函数

C. 径向基核函数

D. Sigmoid核函数



假如我们使用非线性可分的SVM目标函数作为最优化对象, 我们怎么保证模型线性可分 :                                （C）

A. 设C=1

B. 设C=0

C. 设C=无穷大

D. 以上都不对

【解析】

无穷大保证了所有的线性不可分都是可以忍受的



**聚类算法**

在有监督学习中，我们如何使用聚类算法           （B）

1. 我们可以先创建聚类类别，然后在每个类别上用监督学习分别进行学习

2. 我们可以使用聚类“类别id”作为一个新的特征项，然后再用监督学习分别进行学习

3. 在进行监督学习之前，我们不能新建聚类类别

4. 我们不可以使用聚类“类别id”作为一个新的特征项，然后再用监督学习分别进行学习

A. 2和4

B. 1和2

C. 3和4

D. 1和3



**特征降维**

下列方法中，不可以用于特征降维的方法包括。     （E）

A 主成分分析PCA

B 线性判别分析LDA

C 深度学习SparseAutoEncoder

D 矩阵奇异值分解SVD

E 最小二乘法LeastSquares

【解析】

A.特征降维方法主要有：PCA，LLE，Isomap

B.LDA:线性判别分析，可用于降维

C.稀疏自编码就是用少于输入层神经元数量的隐含层神经元去学习表征输入层的特征，相当于把输入层的特征压缩了，所以是特征降维。

D.SVD和PCA类似，也能看成一种降维方法



想要减少数据集中的特征数, 即降维，以下哪些方案适合:       （D）

1. 使用前向特征选择方法

2. 使用后向特征排除方法

3. 我们先把所有特征都使用, 去训练一个模型, 得到测试集上的表现. 然后去掉一个特征, 再去训练, 用交叉验证看看测试集上的表现. 如果表 现比原来还要好, 我们可以去除这个特征.

4. 查看相关性表, 去除相关性最高的一些特征

A. 1、2

B. 2、3、4

C. 1、2、4

D. All



**神经网络**

对于神经网络的说法, 下面正确的是 :            （A）

1. 增加神经网络层数, 可能会增加测试数据集的分类错误率

2. 减少神经网络层数, 总是能减小测试数据集的分类错误率

3. 增加神经网络层数, 总是能减小训练数据集的分类错误率

A. 1

B. 1 和 3

C. 1 和 2

D. 2

【解析】

深度神经网络的成功, 已经证明增加神经网络层数, 可以增加模型范化能力, 也就是，训练数据集和测试数据集都表现得更好。但这篇文献中（<https://arxiv.org/pdf/1512.03385v1.pdf>）, 作者提到更多的层, 也不一定能保证有更好的表现. 所以不能绝对地说层数多的好坏, 只能选A



**决策树**

想在大数据集上训练决策树, 为了使用较少时间, 我们可以 :                   （C）

A.增加树的深度

B.增加学习率 (learning rate)

C.减少树的深度

D.减少树的数量

【解析】

增加树的深度, 会导致所有节点不断分裂, 直到叶子节点是纯的为止. 所以, 增加深度, 会延长训练时间.

决策树没有学习率参数可以调. (不像集成学习和其它有步长的学习方法)

决策树只有一棵树, 不是随机森林.