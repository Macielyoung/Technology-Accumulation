{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Tensorflow即时执行模式（TF2默认使用，TF1中需调用tf.enable_eager_execution()函数才能启用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.90205395, shape=(), dtype=float32)\n",
      "tf.Tensor([0.93093073], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "random_float = tf.random.uniform(shape=())\n",
    "random_float2 = tf.random.uniform(shape=([1]))\n",
    "print(random_float)\n",
    "print(random_float2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "A = tf.constant([[1, 2], [3, 4]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "<dtype: 'int32'>\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "print(A.shape)\n",
    "print(A.dtype)\n",
    "print(A.numpy())\n",
    "# 查看张量的形状，类型和值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = tf.constant([[5, 6], [7, 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 6  8]\n",
      " [10 12]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[19 22]\n",
      " [43 50]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "C = tf.add(A, B)        #矩阵加法\n",
    "D = tf.matmul(A, B)     #矩阵内积\n",
    "print(C)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32) tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 计算函数导数\n",
    "x = tf.Variable(initial_value=3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.square(x)\n",
    "y_grad = tape.gradient(y, x)\n",
    "print(y, y_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多元函数求导\n",
    "# 函数 L(w,b)=||Xw+b-y||^2 w=(1,2)^T, b=1\n",
    "X = tf.constant([[1., 2.], [3., 4.]])\n",
    "y = tf.constant([[1.], [2.]])\n",
    "w = tf.Variable(initial_value=[[1.], [2.]])\n",
    "b = tf.Variable(initial_value=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(125.0, shape=(), dtype=float32) tf.Tensor(\n",
      "[[ 70.]\n",
      " [100.]], shape=(2, 1), dtype=float32)\n",
      "tf.Tensor(125.0, shape=(), dtype=float32) tf.Tensor(30.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    L = tf.reduce_sum(tf.square(tf.matmul(X, w)+b-y))\n",
    "w_grad, b_grad = tape.gradient(L, [w, b])\n",
    "print(L, w_grad)\n",
    "print(L, b_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.25 0.5  0.75 1.  ]\n",
      "[0.         0.36363637 0.54545456 0.8181818  1.        ]\n"
     ]
    }
   ],
   "source": [
    "# TF简单实现线性回归\n",
    "import numpy as np\n",
    "X_raw = np.array([2013, 2014, 2015, 2016, 2017], dtype=np.float32)\n",
    "Y_raw = np.array([12000, 14000, 15000, 16500, 17500], dtype=np.float32)\n",
    "\n",
    "X = (X_raw-X_raw.min()) / (X_raw.max()-X_raw.min())\n",
    "Y = (Y_raw-Y_raw.min()) / (Y_raw.max()-Y_raw.min())\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.08963588> <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.4503194>\n"
     ]
    }
   ],
   "source": [
    "X = tf.constant(X)\n",
    "Y = tf.constant(Y)\n",
    "\n",
    "a = tf.Variable(initial_value=0.)\n",
    "b = tf.Variable(initial_value=0.)\n",
    "variables = [a, b]\n",
    "\n",
    "num_epoch = 1000\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "for epoch in range(num_epoch):\n",
    "    # 使用tf.GradientTape()记录损失函数的梯度信息\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = a * X + b\n",
    "        loss = tf.reduce_sum(tf.square(y_pred - y))\n",
    "    # TF自动计算损失函数关于自变量（即模型参数）的梯度\n",
    "    grads = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, variables))\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
      "606208/600901 [==============================] - 1s 2us/step\n",
      "/home/opt/.keras/datasets/nietzsche.txt\n"
     ]
    }
   ],
   "source": [
    "path = tf.keras.utils.get_file(\"nietzsche.txt\", origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting zh\n",
      "  Downloading zh-1.0.0.zip (876 bytes)\n",
      "Building wheels for collected packages: zh\n",
      "  Building wheel for zh (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for zh: filename=zh-1.0.0-py3-none-any.whl size=1267 sha256=77c5a4f265d35f85f52c62f940aee299b52004e760852e301b1e5e2fce4462ae\n",
      "  Stored in directory: /home/opt/.cache/pip/wheels/6e/a4/9d/15dfd9db4199cbb84abc69dc8d3c0648a9664ae3fa30f7218a\n",
      "Successfully built zh\n",
      "Installing collected packages: zh\n",
      "Successfully installed zh-1.0.0\n"
     ]
    }
   ],
   "source": [
    "! pip3 install zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个简单的训练，并保存加载模型流程\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()    # flatten层会将除batch_size(第一维)外的维度合并\n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.flatten(inputs)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # MNIST中的图像默认为uint8（0-255的数字）。以下代码将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        # 从数据集中随机取出batch_size个元素并返回\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(lr, batch_size, epoches, model_path, data_loader):\n",
    "    model = MLP()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    num_batches = int(data_loader.num_train_data // batch_size * epoches)\n",
    "    checkpoint = tf.train.Checkpoint(myAwesomeModel=model)\n",
    "    \n",
    "    for batch_index in range(1, num_batches+1):\n",
    "        X, y = data_loader.get_batch(batch_size)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X)\n",
    "            # TF2 Keras中有两个与交叉熵相关的损失函数：\n",
    "            # tf.keras.losses.categorical_crossentropy和tf.keras.losses.sparse_categorical_crossentropy\n",
    "            # 后者可以直接传入int类型的标签类别，它会将标签one_hot化\n",
    "            loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            print(\"batch: {}, loss: {}\".format(batch_index, loss.numpy()))\n",
    "        grads = tape.gradient(loss, model.variables)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "        if batch_index % 100 == 0:\n",
    "            path = checkpoint.save(model_path)\n",
    "            print(\"model saved to {}\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_root, data_loader):\n",
    "    model = MLP()\n",
    "    # 从保存的参数中恢复模型\n",
    "    checkpoint = tf.train.Checkpoint(myAwesomeModel=model)\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(model_root))\n",
    "    y_pred = np.argmax(model.call(data_loader.test_data), axis=-1)\n",
    "    test_acc = sum(y_pred == data_loader.test_label) / data_loader.num_test_data\n",
    "    print(\"test accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoches = 20\n",
    "batch_size = 64\n",
    "learning_rate = 0.0001\n",
    "data_loader = MNISTLoader()\n",
    "model_path = \"./saving/model.ckpt\"\n",
    "model_root = \"./saving\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1, loss: 2.325194835662842\n",
      "batch: 2, loss: 2.389965295791626\n",
      "batch: 3, loss: 2.4046218395233154\n",
      "batch: 4, loss: 2.3886492252349854\n",
      "batch: 5, loss: 2.5222110748291016\n",
      "batch: 6, loss: 2.311737060546875\n",
      "batch: 7, loss: 2.3218295574188232\n",
      "batch: 8, loss: 2.3464279174804688\n",
      "batch: 9, loss: 2.297900676727295\n",
      "batch: 10, loss: 2.3356008529663086\n",
      "batch: 11, loss: 2.3765928745269775\n",
      "batch: 12, loss: 2.2525863647460938\n",
      "batch: 13, loss: 2.404512643814087\n",
      "batch: 14, loss: 2.28544282913208\n",
      "batch: 15, loss: 2.334932327270508\n",
      "batch: 16, loss: 2.200711488723755\n",
      "batch: 17, loss: 2.2805228233337402\n",
      "batch: 18, loss: 2.238523483276367\n",
      "batch: 19, loss: 2.254483699798584\n",
      "batch: 20, loss: 2.2005114555358887\n",
      "batch: 21, loss: 2.179486036300659\n",
      "batch: 22, loss: 2.198049545288086\n",
      "batch: 23, loss: 2.1824395656585693\n",
      "batch: 24, loss: 2.08872389793396\n",
      "batch: 25, loss: 2.25022029876709\n",
      "batch: 26, loss: 2.1311352252960205\n",
      "batch: 27, loss: 2.1372039318084717\n",
      "batch: 28, loss: 2.181314468383789\n",
      "batch: 29, loss: 2.288290023803711\n",
      "batch: 30, loss: 2.096345901489258\n",
      "batch: 31, loss: 2.1235404014587402\n",
      "batch: 32, loss: 2.134448766708374\n",
      "batch: 33, loss: 2.1248152256011963\n",
      "batch: 34, loss: 2.143030881881714\n",
      "batch: 35, loss: 2.1212358474731445\n",
      "batch: 36, loss: 2.1435227394104004\n",
      "batch: 37, loss: 2.006190299987793\n",
      "batch: 38, loss: 2.1313552856445312\n",
      "batch: 39, loss: 2.081785202026367\n",
      "batch: 40, loss: 2.0196428298950195\n",
      "batch: 41, loss: 2.041454553604126\n",
      "batch: 42, loss: 2.001826286315918\n",
      "batch: 43, loss: 2.0588510036468506\n",
      "batch: 44, loss: 1.99933922290802\n",
      "batch: 45, loss: 1.9609990119934082\n",
      "batch: 46, loss: 2.0110573768615723\n",
      "batch: 47, loss: 2.024341583251953\n",
      "batch: 48, loss: 2.040909767150879\n",
      "batch: 49, loss: 2.0188093185424805\n",
      "batch: 50, loss: 1.9412988424301147\n",
      "batch: 51, loss: 2.016317367553711\n",
      "batch: 52, loss: 1.9024696350097656\n",
      "batch: 53, loss: 1.9975775480270386\n",
      "batch: 54, loss: 1.9678963422775269\n",
      "batch: 55, loss: 2.0425820350646973\n",
      "batch: 56, loss: 2.0192363262176514\n",
      "batch: 57, loss: 1.9559745788574219\n",
      "batch: 58, loss: 1.8812042474746704\n",
      "batch: 59, loss: 1.9178099632263184\n",
      "batch: 60, loss: 1.9158384799957275\n",
      "batch: 61, loss: 1.901402235031128\n",
      "batch: 62, loss: 1.8465815782546997\n",
      "batch: 63, loss: 1.9488807916641235\n",
      "batch: 64, loss: 1.7950553894042969\n",
      "batch: 65, loss: 1.8611687421798706\n",
      "batch: 66, loss: 1.9009063243865967\n",
      "batch: 67, loss: 1.803659200668335\n",
      "batch: 68, loss: 1.8303859233856201\n",
      "batch: 69, loss: 1.7716169357299805\n",
      "batch: 70, loss: 1.8160842657089233\n",
      "batch: 71, loss: 1.851252794265747\n",
      "batch: 72, loss: 1.842100977897644\n",
      "batch: 73, loss: 1.8042879104614258\n",
      "batch: 74, loss: 1.799948811531067\n",
      "batch: 75, loss: 1.7787991762161255\n",
      "batch: 76, loss: 1.7226176261901855\n",
      "batch: 77, loss: 1.7775717973709106\n",
      "batch: 78, loss: 1.8058958053588867\n",
      "batch: 79, loss: 1.7580900192260742\n",
      "batch: 80, loss: 1.715726613998413\n",
      "batch: 81, loss: 1.7460393905639648\n",
      "batch: 82, loss: 1.711680293083191\n",
      "batch: 83, loss: 1.673709511756897\n",
      "batch: 84, loss: 1.7123886346817017\n",
      "batch: 85, loss: 1.73819899559021\n",
      "batch: 86, loss: 1.7342662811279297\n",
      "batch: 87, loss: 1.7906752824783325\n",
      "batch: 88, loss: 1.7173625230789185\n",
      "batch: 89, loss: 1.6667627096176147\n",
      "batch: 90, loss: 1.644578218460083\n",
      "batch: 91, loss: 1.6638624668121338\n",
      "batch: 92, loss: 1.7391252517700195\n",
      "batch: 93, loss: 1.6269607543945312\n",
      "batch: 94, loss: 1.6577188968658447\n",
      "batch: 95, loss: 1.769928216934204\n",
      "batch: 96, loss: 1.719653606414795\n",
      "batch: 97, loss: 1.6756970882415771\n",
      "batch: 98, loss: 1.6456427574157715\n",
      "batch: 99, loss: 1.5893968343734741\n",
      "batch: 100, loss: 1.6718108654022217\n",
      "model saved to ./saving/model.ckpt-1\n",
      "batch: 101, loss: 1.5657631158828735\n",
      "batch: 102, loss: 1.7005189657211304\n",
      "batch: 103, loss: 1.5993051528930664\n",
      "batch: 104, loss: 1.5605794191360474\n",
      "batch: 105, loss: 1.5983529090881348\n",
      "batch: 106, loss: 1.6823135614395142\n",
      "batch: 107, loss: 1.6199166774749756\n",
      "batch: 108, loss: 1.6277527809143066\n",
      "batch: 109, loss: 1.6467623710632324\n",
      "batch: 110, loss: 1.6405603885650635\n",
      "batch: 111, loss: 1.5110905170440674\n",
      "batch: 112, loss: 1.6345405578613281\n",
      "batch: 113, loss: 1.5324252843856812\n",
      "batch: 114, loss: 1.5690240859985352\n",
      "batch: 115, loss: 1.6679022312164307\n",
      "batch: 116, loss: 1.5759124755859375\n",
      "batch: 117, loss: 1.6142573356628418\n",
      "batch: 118, loss: 1.5009362697601318\n",
      "batch: 119, loss: 1.5307426452636719\n",
      "batch: 120, loss: 1.503450632095337\n",
      "batch: 121, loss: 1.47212553024292\n",
      "batch: 122, loss: 1.5260926485061646\n",
      "batch: 123, loss: 1.4868321418762207\n",
      "batch: 124, loss: 1.5803754329681396\n",
      "batch: 125, loss: 1.423120141029358\n",
      "batch: 126, loss: 1.502619743347168\n",
      "batch: 127, loss: 1.456648349761963\n",
      "batch: 128, loss: 1.4265773296356201\n",
      "batch: 129, loss: 1.4144604206085205\n",
      "batch: 130, loss: 1.329556941986084\n",
      "batch: 131, loss: 1.4130215644836426\n",
      "batch: 132, loss: 1.3880257606506348\n",
      "batch: 133, loss: 1.5146915912628174\n",
      "batch: 134, loss: 1.3986259698867798\n",
      "batch: 135, loss: 1.3041646480560303\n",
      "batch: 136, loss: 1.4190016984939575\n",
      "batch: 137, loss: 1.417600393295288\n",
      "batch: 138, loss: 1.3434786796569824\n",
      "batch: 139, loss: 1.3041714429855347\n",
      "batch: 140, loss: 1.412470817565918\n",
      "batch: 141, loss: 1.4682447910308838\n",
      "batch: 142, loss: 1.2874174118041992\n",
      "batch: 143, loss: 1.3032437562942505\n",
      "batch: 144, loss: 1.3070485591888428\n",
      "batch: 145, loss: 1.436118721961975\n",
      "batch: 146, loss: 1.323133111000061\n",
      "batch: 147, loss: 1.3927552700042725\n",
      "batch: 148, loss: 1.3621628284454346\n",
      "batch: 149, loss: 1.3240175247192383\n",
      "batch: 150, loss: 1.3182346820831299\n",
      "batch: 151, loss: 1.257283329963684\n",
      "batch: 152, loss: 1.2823632955551147\n",
      "batch: 153, loss: 1.2714781761169434\n",
      "batch: 154, loss: 1.2861756086349487\n",
      "batch: 155, loss: 1.189107894897461\n",
      "batch: 156, loss: 1.2759099006652832\n",
      "batch: 157, loss: 1.2782213687896729\n",
      "batch: 158, loss: 1.285761833190918\n",
      "batch: 159, loss: 1.2694703340530396\n",
      "batch: 160, loss: 1.3635694980621338\n",
      "batch: 161, loss: 1.3718122243881226\n",
      "batch: 162, loss: 1.288601279258728\n",
      "batch: 163, loss: 1.1609691381454468\n",
      "batch: 164, loss: 1.2346012592315674\n",
      "batch: 165, loss: 1.3028446435928345\n",
      "batch: 166, loss: 1.318945050239563\n",
      "batch: 167, loss: 1.1704459190368652\n",
      "batch: 168, loss: 1.1667695045471191\n",
      "batch: 169, loss: 1.2096261978149414\n",
      "batch: 170, loss: 1.1824864149093628\n",
      "batch: 171, loss: 1.12451171875\n",
      "batch: 172, loss: 1.3304827213287354\n",
      "batch: 173, loss: 1.2573864459991455\n",
      "batch: 174, loss: 1.2993419170379639\n",
      "batch: 175, loss: 1.1314215660095215\n",
      "batch: 176, loss: 1.0924897193908691\n",
      "batch: 177, loss: 1.221097469329834\n",
      "batch: 178, loss: 1.0771136283874512\n",
      "batch: 179, loss: 1.181530475616455\n",
      "batch: 180, loss: 1.0926611423492432\n",
      "batch: 181, loss: 1.0596065521240234\n",
      "batch: 182, loss: 1.2419377565383911\n",
      "batch: 183, loss: 1.0105758905410767\n",
      "batch: 184, loss: 1.1880030632019043\n",
      "batch: 185, loss: 1.0931061506271362\n",
      "batch: 186, loss: 1.2693356275558472\n",
      "batch: 187, loss: 1.1846418380737305\n",
      "batch: 188, loss: 1.1139893531799316\n",
      "batch: 189, loss: 1.106013298034668\n",
      "batch: 190, loss: 1.0611506700515747\n",
      "batch: 191, loss: 1.0826336145401\n",
      "batch: 192, loss: 1.1230218410491943\n",
      "batch: 193, loss: 1.029989242553711\n",
      "batch: 194, loss: 1.0565505027770996\n",
      "batch: 195, loss: 1.0858464241027832\n",
      "batch: 196, loss: 1.0439121723175049\n",
      "batch: 197, loss: 1.0651800632476807\n",
      "batch: 198, loss: 1.1407191753387451\n",
      "batch: 199, loss: 1.0775275230407715\n",
      "batch: 200, loss: 1.0550708770751953\n",
      "model saved to ./saving/model.ckpt-2\n",
      "batch: 201, loss: 1.2165004014968872\n",
      "batch: 202, loss: 0.9827922582626343\n",
      "batch: 203, loss: 1.0331041812896729\n",
      "batch: 204, loss: 1.1683225631713867\n",
      "batch: 205, loss: 1.030193567276001\n",
      "batch: 206, loss: 1.0163644552230835\n",
      "batch: 207, loss: 1.0830981731414795\n",
      "batch: 208, loss: 1.0516383647918701\n",
      "batch: 209, loss: 1.0935500860214233\n",
      "batch: 210, loss: 1.0206860303878784\n",
      "batch: 211, loss: 0.9683292508125305\n",
      "batch: 212, loss: 1.1030943393707275\n",
      "batch: 213, loss: 0.9841758012771606\n",
      "batch: 214, loss: 1.1156527996063232\n",
      "batch: 215, loss: 1.0358715057373047\n",
      "batch: 216, loss: 1.0417695045471191\n",
      "batch: 217, loss: 1.018219232559204\n",
      "batch: 218, loss: 1.0133635997772217\n",
      "batch: 219, loss: 1.094977855682373\n",
      "batch: 220, loss: 0.875744640827179\n",
      "batch: 221, loss: 1.1044237613677979\n",
      "batch: 222, loss: 1.063016414642334\n",
      "batch: 223, loss: 1.0262068510055542\n",
      "batch: 224, loss: 1.0681244134902954\n",
      "batch: 225, loss: 0.9843069314956665\n",
      "batch: 226, loss: 0.9491353034973145\n",
      "batch: 227, loss: 1.2083141803741455\n",
      "batch: 228, loss: 1.0324862003326416\n",
      "batch: 229, loss: 0.9397190809249878\n",
      "batch: 230, loss: 0.9342494010925293\n",
      "batch: 231, loss: 1.0231068134307861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 232, loss: 0.9153490662574768\n",
      "batch: 233, loss: 0.9473258852958679\n",
      "batch: 234, loss: 0.8783504366874695\n",
      "batch: 235, loss: 1.0314772129058838\n",
      "batch: 236, loss: 1.0794472694396973\n",
      "batch: 237, loss: 0.9361597895622253\n",
      "batch: 238, loss: 0.902553141117096\n",
      "batch: 239, loss: 1.0961575508117676\n",
      "batch: 240, loss: 0.9489743113517761\n",
      "batch: 241, loss: 0.824978232383728\n",
      "batch: 242, loss: 0.8985188007354736\n",
      "batch: 243, loss: 0.9972012042999268\n",
      "batch: 244, loss: 0.8999552726745605\n",
      "batch: 245, loss: 0.9459249973297119\n",
      "batch: 246, loss: 0.8964728713035583\n",
      "batch: 247, loss: 0.9977432489395142\n",
      "batch: 248, loss: 0.9202373027801514\n",
      "batch: 249, loss: 0.8639922142028809\n",
      "batch: 250, loss: 0.8593462109565735\n",
      "batch: 251, loss: 0.8082056045532227\n",
      "batch: 252, loss: 0.8696210384368896\n",
      "batch: 253, loss: 0.8177279233932495\n",
      "batch: 254, loss: 0.7340137958526611\n",
      "batch: 255, loss: 0.9901547431945801\n",
      "batch: 256, loss: 0.8509083986282349\n",
      "batch: 257, loss: 0.9095944166183472\n",
      "batch: 258, loss: 0.7867751121520996\n",
      "batch: 259, loss: 0.8260177969932556\n",
      "batch: 260, loss: 0.7689734697341919\n",
      "batch: 261, loss: 0.8577894568443298\n",
      "batch: 262, loss: 0.8556023836135864\n",
      "batch: 263, loss: 0.7628830671310425\n",
      "batch: 264, loss: 0.9386827945709229\n",
      "batch: 265, loss: 0.9613746404647827\n",
      "batch: 266, loss: 0.8197053074836731\n",
      "batch: 267, loss: 1.1239303350448608\n",
      "batch: 268, loss: 0.8905626535415649\n",
      "batch: 269, loss: 0.796875\n",
      "batch: 270, loss: 0.9738797545433044\n",
      "batch: 271, loss: 0.8721020817756653\n",
      "batch: 272, loss: 0.9028128385543823\n",
      "batch: 273, loss: 0.7497936487197876\n",
      "batch: 274, loss: 0.8138701319694519\n",
      "batch: 275, loss: 0.8329818248748779\n",
      "batch: 276, loss: 0.8237287998199463\n",
      "batch: 277, loss: 0.8608511686325073\n",
      "batch: 278, loss: 0.9660347104072571\n",
      "batch: 279, loss: 0.7686384320259094\n",
      "batch: 280, loss: 0.7867037653923035\n",
      "batch: 281, loss: 0.8712002038955688\n",
      "batch: 282, loss: 0.8687704205513\n",
      "batch: 283, loss: 0.7296045422554016\n",
      "batch: 284, loss: 0.9027702808380127\n",
      "batch: 285, loss: 0.8173377513885498\n",
      "batch: 286, loss: 0.9099909067153931\n",
      "batch: 287, loss: 0.7264819741249084\n",
      "batch: 288, loss: 0.7139191031455994\n",
      "batch: 289, loss: 0.7980877161026001\n",
      "batch: 290, loss: 0.778011679649353\n",
      "batch: 291, loss: 0.8655751943588257\n",
      "batch: 292, loss: 0.8594098091125488\n",
      "batch: 293, loss: 0.7346853017807007\n",
      "batch: 294, loss: 0.9028924107551575\n",
      "batch: 295, loss: 0.6984769701957703\n",
      "batch: 296, loss: 0.8537167906761169\n",
      "batch: 297, loss: 0.882919430732727\n",
      "batch: 298, loss: 0.9131817817687988\n",
      "batch: 299, loss: 0.8743676543235779\n",
      "batch: 300, loss: 0.6761682629585266\n",
      "model saved to ./saving/model.ckpt-3\n",
      "batch: 301, loss: 0.8966623544692993\n",
      "batch: 302, loss: 0.8301888704299927\n",
      "batch: 303, loss: 0.9070197939872742\n",
      "batch: 304, loss: 0.8594294786453247\n",
      "batch: 305, loss: 0.7654801607131958\n",
      "batch: 306, loss: 0.9545865058898926\n",
      "batch: 307, loss: 0.6780298948287964\n",
      "batch: 308, loss: 0.7267789840698242\n",
      "batch: 309, loss: 0.8882986307144165\n",
      "batch: 310, loss: 0.5533646941184998\n",
      "batch: 311, loss: 0.7712719440460205\n",
      "batch: 312, loss: 0.7897653579711914\n",
      "batch: 313, loss: 0.7019021511077881\n",
      "batch: 314, loss: 0.7826828956604004\n",
      "batch: 315, loss: 0.8283489942550659\n",
      "batch: 316, loss: 0.8492174744606018\n",
      "batch: 317, loss: 0.9327748417854309\n",
      "batch: 318, loss: 0.7529928684234619\n",
      "batch: 319, loss: 0.6963884830474854\n",
      "batch: 320, loss: 0.8824536204338074\n",
      "batch: 321, loss: 0.7567071318626404\n",
      "batch: 322, loss: 0.825541615486145\n",
      "batch: 323, loss: 0.7438709735870361\n",
      "batch: 324, loss: 0.7038314342498779\n",
      "batch: 325, loss: 0.7159256935119629\n",
      "batch: 326, loss: 0.9751980304718018\n",
      "batch: 327, loss: 0.7583940029144287\n",
      "batch: 328, loss: 0.645474910736084\n",
      "batch: 329, loss: 0.844979465007782\n",
      "batch: 330, loss: 0.6531392335891724\n",
      "batch: 331, loss: 0.8936896920204163\n",
      "batch: 332, loss: 0.7442356944084167\n",
      "batch: 333, loss: 0.7173939347267151\n",
      "batch: 334, loss: 0.8395575284957886\n",
      "batch: 335, loss: 0.6370446681976318\n",
      "batch: 336, loss: 0.6113929748535156\n",
      "batch: 337, loss: 0.6339409947395325\n",
      "batch: 338, loss: 0.7553696036338806\n",
      "batch: 339, loss: 0.7381842136383057\n",
      "batch: 340, loss: 0.7116671800613403\n",
      "batch: 341, loss: 0.5975476503372192\n",
      "batch: 342, loss: 0.661829948425293\n",
      "batch: 343, loss: 0.7762560844421387\n",
      "batch: 344, loss: 0.8389863967895508\n",
      "batch: 345, loss: 0.6011062860488892\n",
      "batch: 346, loss: 0.7025390863418579\n",
      "batch: 347, loss: 0.7237313985824585\n",
      "batch: 348, loss: 0.6054160594940186\n",
      "batch: 349, loss: 0.7968651056289673\n",
      "batch: 350, loss: 0.7353680729866028\n",
      "batch: 351, loss: 0.8519234657287598\n",
      "batch: 352, loss: 0.758163332939148\n",
      "batch: 353, loss: 0.9325199723243713\n",
      "batch: 354, loss: 0.7355152368545532\n",
      "batch: 355, loss: 0.7389868497848511\n",
      "batch: 356, loss: 0.974945604801178\n",
      "batch: 357, loss: 0.5531607866287231\n",
      "batch: 358, loss: 0.8150967359542847\n",
      "batch: 359, loss: 0.6333532333374023\n",
      "batch: 360, loss: 0.7870263457298279\n",
      "batch: 361, loss: 0.6328092813491821\n",
      "batch: 362, loss: 0.6454917192459106\n",
      "batch: 363, loss: 0.7769649624824524\n",
      "batch: 364, loss: 0.8482439517974854\n",
      "batch: 365, loss: 0.7053272724151611\n",
      "batch: 366, loss: 0.7976638078689575\n",
      "batch: 367, loss: 0.7554830312728882\n",
      "batch: 368, loss: 0.7050083875656128\n",
      "batch: 369, loss: 0.6277928352355957\n",
      "batch: 370, loss: 0.7897685766220093\n",
      "batch: 371, loss: 0.622710108757019\n",
      "batch: 372, loss: 0.8294485807418823\n",
      "batch: 373, loss: 0.7644222974777222\n",
      "batch: 374, loss: 0.67548668384552\n",
      "batch: 375, loss: 0.7848870158195496\n",
      "batch: 376, loss: 0.9289926290512085\n",
      "batch: 377, loss: 0.7057640552520752\n",
      "batch: 378, loss: 0.583354115486145\n",
      "batch: 379, loss: 0.6201740503311157\n",
      "batch: 380, loss: 0.7941315174102783\n",
      "batch: 381, loss: 0.6446287631988525\n",
      "batch: 382, loss: 0.7574032545089722\n",
      "batch: 383, loss: 0.6187793612480164\n",
      "batch: 384, loss: 0.6772922277450562\n",
      "batch: 385, loss: 0.8245643973350525\n",
      "batch: 386, loss: 0.6722732186317444\n",
      "batch: 387, loss: 0.6807230710983276\n",
      "batch: 388, loss: 0.6345588564872742\n",
      "batch: 389, loss: 0.6187655925750732\n",
      "batch: 390, loss: 0.6841373443603516\n",
      "batch: 391, loss: 0.5623624920845032\n",
      "batch: 392, loss: 0.7998371124267578\n",
      "batch: 393, loss: 0.6033763885498047\n",
      "batch: 394, loss: 0.77095627784729\n",
      "batch: 395, loss: 0.7304580211639404\n",
      "batch: 396, loss: 0.6138966679573059\n",
      "batch: 397, loss: 0.7471869587898254\n",
      "batch: 398, loss: 0.6356121301651001\n",
      "batch: 399, loss: 0.530735194683075\n",
      "batch: 400, loss: 0.6984838247299194\n",
      "model saved to ./saving/model.ckpt-4\n",
      "batch: 401, loss: 0.5865162014961243\n",
      "batch: 402, loss: 0.6149247288703918\n",
      "batch: 403, loss: 0.6325597167015076\n",
      "batch: 404, loss: 0.6313897371292114\n",
      "batch: 405, loss: 0.5141173601150513\n",
      "batch: 406, loss: 0.6660522222518921\n",
      "batch: 407, loss: 0.7384740114212036\n",
      "batch: 408, loss: 0.5516325831413269\n",
      "batch: 409, loss: 0.6538506150245667\n",
      "batch: 410, loss: 0.7662755250930786\n",
      "batch: 411, loss: 0.6098921298980713\n",
      "batch: 412, loss: 0.6017570495605469\n",
      "batch: 413, loss: 0.7891700267791748\n",
      "batch: 414, loss: 0.6152973175048828\n",
      "batch: 415, loss: 0.6039561629295349\n",
      "batch: 416, loss: 0.7848489284515381\n",
      "batch: 417, loss: 0.7974883317947388\n",
      "batch: 418, loss: 0.6615052223205566\n",
      "batch: 419, loss: 0.49503087997436523\n",
      "batch: 420, loss: 0.6531819105148315\n",
      "batch: 421, loss: 0.5938883423805237\n",
      "batch: 422, loss: 0.7319700717926025\n",
      "batch: 423, loss: 0.7037124633789062\n",
      "batch: 424, loss: 0.6674242615699768\n",
      "batch: 425, loss: 0.5866299867630005\n",
      "batch: 426, loss: 0.6530231237411499\n",
      "batch: 427, loss: 0.6092647314071655\n",
      "batch: 428, loss: 0.6534383296966553\n",
      "batch: 429, loss: 0.5897137522697449\n",
      "batch: 430, loss: 0.6954320669174194\n",
      "batch: 431, loss: 0.6402285695075989\n",
      "batch: 432, loss: 0.6723911762237549\n",
      "batch: 433, loss: 0.5733917355537415\n",
      "batch: 434, loss: 0.6740397214889526\n",
      "batch: 435, loss: 0.5430022478103638\n",
      "batch: 436, loss: 0.7246992588043213\n",
      "batch: 437, loss: 0.6189011335372925\n",
      "batch: 438, loss: 0.5461705327033997\n",
      "batch: 439, loss: 0.7256547808647156\n",
      "batch: 440, loss: 0.638258695602417\n",
      "batch: 441, loss: 0.7454023957252502\n",
      "batch: 442, loss: 0.7653477191925049\n",
      "batch: 443, loss: 0.6711821556091309\n",
      "batch: 444, loss: 0.574372410774231\n",
      "batch: 445, loss: 0.6091164350509644\n",
      "batch: 446, loss: 0.5614562630653381\n",
      "batch: 447, loss: 0.6809242963790894\n",
      "batch: 448, loss: 0.6546472907066345\n",
      "batch: 449, loss: 0.5621504187583923\n",
      "batch: 450, loss: 0.5447883605957031\n",
      "batch: 451, loss: 0.6307882070541382\n",
      "batch: 452, loss: 0.5637928247451782\n",
      "batch: 453, loss: 0.6178235411643982\n",
      "batch: 454, loss: 0.5469799041748047\n",
      "batch: 455, loss: 0.7590783834457397\n",
      "batch: 456, loss: 0.6631162166595459\n",
      "batch: 457, loss: 0.5511264801025391\n",
      "batch: 458, loss: 0.48420900106430054\n",
      "batch: 459, loss: 0.6100704669952393\n",
      "batch: 460, loss: 0.7447009086608887\n",
      "batch: 461, loss: 0.6440830826759338\n",
      "batch: 462, loss: 0.3971938192844391\n",
      "batch: 463, loss: 0.45902320742607117\n",
      "batch: 464, loss: 0.5470449924468994\n",
      "batch: 465, loss: 0.6504766941070557\n",
      "batch: 466, loss: 0.7326709032058716\n",
      "batch: 467, loss: 0.50868821144104\n",
      "batch: 468, loss: 0.6607167720794678\n",
      "batch: 469, loss: 0.6423988938331604\n",
      "batch: 470, loss: 0.5749943852424622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 471, loss: 0.6976512670516968\n",
      "batch: 472, loss: 0.6713790893554688\n",
      "batch: 473, loss: 0.4526668190956116\n",
      "batch: 474, loss: 0.4625406265258789\n",
      "batch: 475, loss: 0.5723934173583984\n",
      "batch: 476, loss: 0.7038400173187256\n",
      "batch: 477, loss: 0.7028857469558716\n",
      "batch: 478, loss: 0.5926361083984375\n",
      "batch: 479, loss: 0.5913316011428833\n",
      "batch: 480, loss: 0.8051643967628479\n",
      "batch: 481, loss: 0.6355253458023071\n",
      "batch: 482, loss: 0.6942131519317627\n",
      "batch: 483, loss: 0.5657846331596375\n",
      "batch: 484, loss: 0.4419146776199341\n",
      "batch: 485, loss: 0.7403565645217896\n",
      "batch: 486, loss: 0.6423547863960266\n",
      "batch: 487, loss: 0.5630885362625122\n",
      "batch: 488, loss: 0.6361466646194458\n",
      "batch: 489, loss: 0.5762280821800232\n",
      "batch: 490, loss: 0.559424102306366\n",
      "batch: 491, loss: 0.5636579990386963\n",
      "batch: 492, loss: 0.5869840383529663\n",
      "batch: 493, loss: 0.4984332025051117\n",
      "batch: 494, loss: 0.6022838354110718\n",
      "batch: 495, loss: 0.523472011089325\n",
      "batch: 496, loss: 0.5811885595321655\n",
      "batch: 497, loss: 0.632291853427887\n",
      "batch: 498, loss: 0.6367523074150085\n",
      "batch: 499, loss: 0.6326967477798462\n",
      "batch: 500, loss: 0.4297129213809967\n",
      "model saved to ./saving/model.ckpt-5\n",
      "batch: 501, loss: 0.4315694272518158\n",
      "batch: 502, loss: 0.5162258148193359\n",
      "batch: 503, loss: 0.5173115730285645\n",
      "batch: 504, loss: 0.489169180393219\n",
      "batch: 505, loss: 0.6970646381378174\n",
      "batch: 506, loss: 0.7183830738067627\n",
      "batch: 507, loss: 0.5493775010108948\n",
      "batch: 508, loss: 0.6638534069061279\n",
      "batch: 509, loss: 0.48386579751968384\n",
      "batch: 510, loss: 0.6318316459655762\n",
      "batch: 511, loss: 0.6261988878250122\n",
      "batch: 512, loss: 0.539588451385498\n",
      "batch: 513, loss: 0.6090474128723145\n",
      "batch: 514, loss: 0.48386350274086\n",
      "batch: 515, loss: 0.5697155594825745\n",
      "batch: 516, loss: 0.6018663644790649\n",
      "batch: 517, loss: 0.6199612021446228\n",
      "batch: 518, loss: 0.526720404624939\n",
      "batch: 519, loss: 0.4609341621398926\n",
      "batch: 520, loss: 0.6325945854187012\n",
      "batch: 521, loss: 0.4573443531990051\n",
      "batch: 522, loss: 0.589998722076416\n",
      "batch: 523, loss: 0.6303407549858093\n",
      "batch: 524, loss: 0.5672563314437866\n",
      "batch: 525, loss: 0.5636842250823975\n",
      "batch: 526, loss: 0.5633489489555359\n",
      "batch: 527, loss: 0.47254541516304016\n",
      "batch: 528, loss: 0.48161205649375916\n",
      "batch: 529, loss: 0.4986165761947632\n",
      "batch: 530, loss: 0.5064046382904053\n",
      "batch: 531, loss: 0.4525819420814514\n",
      "batch: 532, loss: 0.7715219259262085\n",
      "batch: 533, loss: 0.5974279642105103\n",
      "batch: 534, loss: 0.4632340669631958\n",
      "batch: 535, loss: 0.5153528451919556\n",
      "batch: 536, loss: 0.5288306474685669\n",
      "batch: 537, loss: 0.6238760352134705\n",
      "batch: 538, loss: 0.5099086761474609\n",
      "batch: 539, loss: 0.5045105814933777\n",
      "batch: 540, loss: 0.4837679862976074\n",
      "batch: 541, loss: 0.745134711265564\n",
      "batch: 542, loss: 0.5484403371810913\n",
      "batch: 543, loss: 0.6515144109725952\n",
      "batch: 544, loss: 0.5726038217544556\n",
      "batch: 545, loss: 0.48246875405311584\n",
      "batch: 546, loss: 0.709012508392334\n",
      "batch: 547, loss: 0.4624333083629608\n",
      "batch: 548, loss: 0.49769747257232666\n",
      "batch: 549, loss: 0.4340181350708008\n",
      "batch: 550, loss: 0.5704413652420044\n",
      "batch: 551, loss: 0.5216308832168579\n",
      "batch: 552, loss: 0.5359109044075012\n",
      "batch: 553, loss: 0.5388231873512268\n",
      "batch: 554, loss: 0.5577478408813477\n",
      "batch: 555, loss: 0.5418977737426758\n",
      "batch: 556, loss: 0.4471723139286041\n",
      "batch: 557, loss: 0.6437646746635437\n",
      "batch: 558, loss: 0.4557161331176758\n",
      "batch: 559, loss: 0.5654376149177551\n",
      "batch: 560, loss: 0.48513928055763245\n",
      "batch: 561, loss: 0.6813616156578064\n",
      "batch: 562, loss: 0.46059250831604004\n",
      "batch: 563, loss: 0.5409703254699707\n",
      "batch: 564, loss: 0.3990544378757477\n",
      "batch: 565, loss: 0.671413242816925\n",
      "batch: 566, loss: 0.3624436855316162\n",
      "batch: 567, loss: 0.5305518507957458\n",
      "batch: 568, loss: 0.392069935798645\n",
      "batch: 569, loss: 0.7176908850669861\n",
      "batch: 570, loss: 0.5821425318717957\n",
      "batch: 571, loss: 0.4945252239704132\n",
      "batch: 572, loss: 0.45573216676712036\n",
      "batch: 573, loss: 0.6521310806274414\n",
      "batch: 574, loss: 0.5160362124443054\n",
      "batch: 575, loss: 0.6249440312385559\n",
      "batch: 576, loss: 0.5192341804504395\n",
      "batch: 577, loss: 0.5617863535881042\n",
      "batch: 578, loss: 0.3429487943649292\n",
      "batch: 579, loss: 0.4890221953392029\n",
      "batch: 580, loss: 0.44238245487213135\n",
      "batch: 581, loss: 0.43948572874069214\n",
      "batch: 582, loss: 0.6231415271759033\n",
      "batch: 583, loss: 0.4833418130874634\n",
      "batch: 584, loss: 0.5424723625183105\n",
      "batch: 585, loss: 0.5912325382232666\n",
      "batch: 586, loss: 0.417782187461853\n",
      "batch: 587, loss: 0.6458195447921753\n",
      "batch: 588, loss: 0.3116750121116638\n",
      "batch: 589, loss: 0.5882430672645569\n",
      "batch: 590, loss: 0.4874951243400574\n",
      "batch: 591, loss: 0.5212283134460449\n",
      "batch: 592, loss: 0.43574780225753784\n",
      "batch: 593, loss: 0.5232800841331482\n",
      "batch: 594, loss: 0.47905829548835754\n",
      "batch: 595, loss: 0.6475844383239746\n",
      "batch: 596, loss: 0.5288625359535217\n",
      "batch: 597, loss: 0.5592384338378906\n",
      "batch: 598, loss: 0.6007255911827087\n",
      "batch: 599, loss: 0.48572054505348206\n",
      "batch: 600, loss: 0.5367991924285889\n",
      "model saved to ./saving/model.ckpt-6\n",
      "batch: 601, loss: 0.46390050649642944\n",
      "batch: 602, loss: 0.5978240966796875\n",
      "batch: 603, loss: 0.5335065722465515\n",
      "batch: 604, loss: 0.537632942199707\n",
      "batch: 605, loss: 0.4077221155166626\n",
      "batch: 606, loss: 0.4130420684814453\n",
      "batch: 607, loss: 0.5560750365257263\n",
      "batch: 608, loss: 0.5389996767044067\n",
      "batch: 609, loss: 0.46089160442352295\n",
      "batch: 610, loss: 0.45577865839004517\n",
      "batch: 611, loss: 0.4971603751182556\n",
      "batch: 612, loss: 0.48799842596054077\n",
      "batch: 613, loss: 0.5787330865859985\n",
      "batch: 614, loss: 0.5369975566864014\n",
      "batch: 615, loss: 0.4262944459915161\n",
      "batch: 616, loss: 0.5825086236000061\n",
      "batch: 617, loss: 0.5477017164230347\n",
      "batch: 618, loss: 0.4984600245952606\n",
      "batch: 619, loss: 0.4893309772014618\n",
      "batch: 620, loss: 0.42468464374542236\n",
      "batch: 621, loss: 0.5062304139137268\n",
      "batch: 622, loss: 0.4411298930644989\n",
      "batch: 623, loss: 0.5434410572052002\n",
      "batch: 624, loss: 0.5355895757675171\n",
      "batch: 625, loss: 0.34532594680786133\n",
      "batch: 626, loss: 0.39793750643730164\n",
      "batch: 627, loss: 0.491955041885376\n",
      "batch: 628, loss: 0.44683897495269775\n",
      "batch: 629, loss: 0.5318851470947266\n",
      "batch: 630, loss: 0.5504734516143799\n",
      "batch: 631, loss: 0.4678748548030853\n",
      "batch: 632, loss: 0.416265070438385\n",
      "batch: 633, loss: 0.4820444583892822\n",
      "batch: 634, loss: 0.4419010877609253\n",
      "batch: 635, loss: 0.4903895854949951\n",
      "batch: 636, loss: 0.6173045635223389\n",
      "batch: 637, loss: 0.36745887994766235\n",
      "batch: 638, loss: 0.5896443128585815\n",
      "batch: 639, loss: 0.44743987917900085\n",
      "batch: 640, loss: 0.6785943508148193\n",
      "batch: 641, loss: 0.48207759857177734\n",
      "batch: 642, loss: 0.5517687797546387\n",
      "batch: 643, loss: 0.41115909814834595\n",
      "batch: 644, loss: 0.3979824483394623\n",
      "batch: 645, loss: 0.35534539818763733\n",
      "batch: 646, loss: 0.41557586193084717\n",
      "batch: 647, loss: 0.4623590111732483\n",
      "batch: 648, loss: 0.665235161781311\n",
      "batch: 649, loss: 0.5410246849060059\n",
      "batch: 650, loss: 0.6369752287864685\n",
      "batch: 651, loss: 0.48997825384140015\n",
      "batch: 652, loss: 0.5324743986129761\n",
      "batch: 653, loss: 0.40675246715545654\n",
      "batch: 654, loss: 0.4822390079498291\n",
      "batch: 655, loss: 0.6729612350463867\n",
      "batch: 656, loss: 0.3544626235961914\n",
      "batch: 657, loss: 0.4384700357913971\n",
      "batch: 658, loss: 0.5656816959381104\n",
      "batch: 659, loss: 0.7375574111938477\n",
      "batch: 660, loss: 0.5340946912765503\n",
      "batch: 661, loss: 0.38695061206817627\n",
      "batch: 662, loss: 0.39964914321899414\n",
      "batch: 663, loss: 0.532103955745697\n",
      "batch: 664, loss: 0.49728304147720337\n",
      "batch: 665, loss: 0.6128637790679932\n",
      "batch: 666, loss: 0.42680928111076355\n",
      "batch: 667, loss: 0.5475648045539856\n",
      "batch: 668, loss: 0.36668097972869873\n",
      "batch: 669, loss: 0.3344908356666565\n",
      "batch: 670, loss: 0.4156937897205353\n",
      "batch: 671, loss: 0.44508039951324463\n",
      "batch: 672, loss: 0.4825593829154968\n",
      "batch: 673, loss: 0.465844988822937\n",
      "batch: 674, loss: 0.48079752922058105\n",
      "batch: 675, loss: 0.374164879322052\n",
      "batch: 676, loss: 0.547072172164917\n",
      "batch: 677, loss: 0.4085874557495117\n",
      "batch: 678, loss: 0.5100700259208679\n",
      "batch: 679, loss: 0.48248809576034546\n",
      "batch: 680, loss: 0.6734830737113953\n",
      "batch: 681, loss: 0.46793386340141296\n",
      "batch: 682, loss: 0.5200013518333435\n",
      "batch: 683, loss: 0.30759185552597046\n",
      "batch: 684, loss: 0.43416255712509155\n",
      "batch: 685, loss: 0.5947797298431396\n",
      "batch: 686, loss: 0.37905895709991455\n",
      "batch: 687, loss: 0.43243372440338135\n",
      "batch: 688, loss: 0.5381649732589722\n",
      "batch: 689, loss: 0.37605980038642883\n",
      "batch: 690, loss: 0.45117318630218506\n",
      "batch: 691, loss: 0.39798223972320557\n",
      "batch: 692, loss: 0.5545302629470825\n",
      "batch: 693, loss: 0.42610132694244385\n",
      "batch: 694, loss: 0.4197283685207367\n",
      "batch: 695, loss: 0.47016799449920654\n",
      "batch: 696, loss: 0.5781100988388062\n",
      "batch: 697, loss: 0.5437129735946655\n",
      "batch: 698, loss: 0.5806474089622498\n",
      "batch: 699, loss: 0.3871996998786926\n",
      "batch: 700, loss: 0.49442213773727417\n",
      "model saved to ./saving/model.ckpt-7\n",
      "batch: 701, loss: 0.5245931148529053\n",
      "batch: 702, loss: 0.41440141201019287\n",
      "batch: 703, loss: 0.49797025322914124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 704, loss: 0.3927021324634552\n",
      "batch: 705, loss: 0.5872839689254761\n",
      "batch: 706, loss: 0.5366857647895813\n",
      "batch: 707, loss: 0.5108003616333008\n",
      "batch: 708, loss: 0.52535480260849\n",
      "batch: 709, loss: 0.535590410232544\n",
      "batch: 710, loss: 0.6593323945999146\n",
      "batch: 711, loss: 0.524250864982605\n",
      "batch: 712, loss: 0.5286321640014648\n",
      "batch: 713, loss: 0.5350164175033569\n",
      "batch: 714, loss: 0.4682174026966095\n",
      "batch: 715, loss: 0.3500710427761078\n",
      "batch: 716, loss: 0.4571795165538788\n",
      "batch: 717, loss: 0.439942330121994\n",
      "batch: 718, loss: 0.5061509609222412\n",
      "batch: 719, loss: 0.3944440484046936\n",
      "batch: 720, loss: 0.5777555704116821\n",
      "batch: 721, loss: 0.41974514722824097\n",
      "batch: 722, loss: 0.45940738916397095\n",
      "batch: 723, loss: 0.4874120354652405\n",
      "batch: 724, loss: 0.4545799791812897\n",
      "batch: 725, loss: 0.4595556855201721\n",
      "batch: 726, loss: 0.5065666437149048\n",
      "batch: 727, loss: 0.5035240650177002\n",
      "batch: 728, loss: 0.3831554055213928\n",
      "batch: 729, loss: 0.43563687801361084\n",
      "batch: 730, loss: 0.43350157141685486\n",
      "batch: 731, loss: 0.5180938243865967\n",
      "batch: 732, loss: 0.5077996850013733\n",
      "batch: 733, loss: 0.3767341375350952\n",
      "batch: 734, loss: 0.4192558526992798\n",
      "batch: 735, loss: 0.46847838163375854\n",
      "batch: 736, loss: 0.48920196294784546\n",
      "batch: 737, loss: 0.6394098997116089\n",
      "batch: 738, loss: 0.4268161654472351\n",
      "batch: 739, loss: 0.5794429779052734\n",
      "batch: 740, loss: 0.48394978046417236\n",
      "batch: 741, loss: 0.4738456606864929\n",
      "batch: 742, loss: 0.5536358952522278\n",
      "batch: 743, loss: 0.4743502736091614\n",
      "batch: 744, loss: 0.605982780456543\n",
      "batch: 745, loss: 0.5094246864318848\n",
      "batch: 746, loss: 0.4368055462837219\n",
      "batch: 747, loss: 0.46295079588890076\n",
      "batch: 748, loss: 0.4670957326889038\n",
      "batch: 749, loss: 0.39183270931243896\n",
      "batch: 750, loss: 0.4476098418235779\n",
      "batch: 751, loss: 0.4062700867652893\n",
      "batch: 752, loss: 0.48852548003196716\n",
      "batch: 753, loss: 0.5575097799301147\n",
      "batch: 754, loss: 0.5672765970230103\n",
      "batch: 755, loss: 0.52887362241745\n",
      "batch: 756, loss: 0.44754213094711304\n",
      "batch: 757, loss: 0.43972015380859375\n",
      "batch: 758, loss: 0.48443230986595154\n",
      "batch: 759, loss: 0.4506802558898926\n",
      "batch: 760, loss: 0.759617805480957\n",
      "batch: 761, loss: 0.5075368285179138\n",
      "batch: 762, loss: 0.3273983299732208\n",
      "batch: 763, loss: 0.5068480372428894\n",
      "batch: 764, loss: 0.32662469148635864\n",
      "batch: 765, loss: 0.6818675994873047\n",
      "batch: 766, loss: 0.3857830762863159\n",
      "batch: 767, loss: 0.5323684215545654\n",
      "batch: 768, loss: 0.40418973565101624\n",
      "batch: 769, loss: 0.7100422978401184\n",
      "batch: 770, loss: 0.5128092765808105\n",
      "batch: 771, loss: 0.33121228218078613\n",
      "batch: 772, loss: 0.3972824513912201\n",
      "batch: 773, loss: 0.36440515518188477\n",
      "batch: 774, loss: 0.4843273162841797\n",
      "batch: 775, loss: 0.4297703504562378\n",
      "batch: 776, loss: 0.5356448888778687\n",
      "batch: 777, loss: 0.5719283223152161\n",
      "batch: 778, loss: 0.3727318346500397\n",
      "batch: 779, loss: 0.5255153179168701\n",
      "batch: 780, loss: 0.5700904130935669\n",
      "batch: 781, loss: 0.3398420810699463\n",
      "batch: 782, loss: 0.5655041933059692\n",
      "batch: 783, loss: 0.4203703999519348\n",
      "batch: 784, loss: 0.3727486729621887\n",
      "batch: 785, loss: 0.5880707502365112\n",
      "batch: 786, loss: 0.35029685497283936\n",
      "batch: 787, loss: 0.38154882192611694\n",
      "batch: 788, loss: 0.6262521147727966\n",
      "batch: 789, loss: 0.5211524963378906\n",
      "batch: 790, loss: 0.5129846930503845\n",
      "batch: 791, loss: 0.5011848211288452\n",
      "batch: 792, loss: 0.40953177213668823\n",
      "batch: 793, loss: 0.6295886039733887\n",
      "batch: 794, loss: 0.46410059928894043\n",
      "batch: 795, loss: 0.40453699231147766\n",
      "batch: 796, loss: 0.5486681461334229\n",
      "batch: 797, loss: 0.3367401361465454\n",
      "batch: 798, loss: 0.4448001980781555\n",
      "batch: 799, loss: 0.7024779319763184\n",
      "batch: 800, loss: 0.2816546857357025\n",
      "model saved to ./saving/model.ckpt-8\n",
      "batch: 801, loss: 0.3966958820819855\n",
      "batch: 802, loss: 0.6452226638793945\n",
      "batch: 803, loss: 0.5904660224914551\n",
      "batch: 804, loss: 0.5106348991394043\n",
      "batch: 805, loss: 0.29438066482543945\n",
      "batch: 806, loss: 0.4038321375846863\n",
      "batch: 807, loss: 0.41931286454200745\n",
      "batch: 808, loss: 0.3923797905445099\n",
      "batch: 809, loss: 0.3787524402141571\n",
      "batch: 810, loss: 0.4066498875617981\n",
      "batch: 811, loss: 0.436867356300354\n",
      "batch: 812, loss: 0.3189789056777954\n",
      "batch: 813, loss: 0.44252198934555054\n",
      "batch: 814, loss: 0.36999931931495667\n",
      "batch: 815, loss: 0.48693424463272095\n",
      "batch: 816, loss: 0.3477315604686737\n",
      "batch: 817, loss: 0.45990267395973206\n",
      "batch: 818, loss: 0.2575558125972748\n",
      "batch: 819, loss: 0.4511048197746277\n",
      "batch: 820, loss: 0.6302866339683533\n",
      "batch: 821, loss: 0.5106748938560486\n",
      "batch: 822, loss: 0.5916892886161804\n",
      "batch: 823, loss: 0.4074211120605469\n",
      "batch: 824, loss: 0.40783607959747314\n",
      "batch: 825, loss: 0.47595545649528503\n",
      "batch: 826, loss: 0.44147658348083496\n",
      "batch: 827, loss: 0.5069025754928589\n",
      "batch: 828, loss: 0.3401561379432678\n",
      "batch: 829, loss: 0.5452199578285217\n",
      "batch: 830, loss: 0.41838011145591736\n",
      "batch: 831, loss: 0.5573083162307739\n",
      "batch: 832, loss: 0.45026761293411255\n",
      "batch: 833, loss: 0.37088167667388916\n",
      "batch: 834, loss: 0.36480480432510376\n",
      "batch: 835, loss: 0.47707441449165344\n",
      "batch: 836, loss: 0.317990243434906\n",
      "batch: 837, loss: 0.3789813220500946\n",
      "batch: 838, loss: 0.37749189138412476\n",
      "batch: 839, loss: 0.4567399024963379\n",
      "batch: 840, loss: 0.4498388469219208\n",
      "batch: 841, loss: 0.46622276306152344\n",
      "batch: 842, loss: 0.5429394841194153\n",
      "batch: 843, loss: 0.4855618476867676\n",
      "batch: 844, loss: 0.47173088788986206\n",
      "batch: 845, loss: 0.4707507789134979\n",
      "batch: 846, loss: 0.4777849316596985\n",
      "batch: 847, loss: 0.3456796407699585\n",
      "batch: 848, loss: 0.4326772093772888\n",
      "batch: 849, loss: 0.4564245343208313\n",
      "batch: 850, loss: 0.37896203994750977\n",
      "batch: 851, loss: 0.3906466066837311\n",
      "batch: 852, loss: 0.4912557899951935\n",
      "batch: 853, loss: 0.42936640977859497\n",
      "batch: 854, loss: 0.31324923038482666\n",
      "batch: 855, loss: 0.47226500511169434\n",
      "batch: 856, loss: 0.3534306585788727\n",
      "batch: 857, loss: 0.35054680705070496\n",
      "batch: 858, loss: 0.738437294960022\n",
      "batch: 859, loss: 0.37398236989974976\n",
      "batch: 860, loss: 0.44664266705513\n",
      "batch: 861, loss: 0.5665206909179688\n",
      "batch: 862, loss: 0.6703716516494751\n",
      "batch: 863, loss: 0.5486053228378296\n",
      "batch: 864, loss: 0.48136433959007263\n",
      "batch: 865, loss: 0.4419315457344055\n",
      "batch: 866, loss: 0.30384108424186707\n",
      "batch: 867, loss: 0.4824336767196655\n",
      "batch: 868, loss: 0.4085730016231537\n",
      "batch: 869, loss: 0.30751800537109375\n",
      "batch: 870, loss: 0.34786003828048706\n",
      "batch: 871, loss: 0.31390702724456787\n",
      "batch: 872, loss: 0.4213077425956726\n",
      "batch: 873, loss: 0.39122146368026733\n",
      "batch: 874, loss: 0.386211633682251\n",
      "batch: 875, loss: 0.5473785400390625\n",
      "batch: 876, loss: 0.3611829876899719\n",
      "batch: 877, loss: 0.30037903785705566\n",
      "batch: 878, loss: 0.4595376253128052\n",
      "batch: 879, loss: 0.30903977155685425\n",
      "batch: 880, loss: 0.4561725854873657\n",
      "batch: 881, loss: 0.46905186772346497\n",
      "batch: 882, loss: 0.5211365222930908\n",
      "batch: 883, loss: 0.3694724440574646\n",
      "batch: 884, loss: 0.39193984866142273\n",
      "batch: 885, loss: 0.4013190269470215\n",
      "batch: 886, loss: 0.4710403382778168\n",
      "batch: 887, loss: 0.6004886627197266\n",
      "batch: 888, loss: 0.3769999146461487\n",
      "batch: 889, loss: 0.3246001601219177\n",
      "batch: 890, loss: 0.3599585294723511\n",
      "batch: 891, loss: 0.5025966763496399\n",
      "batch: 892, loss: 0.38703134655952454\n",
      "batch: 893, loss: 0.45911869406700134\n",
      "batch: 894, loss: 0.3387487232685089\n",
      "batch: 895, loss: 0.4582511782646179\n",
      "batch: 896, loss: 0.3250851631164551\n",
      "batch: 897, loss: 0.46484464406967163\n",
      "batch: 898, loss: 0.30496931076049805\n",
      "batch: 899, loss: 0.29549485445022583\n",
      "batch: 900, loss: 0.5470750331878662\n",
      "model saved to ./saving/model.ckpt-9\n",
      "batch: 901, loss: 0.5320886969566345\n",
      "batch: 902, loss: 0.48198747634887695\n",
      "batch: 903, loss: 0.24728432297706604\n",
      "batch: 904, loss: 0.41709059476852417\n",
      "batch: 905, loss: 0.5105688571929932\n",
      "batch: 906, loss: 0.3430555462837219\n",
      "batch: 907, loss: 0.47631511092185974\n",
      "batch: 908, loss: 0.37188178300857544\n",
      "batch: 909, loss: 0.5448289513587952\n",
      "batch: 910, loss: 0.39216846227645874\n",
      "batch: 911, loss: 0.3430860936641693\n",
      "batch: 912, loss: 0.4204134941101074\n",
      "batch: 913, loss: 0.4256187677383423\n",
      "batch: 914, loss: 0.5271746516227722\n",
      "batch: 915, loss: 0.46720266342163086\n",
      "batch: 916, loss: 0.4236539602279663\n",
      "batch: 917, loss: 0.5106850266456604\n",
      "batch: 918, loss: 0.48749327659606934\n",
      "batch: 919, loss: 0.5036653876304626\n",
      "batch: 920, loss: 0.390327513217926\n",
      "batch: 921, loss: 0.4979754388332367\n",
      "batch: 922, loss: 0.3719785511493683\n",
      "batch: 923, loss: 0.29079413414001465\n",
      "batch: 924, loss: 0.6498384475708008\n",
      "batch: 925, loss: 0.3347143530845642\n",
      "batch: 926, loss: 0.6373131275177002\n",
      "batch: 927, loss: 0.4828493893146515\n",
      "batch: 928, loss: 0.3334626257419586\n",
      "batch: 929, loss: 0.33558389544487\n",
      "batch: 930, loss: 0.48044586181640625\n",
      "batch: 931, loss: 0.35356226563453674\n",
      "batch: 932, loss: 0.4504627287387848\n",
      "batch: 933, loss: 0.42436784505844116\n",
      "batch: 934, loss: 0.49915117025375366\n",
      "batch: 935, loss: 0.3346235752105713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 936, loss: 0.35215258598327637\n",
      "batch: 937, loss: 0.5231376886367798\n",
      "batch: 938, loss: 0.5618042349815369\n",
      "batch: 939, loss: 0.42729678750038147\n",
      "batch: 940, loss: 0.5354726314544678\n",
      "batch: 941, loss: 0.4637874364852905\n",
      "batch: 942, loss: 0.2887837290763855\n",
      "batch: 943, loss: 0.4660528302192688\n",
      "batch: 944, loss: 0.48122337460517883\n",
      "batch: 945, loss: 0.3712397515773773\n",
      "batch: 946, loss: 0.37483295798301697\n",
      "batch: 947, loss: 0.4457261264324188\n",
      "batch: 948, loss: 0.5132746696472168\n",
      "batch: 949, loss: 0.42343100905418396\n",
      "batch: 950, loss: 0.5399971008300781\n",
      "batch: 951, loss: 0.2473340928554535\n",
      "batch: 952, loss: 0.45702478289604187\n",
      "batch: 953, loss: 0.5188466310501099\n",
      "batch: 954, loss: 0.30947211384773254\n",
      "batch: 955, loss: 0.37731051445007324\n",
      "batch: 956, loss: 0.3257981538772583\n",
      "batch: 957, loss: 0.40166354179382324\n",
      "batch: 958, loss: 0.38127392530441284\n",
      "batch: 959, loss: 0.24734972417354584\n",
      "batch: 960, loss: 0.5011435747146606\n",
      "batch: 961, loss: 0.48490577936172485\n",
      "batch: 962, loss: 0.3050168454647064\n",
      "batch: 963, loss: 0.5608018636703491\n",
      "batch: 964, loss: 0.3964523673057556\n",
      "batch: 965, loss: 0.487672358751297\n",
      "batch: 966, loss: 0.47224161028862\n",
      "batch: 967, loss: 0.4608687162399292\n",
      "batch: 968, loss: 0.28404122591018677\n",
      "batch: 969, loss: 0.44025325775146484\n",
      "batch: 970, loss: 0.42670124769210815\n",
      "batch: 971, loss: 0.39595872163772583\n",
      "batch: 972, loss: 0.36385664343833923\n",
      "batch: 973, loss: 0.4065714478492737\n",
      "batch: 974, loss: 0.4623957872390747\n",
      "batch: 975, loss: 0.46692216396331787\n",
      "batch: 976, loss: 0.4176342785358429\n",
      "batch: 977, loss: 0.33584028482437134\n",
      "batch: 978, loss: 0.4533979892730713\n",
      "batch: 979, loss: 0.40503835678100586\n",
      "batch: 980, loss: 0.3416396379470825\n",
      "batch: 981, loss: 0.3500857651233673\n",
      "batch: 982, loss: 0.4178265929222107\n",
      "batch: 983, loss: 0.49616920948028564\n",
      "batch: 984, loss: 0.4698246121406555\n",
      "batch: 985, loss: 0.630358099937439\n",
      "batch: 986, loss: 0.4357668161392212\n",
      "batch: 987, loss: 0.34901732206344604\n",
      "batch: 988, loss: 0.4785671830177307\n",
      "batch: 989, loss: 0.23608453571796417\n",
      "batch: 990, loss: 0.29119759798049927\n",
      "batch: 991, loss: 0.413030207157135\n",
      "batch: 992, loss: 0.3074992299079895\n",
      "batch: 993, loss: 0.4176165461540222\n",
      "batch: 994, loss: 0.3739933371543884\n",
      "batch: 995, loss: 0.3727026581764221\n",
      "batch: 996, loss: 0.43196630477905273\n",
      "batch: 997, loss: 0.272333562374115\n",
      "batch: 998, loss: 0.3243524432182312\n",
      "batch: 999, loss: 0.3416569232940674\n",
      "batch: 1000, loss: 0.44720637798309326\n",
      "model saved to ./saving/model.ckpt-10\n",
      "batch: 1001, loss: 0.45318707823753357\n",
      "batch: 1002, loss: 0.4951011538505554\n",
      "batch: 1003, loss: 0.3093961775302887\n",
      "batch: 1004, loss: 0.42978858947753906\n",
      "batch: 1005, loss: 0.5430479049682617\n",
      "batch: 1006, loss: 0.1813618391752243\n",
      "batch: 1007, loss: 0.3203756511211395\n",
      "batch: 1008, loss: 0.39845508337020874\n",
      "batch: 1009, loss: 0.3363579511642456\n",
      "batch: 1010, loss: 0.36306363344192505\n",
      "batch: 1011, loss: 0.6762526631355286\n",
      "batch: 1012, loss: 0.43320536613464355\n",
      "batch: 1013, loss: 0.4599498510360718\n",
      "batch: 1014, loss: 0.25588709115982056\n",
      "batch: 1015, loss: 0.3379358649253845\n",
      "batch: 1016, loss: 0.34297358989715576\n",
      "batch: 1017, loss: 0.32680565118789673\n",
      "batch: 1018, loss: 0.3942432403564453\n",
      "batch: 1019, loss: 0.2596225142478943\n",
      "batch: 1020, loss: 0.3073284327983856\n",
      "batch: 1021, loss: 0.5322964191436768\n",
      "batch: 1022, loss: 0.3131726384162903\n",
      "batch: 1023, loss: 0.3964160084724426\n",
      "batch: 1024, loss: 0.5390869975090027\n",
      "batch: 1025, loss: 0.487201988697052\n",
      "batch: 1026, loss: 0.43680956959724426\n",
      "batch: 1027, loss: 0.39577409625053406\n",
      "batch: 1028, loss: 0.41214436292648315\n",
      "batch: 1029, loss: 0.3144294321537018\n",
      "batch: 1030, loss: 0.3791743814945221\n",
      "batch: 1031, loss: 0.29896432161331177\n",
      "batch: 1032, loss: 0.4541478753089905\n",
      "batch: 1033, loss: 0.2970453202724457\n",
      "batch: 1034, loss: 0.5041187405586243\n",
      "batch: 1035, loss: 0.4556594789028168\n",
      "batch: 1036, loss: 0.28980663418769836\n",
      "batch: 1037, loss: 0.3596996068954468\n",
      "batch: 1038, loss: 0.39860236644744873\n",
      "batch: 1039, loss: 0.2814628779888153\n",
      "batch: 1040, loss: 0.4899861216545105\n",
      "batch: 1041, loss: 0.3567623794078827\n",
      "batch: 1042, loss: 0.46612411737442017\n",
      "batch: 1043, loss: 0.5820491313934326\n",
      "batch: 1044, loss: 0.3410954475402832\n",
      "batch: 1045, loss: 0.34450358152389526\n",
      "batch: 1046, loss: 0.5658534169197083\n",
      "batch: 1047, loss: 0.41182219982147217\n",
      "batch: 1048, loss: 0.36025887727737427\n",
      "batch: 1049, loss: 0.3368713855743408\n",
      "batch: 1050, loss: 0.33243799209594727\n",
      "batch: 1051, loss: 0.31735801696777344\n",
      "batch: 1052, loss: 0.42045724391937256\n",
      "batch: 1053, loss: 0.41286948323249817\n",
      "batch: 1054, loss: 0.4610990285873413\n",
      "batch: 1055, loss: 0.486733615398407\n",
      "batch: 1056, loss: 0.34279900789260864\n",
      "batch: 1057, loss: 0.5461357831954956\n",
      "batch: 1058, loss: 0.4593934416770935\n",
      "batch: 1059, loss: 0.3559921979904175\n",
      "batch: 1060, loss: 0.5009907484054565\n",
      "batch: 1061, loss: 0.41713863611221313\n",
      "batch: 1062, loss: 0.38622140884399414\n",
      "batch: 1063, loss: 0.4346846044063568\n",
      "batch: 1064, loss: 0.2756218910217285\n",
      "batch: 1065, loss: 0.29740551114082336\n",
      "batch: 1066, loss: 0.3731343448162079\n",
      "batch: 1067, loss: 0.4065406918525696\n",
      "batch: 1068, loss: 0.46693533658981323\n",
      "batch: 1069, loss: 0.33469778299331665\n",
      "batch: 1070, loss: 0.5430405139923096\n",
      "batch: 1071, loss: 0.4667591452598572\n",
      "batch: 1072, loss: 0.3916749060153961\n",
      "batch: 1073, loss: 0.4262772798538208\n",
      "batch: 1074, loss: 0.49981212615966797\n",
      "batch: 1075, loss: 0.4354868233203888\n",
      "batch: 1076, loss: 0.24165019392967224\n",
      "batch: 1077, loss: 0.3120512366294861\n",
      "batch: 1078, loss: 0.2673726975917816\n",
      "batch: 1079, loss: 0.3645741045475006\n",
      "batch: 1080, loss: 0.4490271806716919\n",
      "batch: 1081, loss: 0.41225865483283997\n",
      "batch: 1082, loss: 0.5465798377990723\n",
      "batch: 1083, loss: 0.38509953022003174\n",
      "batch: 1084, loss: 0.35684752464294434\n",
      "batch: 1085, loss: 0.6056897044181824\n",
      "batch: 1086, loss: 0.30629926919937134\n",
      "batch: 1087, loss: 0.2719263434410095\n",
      "batch: 1088, loss: 0.5673257112503052\n",
      "batch: 1089, loss: 0.3677462339401245\n",
      "batch: 1090, loss: 0.49658527970314026\n",
      "batch: 1091, loss: 0.30776840448379517\n",
      "batch: 1092, loss: 0.34859293699264526\n",
      "batch: 1093, loss: 0.46926504373550415\n",
      "batch: 1094, loss: 0.552965521812439\n",
      "batch: 1095, loss: 0.2666887044906616\n",
      "batch: 1096, loss: 0.388970285654068\n",
      "batch: 1097, loss: 0.2832677364349365\n",
      "batch: 1098, loss: 0.31172144412994385\n",
      "batch: 1099, loss: 0.2736746072769165\n",
      "batch: 1100, loss: 0.25721240043640137\n",
      "model saved to ./saving/model.ckpt-11\n",
      "batch: 1101, loss: 0.4013447165489197\n",
      "batch: 1102, loss: 0.3619818687438965\n",
      "batch: 1103, loss: 0.36917996406555176\n",
      "batch: 1104, loss: 0.5602239966392517\n",
      "batch: 1105, loss: 0.29462143778800964\n",
      "batch: 1106, loss: 0.30251753330230713\n",
      "batch: 1107, loss: 0.36631643772125244\n",
      "batch: 1108, loss: 0.3821837604045868\n",
      "batch: 1109, loss: 0.26853227615356445\n",
      "batch: 1110, loss: 0.4797573685646057\n",
      "batch: 1111, loss: 0.4042016267776489\n",
      "batch: 1112, loss: 0.47329819202423096\n",
      "batch: 1113, loss: 0.35961487889289856\n",
      "batch: 1114, loss: 0.21056431531906128\n",
      "batch: 1115, loss: 0.2927910387516022\n",
      "batch: 1116, loss: 0.3695072829723358\n",
      "batch: 1117, loss: 0.28014788031578064\n",
      "batch: 1118, loss: 0.27878761291503906\n",
      "batch: 1119, loss: 0.40877795219421387\n",
      "batch: 1120, loss: 0.38818663358688354\n",
      "batch: 1121, loss: 0.41230976581573486\n",
      "batch: 1122, loss: 0.3321933448314667\n",
      "batch: 1123, loss: 0.38067159056663513\n",
      "batch: 1124, loss: 0.38024210929870605\n",
      "batch: 1125, loss: 0.3462549149990082\n",
      "batch: 1126, loss: 0.3988218903541565\n",
      "batch: 1127, loss: 0.33843615651130676\n",
      "batch: 1128, loss: 0.27995213866233826\n",
      "batch: 1129, loss: 0.31705600023269653\n",
      "batch: 1130, loss: 0.3103291988372803\n",
      "batch: 1131, loss: 0.2936428487300873\n",
      "batch: 1132, loss: 0.3897063732147217\n",
      "batch: 1133, loss: 0.28516459465026855\n",
      "batch: 1134, loss: 0.3458852171897888\n",
      "batch: 1135, loss: 0.4570402503013611\n",
      "batch: 1136, loss: 0.39132869243621826\n",
      "batch: 1137, loss: 0.3528226912021637\n",
      "batch: 1138, loss: 0.4189179539680481\n",
      "batch: 1139, loss: 0.2860579788684845\n",
      "batch: 1140, loss: 0.38849180936813354\n",
      "batch: 1141, loss: 0.2645280659198761\n",
      "batch: 1142, loss: 0.48854005336761475\n",
      "batch: 1143, loss: 0.5095796585083008\n",
      "batch: 1144, loss: 0.4105047285556793\n",
      "batch: 1145, loss: 0.32869672775268555\n",
      "batch: 1146, loss: 0.37961629033088684\n",
      "batch: 1147, loss: 0.4869096875190735\n",
      "batch: 1148, loss: 0.34888917207717896\n",
      "batch: 1149, loss: 0.4333611726760864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1150, loss: 0.5090944766998291\n",
      "batch: 1151, loss: 0.5090545415878296\n",
      "batch: 1152, loss: 0.3341318964958191\n",
      "batch: 1153, loss: 0.28178513050079346\n",
      "batch: 1154, loss: 0.5153569579124451\n",
      "batch: 1155, loss: 0.31286418437957764\n",
      "batch: 1156, loss: 0.30833619832992554\n",
      "batch: 1157, loss: 0.43117937445640564\n",
      "batch: 1158, loss: 0.3928337097167969\n",
      "batch: 1159, loss: 0.4442348778247833\n",
      "batch: 1160, loss: 0.4162852168083191\n",
      "batch: 1161, loss: 0.24641743302345276\n",
      "batch: 1162, loss: 0.30903440713882446\n",
      "batch: 1163, loss: 0.36470967531204224\n",
      "batch: 1164, loss: 0.5937768816947937\n",
      "batch: 1165, loss: 0.357233464717865\n",
      "batch: 1166, loss: 0.23450332880020142\n",
      "batch: 1167, loss: 0.4211779832839966\n",
      "batch: 1168, loss: 0.3071173429489136\n",
      "batch: 1169, loss: 0.38021937012672424\n",
      "batch: 1170, loss: 0.28239622712135315\n",
      "batch: 1171, loss: 0.2822374105453491\n",
      "batch: 1172, loss: 0.4316883385181427\n",
      "batch: 1173, loss: 0.4933720827102661\n",
      "batch: 1174, loss: 0.362942099571228\n",
      "batch: 1175, loss: 0.39143460988998413\n",
      "batch: 1176, loss: 0.36906126141548157\n",
      "batch: 1177, loss: 0.3918575644493103\n",
      "batch: 1178, loss: 0.382371723651886\n",
      "batch: 1179, loss: 0.3711618185043335\n",
      "batch: 1180, loss: 0.4226263165473938\n",
      "batch: 1181, loss: 0.3326949179172516\n",
      "batch: 1182, loss: 0.3346785306930542\n",
      "batch: 1183, loss: 0.43995702266693115\n",
      "batch: 1184, loss: 0.33504292368888855\n",
      "batch: 1185, loss: 0.32694411277770996\n",
      "batch: 1186, loss: 0.41941747069358826\n",
      "batch: 1187, loss: 0.4610651731491089\n",
      "batch: 1188, loss: 0.3109721541404724\n",
      "batch: 1189, loss: 0.28274640440940857\n",
      "batch: 1190, loss: 0.41774922609329224\n",
      "batch: 1191, loss: 0.37305599451065063\n",
      "batch: 1192, loss: 0.3703687787055969\n",
      "batch: 1193, loss: 0.33329716324806213\n",
      "batch: 1194, loss: 0.5020537376403809\n",
      "batch: 1195, loss: 0.3169061839580536\n",
      "batch: 1196, loss: 0.46770182251930237\n",
      "batch: 1197, loss: 0.46620047092437744\n",
      "batch: 1198, loss: 0.31716829538345337\n",
      "batch: 1199, loss: 0.20612764358520508\n",
      "batch: 1200, loss: 0.3622259199619293\n",
      "model saved to ./saving/model.ckpt-12\n",
      "batch: 1201, loss: 0.46487462520599365\n",
      "batch: 1202, loss: 0.3071748614311218\n",
      "batch: 1203, loss: 0.3803383708000183\n",
      "batch: 1204, loss: 0.4999943971633911\n",
      "batch: 1205, loss: 0.42628708481788635\n",
      "batch: 1206, loss: 0.35896340012550354\n",
      "batch: 1207, loss: 0.30479395389556885\n",
      "batch: 1208, loss: 0.44435781240463257\n",
      "batch: 1209, loss: 0.3911406397819519\n",
      "batch: 1210, loss: 0.4582423269748688\n",
      "batch: 1211, loss: 0.3317769467830658\n",
      "batch: 1212, loss: 0.2758026123046875\n",
      "batch: 1213, loss: 0.23557263612747192\n",
      "batch: 1214, loss: 0.32291674613952637\n",
      "batch: 1215, loss: 0.4203683137893677\n",
      "batch: 1216, loss: 0.48733341693878174\n",
      "batch: 1217, loss: 0.477084219455719\n",
      "batch: 1218, loss: 0.27573058009147644\n",
      "batch: 1219, loss: 0.37665170431137085\n",
      "batch: 1220, loss: 0.39586126804351807\n",
      "batch: 1221, loss: 0.316111296415329\n",
      "batch: 1222, loss: 0.41597980260849\n",
      "batch: 1223, loss: 0.36693963408470154\n",
      "batch: 1224, loss: 0.36437082290649414\n",
      "batch: 1225, loss: 0.22258056700229645\n",
      "batch: 1226, loss: 0.3746206760406494\n",
      "batch: 1227, loss: 0.38884207606315613\n",
      "batch: 1228, loss: 0.20714972913265228\n",
      "batch: 1229, loss: 0.38204461336135864\n",
      "batch: 1230, loss: 0.3780438303947449\n",
      "batch: 1231, loss: 0.5046777725219727\n",
      "batch: 1232, loss: 0.2768898904323578\n",
      "batch: 1233, loss: 0.26896220445632935\n",
      "batch: 1234, loss: 0.31322765350341797\n",
      "batch: 1235, loss: 0.3609868288040161\n",
      "batch: 1236, loss: 0.382814884185791\n",
      "batch: 1237, loss: 0.4305609166622162\n",
      "batch: 1238, loss: 0.5031802654266357\n",
      "batch: 1239, loss: 0.38359662890434265\n",
      "batch: 1240, loss: 0.2418774962425232\n",
      "batch: 1241, loss: 0.31867146492004395\n",
      "batch: 1242, loss: 0.4041794538497925\n",
      "batch: 1243, loss: 0.2591489553451538\n",
      "batch: 1244, loss: 0.3321545720100403\n",
      "batch: 1245, loss: 0.39796286821365356\n",
      "batch: 1246, loss: 0.32693156599998474\n",
      "batch: 1247, loss: 0.33953016996383667\n",
      "batch: 1248, loss: 0.4137186110019684\n",
      "batch: 1249, loss: 0.37619906663894653\n",
      "batch: 1250, loss: 0.4455465078353882\n",
      "batch: 1251, loss: 0.3193780481815338\n",
      "batch: 1252, loss: 0.39699533581733704\n",
      "batch: 1253, loss: 0.49071261286735535\n",
      "batch: 1254, loss: 0.32257768511772156\n",
      "batch: 1255, loss: 0.33190611004829407\n",
      "batch: 1256, loss: 0.32221508026123047\n",
      "batch: 1257, loss: 0.2451172173023224\n",
      "batch: 1258, loss: 0.6057835221290588\n",
      "batch: 1259, loss: 0.3119443655014038\n",
      "batch: 1260, loss: 0.3625684976577759\n",
      "batch: 1261, loss: 0.41289401054382324\n",
      "batch: 1262, loss: 0.33237209916114807\n",
      "batch: 1263, loss: 0.29881906509399414\n",
      "batch: 1264, loss: 0.4320613741874695\n",
      "batch: 1265, loss: 0.3233243227005005\n",
      "batch: 1266, loss: 0.3438777029514313\n",
      "batch: 1267, loss: 0.40232324600219727\n",
      "batch: 1268, loss: 0.39692914485931396\n",
      "batch: 1269, loss: 0.5338468551635742\n",
      "batch: 1270, loss: 0.490238219499588\n",
      "batch: 1271, loss: 0.43609726428985596\n",
      "batch: 1272, loss: 0.3110588490962982\n",
      "batch: 1273, loss: 0.5516657829284668\n",
      "batch: 1274, loss: 0.3081364631652832\n",
      "batch: 1275, loss: 0.40116816759109497\n",
      "batch: 1276, loss: 0.6167967319488525\n",
      "batch: 1277, loss: 0.35038629174232483\n",
      "batch: 1278, loss: 0.45083239674568176\n",
      "batch: 1279, loss: 0.26978713274002075\n",
      "batch: 1280, loss: 0.44348931312561035\n",
      "batch: 1281, loss: 0.36127084493637085\n",
      "batch: 1282, loss: 0.34479355812072754\n",
      "batch: 1283, loss: 0.534946620464325\n",
      "batch: 1284, loss: 0.5862606763839722\n",
      "batch: 1285, loss: 0.2780648171901703\n",
      "batch: 1286, loss: 0.3064815402030945\n",
      "batch: 1287, loss: 0.5321008563041687\n",
      "batch: 1288, loss: 0.38658004999160767\n",
      "batch: 1289, loss: 0.2998223304748535\n",
      "batch: 1290, loss: 0.45216134190559387\n",
      "batch: 1291, loss: 0.28720784187316895\n",
      "batch: 1292, loss: 0.30708953738212585\n",
      "batch: 1293, loss: 0.33391308784484863\n",
      "batch: 1294, loss: 0.33123043179512024\n",
      "batch: 1295, loss: 0.3024650514125824\n",
      "batch: 1296, loss: 0.3961861729621887\n",
      "batch: 1297, loss: 0.2789367437362671\n",
      "batch: 1298, loss: 0.5116953253746033\n",
      "batch: 1299, loss: 0.47874367237091064\n",
      "batch: 1300, loss: 0.30367669463157654\n",
      "model saved to ./saving/model.ckpt-13\n",
      "batch: 1301, loss: 0.37046486139297485\n",
      "batch: 1302, loss: 0.39746856689453125\n",
      "batch: 1303, loss: 0.3399646282196045\n",
      "batch: 1304, loss: 0.2609051764011383\n",
      "batch: 1305, loss: 0.4157180190086365\n",
      "batch: 1306, loss: 0.24468852579593658\n",
      "batch: 1307, loss: 0.29214560985565186\n",
      "batch: 1308, loss: 0.2740829586982727\n",
      "batch: 1309, loss: 0.4639368951320648\n",
      "batch: 1310, loss: 0.4475863575935364\n",
      "batch: 1311, loss: 0.4657008647918701\n",
      "batch: 1312, loss: 0.4060128927230835\n",
      "batch: 1313, loss: 0.4026643931865692\n",
      "batch: 1314, loss: 0.42441320419311523\n",
      "batch: 1315, loss: 0.3524741530418396\n",
      "batch: 1316, loss: 0.3496348559856415\n",
      "batch: 1317, loss: 0.3985057473182678\n",
      "batch: 1318, loss: 0.4348316490650177\n",
      "batch: 1319, loss: 0.42873361706733704\n",
      "batch: 1320, loss: 0.38711947202682495\n",
      "batch: 1321, loss: 0.36665230989456177\n",
      "batch: 1322, loss: 0.5537161827087402\n",
      "batch: 1323, loss: 0.41026273369789124\n",
      "batch: 1324, loss: 0.49224957823753357\n",
      "batch: 1325, loss: 0.34986647963523865\n",
      "batch: 1326, loss: 0.21425896883010864\n",
      "batch: 1327, loss: 0.5557567477226257\n",
      "batch: 1328, loss: 0.47364237904548645\n",
      "batch: 1329, loss: 0.49039918184280396\n",
      "batch: 1330, loss: 0.37508267164230347\n",
      "batch: 1331, loss: 0.2662597894668579\n",
      "batch: 1332, loss: 0.4038751721382141\n",
      "batch: 1333, loss: 0.27294600009918213\n",
      "batch: 1334, loss: 0.3731302320957184\n",
      "batch: 1335, loss: 0.41619569063186646\n",
      "batch: 1336, loss: 0.3568738102912903\n",
      "batch: 1337, loss: 0.29386642575263977\n",
      "batch: 1338, loss: 0.36999449133872986\n",
      "batch: 1339, loss: 0.3749663233757019\n",
      "batch: 1340, loss: 0.19290485978126526\n",
      "batch: 1341, loss: 0.41749483346939087\n",
      "batch: 1342, loss: 0.3503503203392029\n",
      "batch: 1343, loss: 0.37650150060653687\n",
      "batch: 1344, loss: 0.42575472593307495\n",
      "batch: 1345, loss: 0.277011901140213\n",
      "batch: 1346, loss: 0.33491751551628113\n",
      "batch: 1347, loss: 0.3113696873188019\n",
      "batch: 1348, loss: 0.17527391016483307\n",
      "batch: 1349, loss: 0.28854358196258545\n",
      "batch: 1350, loss: 0.44173991680145264\n",
      "batch: 1351, loss: 0.2778860628604889\n",
      "batch: 1352, loss: 0.412715345621109\n",
      "batch: 1353, loss: 0.3298019766807556\n",
      "batch: 1354, loss: 0.3176347315311432\n",
      "batch: 1355, loss: 0.33541083335876465\n",
      "batch: 1356, loss: 0.314828097820282\n",
      "batch: 1357, loss: 0.525772213935852\n",
      "batch: 1358, loss: 0.3159371018409729\n",
      "batch: 1359, loss: 0.3589448928833008\n",
      "batch: 1360, loss: 0.28820526599884033\n",
      "batch: 1361, loss: 0.26381441950798035\n",
      "batch: 1362, loss: 0.41689932346343994\n",
      "batch: 1363, loss: 0.3181059956550598\n",
      "batch: 1364, loss: 0.3012816905975342\n",
      "batch: 1365, loss: 0.37096625566482544\n",
      "batch: 1366, loss: 0.4058989882469177\n",
      "batch: 1367, loss: 0.48200926184654236\n",
      "batch: 1368, loss: 0.6187417507171631\n",
      "batch: 1369, loss: 0.27178096771240234\n",
      "batch: 1370, loss: 0.35547542572021484\n",
      "batch: 1371, loss: 0.3116510510444641\n",
      "batch: 1372, loss: 0.37206751108169556\n",
      "batch: 1373, loss: 0.4124431014060974\n",
      "batch: 1374, loss: 0.3247458338737488\n",
      "batch: 1375, loss: 0.3567829728126526\n",
      "batch: 1376, loss: 0.41550686955451965\n",
      "batch: 1377, loss: 0.46785449981689453\n",
      "batch: 1378, loss: 0.4950003921985626\n",
      "batch: 1379, loss: 0.40631240606307983\n",
      "batch: 1380, loss: 0.5210758447647095\n",
      "batch: 1381, loss: 0.40330204367637634\n",
      "batch: 1382, loss: 0.20636096596717834\n",
      "batch: 1383, loss: 0.3206329047679901\n",
      "batch: 1384, loss: 0.26902681589126587\n",
      "batch: 1385, loss: 0.47886279225349426\n",
      "batch: 1386, loss: 0.47213539481163025\n",
      "batch: 1387, loss: 0.33354413509368896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1388, loss: 0.24920137226581573\n",
      "batch: 1389, loss: 0.30646663904190063\n",
      "batch: 1390, loss: 0.29206693172454834\n",
      "batch: 1391, loss: 0.31082242727279663\n",
      "batch: 1392, loss: 0.387524276971817\n",
      "batch: 1393, loss: 0.320115864276886\n",
      "batch: 1394, loss: 0.46853142976760864\n",
      "batch: 1395, loss: 0.35426339507102966\n",
      "batch: 1396, loss: 0.43315351009368896\n",
      "batch: 1397, loss: 0.28946536779403687\n",
      "batch: 1398, loss: 0.4219953715801239\n",
      "batch: 1399, loss: 0.37698960304260254\n",
      "batch: 1400, loss: 0.42448189854621887\n",
      "model saved to ./saving/model.ckpt-14\n",
      "batch: 1401, loss: 0.19424089789390564\n",
      "batch: 1402, loss: 0.28143084049224854\n",
      "batch: 1403, loss: 0.380035936832428\n",
      "batch: 1404, loss: 0.3264324963092804\n",
      "batch: 1405, loss: 0.23170891404151917\n",
      "batch: 1406, loss: 0.3084562420845032\n",
      "batch: 1407, loss: 0.45597580075263977\n",
      "batch: 1408, loss: 0.29781395196914673\n",
      "batch: 1409, loss: 0.41155245900154114\n",
      "batch: 1410, loss: 0.325695276260376\n",
      "batch: 1411, loss: 0.5214738845825195\n",
      "batch: 1412, loss: 0.27791571617126465\n",
      "batch: 1413, loss: 0.3536752164363861\n",
      "batch: 1414, loss: 0.3553171753883362\n",
      "batch: 1415, loss: 0.493394136428833\n",
      "batch: 1416, loss: 0.2963244318962097\n",
      "batch: 1417, loss: 0.3252236247062683\n",
      "batch: 1418, loss: 0.4369944930076599\n",
      "batch: 1419, loss: 0.27167487144470215\n",
      "batch: 1420, loss: 0.41911983489990234\n",
      "batch: 1421, loss: 0.2890259325504303\n",
      "batch: 1422, loss: 0.4175044894218445\n",
      "batch: 1423, loss: 0.21006567776203156\n",
      "batch: 1424, loss: 0.3044194281101227\n",
      "batch: 1425, loss: 0.3433302640914917\n",
      "batch: 1426, loss: 0.37507233023643494\n",
      "batch: 1427, loss: 0.3358638882637024\n",
      "batch: 1428, loss: 0.32649803161621094\n",
      "batch: 1429, loss: 0.29620087146759033\n",
      "batch: 1430, loss: 0.30623722076416016\n",
      "batch: 1431, loss: 0.4767584800720215\n",
      "batch: 1432, loss: 0.3066929578781128\n",
      "batch: 1433, loss: 0.3879205584526062\n",
      "batch: 1434, loss: 0.27373579144477844\n",
      "batch: 1435, loss: 0.33916735649108887\n",
      "batch: 1436, loss: 0.24367208778858185\n",
      "batch: 1437, loss: 0.3525150418281555\n",
      "batch: 1438, loss: 0.2532058358192444\n",
      "batch: 1439, loss: 0.2557792067527771\n",
      "batch: 1440, loss: 0.5488356351852417\n",
      "batch: 1441, loss: 0.6036350727081299\n",
      "batch: 1442, loss: 0.2871454954147339\n",
      "batch: 1443, loss: 0.25638413429260254\n",
      "batch: 1444, loss: 0.4133312702178955\n",
      "batch: 1445, loss: 0.23008377850055695\n",
      "batch: 1446, loss: 0.3832501769065857\n",
      "batch: 1447, loss: 0.3123745024204254\n",
      "batch: 1448, loss: 0.3275628089904785\n",
      "batch: 1449, loss: 0.3515865206718445\n",
      "batch: 1450, loss: 0.30965378880500793\n",
      "batch: 1451, loss: 0.17354339361190796\n",
      "batch: 1452, loss: 0.41099783778190613\n",
      "batch: 1453, loss: 0.28585514426231384\n",
      "batch: 1454, loss: 0.4358811378479004\n",
      "batch: 1455, loss: 0.38852638006210327\n",
      "batch: 1456, loss: 0.3525164723396301\n",
      "batch: 1457, loss: 0.3781644105911255\n",
      "batch: 1458, loss: 0.26981303095817566\n",
      "batch: 1459, loss: 0.3071679472923279\n",
      "batch: 1460, loss: 0.2997594475746155\n",
      "batch: 1461, loss: 0.25086548924446106\n",
      "batch: 1462, loss: 0.4001504182815552\n",
      "batch: 1463, loss: 0.23591816425323486\n",
      "batch: 1464, loss: 0.3424462080001831\n",
      "batch: 1465, loss: 0.28438901901245117\n",
      "batch: 1466, loss: 0.3904481828212738\n",
      "batch: 1467, loss: 0.2590488791465759\n",
      "batch: 1468, loss: 0.41432130336761475\n",
      "batch: 1469, loss: 0.3137747049331665\n",
      "batch: 1470, loss: 0.2907148599624634\n",
      "batch: 1471, loss: 0.2954375147819519\n",
      "batch: 1472, loss: 0.350385457277298\n",
      "batch: 1473, loss: 0.3033137619495392\n",
      "batch: 1474, loss: 0.45174330472946167\n",
      "batch: 1475, loss: 0.30846840143203735\n",
      "batch: 1476, loss: 0.2842390239238739\n",
      "batch: 1477, loss: 0.320102334022522\n",
      "batch: 1478, loss: 0.36564648151397705\n",
      "batch: 1479, loss: 0.35389894247055054\n",
      "batch: 1480, loss: 0.3730546534061432\n",
      "batch: 1481, loss: 0.3636622428894043\n",
      "batch: 1482, loss: 0.39053618907928467\n",
      "batch: 1483, loss: 0.3282260298728943\n",
      "batch: 1484, loss: 0.5219472646713257\n",
      "batch: 1485, loss: 0.18105363845825195\n",
      "batch: 1486, loss: 0.3487253189086914\n",
      "batch: 1487, loss: 0.18679845333099365\n",
      "batch: 1488, loss: 0.2891885042190552\n",
      "batch: 1489, loss: 0.3243495225906372\n",
      "batch: 1490, loss: 0.429989218711853\n",
      "batch: 1491, loss: 0.3938804268836975\n",
      "batch: 1492, loss: 0.2084423303604126\n",
      "batch: 1493, loss: 0.20085573196411133\n",
      "batch: 1494, loss: 0.3460155725479126\n",
      "batch: 1495, loss: 0.4709911346435547\n",
      "batch: 1496, loss: 0.35933586955070496\n",
      "batch: 1497, loss: 0.35116061568260193\n",
      "batch: 1498, loss: 0.35079705715179443\n",
      "batch: 1499, loss: 0.4008551836013794\n",
      "batch: 1500, loss: 0.3076972961425781\n",
      "model saved to ./saving/model.ckpt-15\n",
      "batch: 1501, loss: 0.24409602582454681\n",
      "batch: 1502, loss: 0.33782362937927246\n",
      "batch: 1503, loss: 0.3241943120956421\n",
      "batch: 1504, loss: 0.2067979872226715\n",
      "batch: 1505, loss: 0.3495265245437622\n",
      "batch: 1506, loss: 0.46049609780311584\n",
      "batch: 1507, loss: 0.482499361038208\n",
      "batch: 1508, loss: 0.37653660774230957\n",
      "batch: 1509, loss: 0.32937729358673096\n",
      "batch: 1510, loss: 0.17302188277244568\n",
      "batch: 1511, loss: 0.3988304138183594\n",
      "batch: 1512, loss: 0.38824647665023804\n",
      "batch: 1513, loss: 0.4359823167324066\n",
      "batch: 1514, loss: 0.24395452439785004\n",
      "batch: 1515, loss: 0.24241194128990173\n",
      "batch: 1516, loss: 0.28920942544937134\n",
      "batch: 1517, loss: 0.3229362964630127\n",
      "batch: 1518, loss: 0.4665476679801941\n",
      "batch: 1519, loss: 0.3283238708972931\n",
      "batch: 1520, loss: 0.31259992718696594\n",
      "batch: 1521, loss: 0.30202075839042664\n",
      "batch: 1522, loss: 0.31469810009002686\n",
      "batch: 1523, loss: 0.2916993498802185\n",
      "batch: 1524, loss: 0.21297869086265564\n",
      "batch: 1525, loss: 0.2478005290031433\n",
      "batch: 1526, loss: 0.3181614875793457\n",
      "batch: 1527, loss: 0.2336043119430542\n",
      "batch: 1528, loss: 0.37196555733680725\n",
      "batch: 1529, loss: 0.37484651803970337\n",
      "batch: 1530, loss: 0.3470642566680908\n",
      "batch: 1531, loss: 0.2397231161594391\n",
      "batch: 1532, loss: 0.1890871524810791\n",
      "batch: 1533, loss: 0.2515527904033661\n",
      "batch: 1534, loss: 0.29908257722854614\n",
      "batch: 1535, loss: 0.2021355777978897\n",
      "batch: 1536, loss: 0.46438974142074585\n",
      "batch: 1537, loss: 0.4922008514404297\n",
      "batch: 1538, loss: 0.38175296783447266\n",
      "batch: 1539, loss: 0.40891483426094055\n",
      "batch: 1540, loss: 0.42228251695632935\n",
      "batch: 1541, loss: 0.2671893239021301\n",
      "batch: 1542, loss: 0.7248789072036743\n",
      "batch: 1543, loss: 0.454642653465271\n",
      "batch: 1544, loss: 0.3271685242652893\n",
      "batch: 1545, loss: 0.41496986150741577\n",
      "batch: 1546, loss: 0.34212130308151245\n",
      "batch: 1547, loss: 0.25276124477386475\n",
      "batch: 1548, loss: 0.3582777678966522\n",
      "batch: 1549, loss: 0.5399725437164307\n",
      "batch: 1550, loss: 0.18281254172325134\n",
      "batch: 1551, loss: 0.3109694719314575\n",
      "batch: 1552, loss: 0.3152354955673218\n",
      "batch: 1553, loss: 0.3100268244743347\n",
      "batch: 1554, loss: 0.29996681213378906\n",
      "batch: 1555, loss: 0.2706236243247986\n",
      "batch: 1556, loss: 0.37331104278564453\n",
      "batch: 1557, loss: 0.2972675859928131\n",
      "batch: 1558, loss: 0.2967132329940796\n",
      "batch: 1559, loss: 0.320746511220932\n",
      "batch: 1560, loss: 0.2825089693069458\n",
      "batch: 1561, loss: 0.4457901120185852\n",
      "batch: 1562, loss: 0.36542463302612305\n",
      "batch: 1563, loss: 0.24677053093910217\n",
      "batch: 1564, loss: 0.22569233179092407\n",
      "batch: 1565, loss: 0.29869982600212097\n",
      "batch: 1566, loss: 0.39055347442626953\n",
      "batch: 1567, loss: 0.297694593667984\n",
      "batch: 1568, loss: 0.48405060172080994\n",
      "batch: 1569, loss: 0.2874256372451782\n",
      "batch: 1570, loss: 0.368103951215744\n",
      "batch: 1571, loss: 0.3463504910469055\n",
      "batch: 1572, loss: 0.34850695729255676\n",
      "batch: 1573, loss: 0.23754578828811646\n",
      "batch: 1574, loss: 0.2621367275714874\n",
      "batch: 1575, loss: 0.32935529947280884\n",
      "batch: 1576, loss: 0.22827693819999695\n",
      "batch: 1577, loss: 0.3607979416847229\n",
      "batch: 1578, loss: 0.2220304012298584\n",
      "batch: 1579, loss: 0.36614519357681274\n",
      "batch: 1580, loss: 0.3092135488986969\n",
      "batch: 1581, loss: 0.270682156085968\n",
      "batch: 1582, loss: 0.20652592182159424\n",
      "batch: 1583, loss: 0.24995854496955872\n",
      "batch: 1584, loss: 0.1492988020181656\n",
      "batch: 1585, loss: 0.27610263228416443\n",
      "batch: 1586, loss: 0.3199799060821533\n",
      "batch: 1587, loss: 0.32088854908943176\n",
      "batch: 1588, loss: 0.35384827852249146\n",
      "batch: 1589, loss: 0.22385811805725098\n",
      "batch: 1590, loss: 0.5206488370895386\n",
      "batch: 1591, loss: 0.25081610679626465\n",
      "batch: 1592, loss: 0.5710723400115967\n",
      "batch: 1593, loss: 0.27448469400405884\n",
      "batch: 1594, loss: 0.3988831341266632\n",
      "batch: 1595, loss: 0.26099151372909546\n",
      "batch: 1596, loss: 0.31478625535964966\n",
      "batch: 1597, loss: 0.3271448612213135\n",
      "batch: 1598, loss: 0.29629144072532654\n",
      "batch: 1599, loss: 0.32628247141838074\n",
      "batch: 1600, loss: 0.3520398736000061\n",
      "model saved to ./saving/model.ckpt-16\n",
      "batch: 1601, loss: 0.452064573764801\n",
      "batch: 1602, loss: 0.22508573532104492\n",
      "batch: 1603, loss: 0.4232451319694519\n",
      "batch: 1604, loss: 0.3266885578632355\n",
      "batch: 1605, loss: 0.3847277760505676\n",
      "batch: 1606, loss: 0.2891998291015625\n",
      "batch: 1607, loss: 0.1793801486492157\n",
      "batch: 1608, loss: 0.2556758522987366\n",
      "batch: 1609, loss: 0.4378063678741455\n",
      "batch: 1610, loss: 0.253584086894989\n",
      "batch: 1611, loss: 0.3162863552570343\n",
      "batch: 1612, loss: 0.3365728259086609\n",
      "batch: 1613, loss: 0.2997886538505554\n",
      "batch: 1614, loss: 0.3299490809440613\n",
      "batch: 1615, loss: 0.32058775424957275\n",
      "batch: 1616, loss: 0.2768786549568176\n",
      "batch: 1617, loss: 0.2794651389122009\n",
      "batch: 1618, loss: 0.3387976288795471\n",
      "batch: 1619, loss: 0.30040040612220764\n",
      "batch: 1620, loss: 0.40410876274108887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1621, loss: 0.2510072886943817\n",
      "batch: 1622, loss: 0.2480393350124359\n",
      "batch: 1623, loss: 0.23898208141326904\n",
      "batch: 1624, loss: 0.3486919105052948\n",
      "batch: 1625, loss: 0.3823986053466797\n",
      "batch: 1626, loss: 0.3348318636417389\n",
      "batch: 1627, loss: 0.36738789081573486\n",
      "batch: 1628, loss: 0.38212060928344727\n",
      "batch: 1629, loss: 0.35948196053504944\n",
      "batch: 1630, loss: 0.317240834236145\n",
      "batch: 1631, loss: 0.2779054641723633\n",
      "batch: 1632, loss: 0.2759813666343689\n",
      "batch: 1633, loss: 0.2577405869960785\n",
      "batch: 1634, loss: 0.31953418254852295\n",
      "batch: 1635, loss: 0.3533358573913574\n",
      "batch: 1636, loss: 0.23588043451309204\n",
      "batch: 1637, loss: 0.34192824363708496\n",
      "batch: 1638, loss: 0.24863985180854797\n",
      "batch: 1639, loss: 0.3313269019126892\n",
      "batch: 1640, loss: 0.35525816679000854\n",
      "batch: 1641, loss: 0.4278450310230255\n",
      "batch: 1642, loss: 0.3539632558822632\n",
      "batch: 1643, loss: 0.18669840693473816\n",
      "batch: 1644, loss: 0.1838739812374115\n",
      "batch: 1645, loss: 0.44654589891433716\n",
      "batch: 1646, loss: 0.2925092875957489\n",
      "batch: 1647, loss: 0.43766462802886963\n",
      "batch: 1648, loss: 0.2598055303096771\n",
      "batch: 1649, loss: 0.3478403091430664\n",
      "batch: 1650, loss: 0.238983154296875\n",
      "batch: 1651, loss: 0.29338327050209045\n",
      "batch: 1652, loss: 0.20140674710273743\n",
      "batch: 1653, loss: 0.31517860293388367\n",
      "batch: 1654, loss: 0.4213666319847107\n",
      "batch: 1655, loss: 0.2657207250595093\n",
      "batch: 1656, loss: 0.3853532075881958\n",
      "batch: 1657, loss: 0.25559601187705994\n",
      "batch: 1658, loss: 0.5118231773376465\n",
      "batch: 1659, loss: 0.23143431544303894\n",
      "batch: 1660, loss: 0.22043518722057343\n",
      "batch: 1661, loss: 0.35503169894218445\n",
      "batch: 1662, loss: 0.2713466286659241\n",
      "batch: 1663, loss: 0.26783573627471924\n",
      "batch: 1664, loss: 0.43321311473846436\n",
      "batch: 1665, loss: 0.4519461691379547\n",
      "batch: 1666, loss: 0.38279449939727783\n",
      "batch: 1667, loss: 0.3831738233566284\n",
      "batch: 1668, loss: 0.38980311155319214\n",
      "batch: 1669, loss: 0.3239435851573944\n",
      "batch: 1670, loss: 0.2401488870382309\n",
      "batch: 1671, loss: 0.2997460961341858\n",
      "batch: 1672, loss: 0.43894168734550476\n",
      "batch: 1673, loss: 0.2928916811943054\n",
      "batch: 1674, loss: 0.47766953706741333\n",
      "batch: 1675, loss: 0.21361854672431946\n",
      "batch: 1676, loss: 0.29631519317626953\n",
      "batch: 1677, loss: 0.3196641802787781\n",
      "batch: 1678, loss: 0.46345028281211853\n",
      "batch: 1679, loss: 0.4943058490753174\n",
      "batch: 1680, loss: 0.3341776132583618\n",
      "batch: 1681, loss: 0.28038880228996277\n",
      "batch: 1682, loss: 0.41298046708106995\n",
      "batch: 1683, loss: 0.2550128698348999\n",
      "batch: 1684, loss: 0.2601085901260376\n",
      "batch: 1685, loss: 0.3886352479457855\n",
      "batch: 1686, loss: 0.4034395217895508\n",
      "batch: 1687, loss: 0.2287677824497223\n",
      "batch: 1688, loss: 0.18930552899837494\n",
      "batch: 1689, loss: 0.24862626194953918\n",
      "batch: 1690, loss: 0.3464576005935669\n",
      "batch: 1691, loss: 0.3118722140789032\n",
      "batch: 1692, loss: 0.5125852227210999\n",
      "batch: 1693, loss: 0.12415720522403717\n",
      "batch: 1694, loss: 0.26748234033584595\n",
      "batch: 1695, loss: 0.3519095480442047\n",
      "batch: 1696, loss: 0.3466845154762268\n",
      "batch: 1697, loss: 0.32646438479423523\n",
      "batch: 1698, loss: 0.3123777508735657\n",
      "batch: 1699, loss: 0.37537914514541626\n",
      "batch: 1700, loss: 0.21174198389053345\n",
      "model saved to ./saving/model.ckpt-17\n",
      "batch: 1701, loss: 0.17828941345214844\n",
      "batch: 1702, loss: 0.24704581499099731\n",
      "batch: 1703, loss: 0.24606703221797943\n",
      "batch: 1704, loss: 0.33795198798179626\n",
      "batch: 1705, loss: 0.25305795669555664\n",
      "batch: 1706, loss: 0.21151617169380188\n",
      "batch: 1707, loss: 0.3024146556854248\n",
      "batch: 1708, loss: 0.3396109342575073\n",
      "batch: 1709, loss: 0.49283766746520996\n",
      "batch: 1710, loss: 0.22296357154846191\n",
      "batch: 1711, loss: 0.3027721345424652\n",
      "batch: 1712, loss: 0.35709789395332336\n",
      "batch: 1713, loss: 0.37630951404571533\n",
      "batch: 1714, loss: 0.4424106180667877\n",
      "batch: 1715, loss: 0.18452203273773193\n",
      "batch: 1716, loss: 0.2409178912639618\n",
      "batch: 1717, loss: 0.32111966609954834\n",
      "batch: 1718, loss: 0.36132997274398804\n",
      "batch: 1719, loss: 0.39215174317359924\n",
      "batch: 1720, loss: 0.27188870310783386\n",
      "batch: 1721, loss: 0.2667407691478729\n",
      "batch: 1722, loss: 0.3497360050678253\n",
      "batch: 1723, loss: 0.4028497040271759\n",
      "batch: 1724, loss: 0.24112489819526672\n",
      "batch: 1725, loss: 0.2590023875236511\n",
      "batch: 1726, loss: 0.4029441475868225\n",
      "batch: 1727, loss: 0.38524338603019714\n",
      "batch: 1728, loss: 0.2215455025434494\n",
      "batch: 1729, loss: 0.32513031363487244\n",
      "batch: 1730, loss: 0.23818834125995636\n",
      "batch: 1731, loss: 0.28941839933395386\n",
      "batch: 1732, loss: 0.40656691789627075\n",
      "batch: 1733, loss: 0.35761550068855286\n",
      "batch: 1734, loss: 0.4326568841934204\n",
      "batch: 1735, loss: 0.42115485668182373\n",
      "batch: 1736, loss: 0.23343494534492493\n",
      "batch: 1737, loss: 0.40705639123916626\n",
      "batch: 1738, loss: 0.2247113585472107\n",
      "batch: 1739, loss: 0.3392574191093445\n",
      "batch: 1740, loss: 0.2321619689464569\n",
      "batch: 1741, loss: 0.37817731499671936\n",
      "batch: 1742, loss: 0.29715996980667114\n",
      "batch: 1743, loss: 0.3755090534687042\n",
      "batch: 1744, loss: 0.3022919297218323\n",
      "batch: 1745, loss: 0.4586035907268524\n",
      "batch: 1746, loss: 0.2985258102416992\n",
      "batch: 1747, loss: 0.3216484487056732\n",
      "batch: 1748, loss: 0.2206779420375824\n",
      "batch: 1749, loss: 0.22512632608413696\n",
      "batch: 1750, loss: 0.34388914704322815\n",
      "batch: 1751, loss: 0.30522027611732483\n",
      "batch: 1752, loss: 0.31455665826797485\n",
      "batch: 1753, loss: 0.30831921100616455\n",
      "batch: 1754, loss: 0.2970990836620331\n",
      "batch: 1755, loss: 0.3838880956172943\n",
      "batch: 1756, loss: 0.21128052473068237\n",
      "batch: 1757, loss: 0.23039591312408447\n",
      "batch: 1758, loss: 0.3194747567176819\n",
      "batch: 1759, loss: 0.4079645872116089\n",
      "batch: 1760, loss: 0.2283802181482315\n",
      "batch: 1761, loss: 0.3239956796169281\n",
      "batch: 1762, loss: 0.26249057054519653\n",
      "batch: 1763, loss: 0.22253237664699554\n",
      "batch: 1764, loss: 0.26897957921028137\n",
      "batch: 1765, loss: 0.2153826653957367\n",
      "batch: 1766, loss: 0.3441171646118164\n",
      "batch: 1767, loss: 0.2906809449195862\n",
      "batch: 1768, loss: 0.2270246297121048\n",
      "batch: 1769, loss: 0.24609893560409546\n",
      "batch: 1770, loss: 0.48093438148498535\n",
      "batch: 1771, loss: 0.28888702392578125\n",
      "batch: 1772, loss: 0.43021100759506226\n",
      "batch: 1773, loss: 0.23638111352920532\n",
      "batch: 1774, loss: 0.2629355192184448\n",
      "batch: 1775, loss: 0.45304805040359497\n",
      "batch: 1776, loss: 0.37928688526153564\n",
      "batch: 1777, loss: 0.2759212851524353\n",
      "batch: 1778, loss: 0.3443865180015564\n",
      "batch: 1779, loss: 0.31383633613586426\n",
      "batch: 1780, loss: 0.3150055408477783\n",
      "batch: 1781, loss: 0.258553683757782\n",
      "batch: 1782, loss: 0.38754016160964966\n",
      "batch: 1783, loss: 0.3174731135368347\n",
      "batch: 1784, loss: 0.22994273900985718\n",
      "batch: 1785, loss: 0.15428434312343597\n",
      "batch: 1786, loss: 0.2854309380054474\n",
      "batch: 1787, loss: 0.2506527602672577\n",
      "batch: 1788, loss: 0.2478877604007721\n",
      "batch: 1789, loss: 0.33148249983787537\n",
      "batch: 1790, loss: 0.3726566433906555\n",
      "batch: 1791, loss: 0.2738913297653198\n",
      "batch: 1792, loss: 0.3890281319618225\n",
      "batch: 1793, loss: 0.3015710115432739\n",
      "batch: 1794, loss: 0.27686500549316406\n",
      "batch: 1795, loss: 0.3521851599216461\n",
      "batch: 1796, loss: 0.3330305516719818\n",
      "batch: 1797, loss: 0.23302364349365234\n",
      "batch: 1798, loss: 0.3820822238922119\n",
      "batch: 1799, loss: 0.3411273956298828\n",
      "batch: 1800, loss: 0.4130765497684479\n",
      "model saved to ./saving/model.ckpt-18\n",
      "batch: 1801, loss: 0.25348007678985596\n",
      "batch: 1802, loss: 0.5426957011222839\n",
      "batch: 1803, loss: 0.3512917757034302\n",
      "batch: 1804, loss: 0.22764818370342255\n",
      "batch: 1805, loss: 0.39071375131607056\n",
      "batch: 1806, loss: 0.2585371136665344\n",
      "batch: 1807, loss: 0.35779616236686707\n",
      "batch: 1808, loss: 0.34066569805145264\n",
      "batch: 1809, loss: 0.19028882682323456\n",
      "batch: 1810, loss: 0.20778396725654602\n",
      "batch: 1811, loss: 0.2539742588996887\n",
      "batch: 1812, loss: 0.26861828565597534\n",
      "batch: 1813, loss: 0.2692759037017822\n",
      "batch: 1814, loss: 0.4885387420654297\n",
      "batch: 1815, loss: 0.3411606550216675\n",
      "batch: 1816, loss: 0.36858615279197693\n",
      "batch: 1817, loss: 0.20451433956623077\n",
      "batch: 1818, loss: 0.43594831228256226\n",
      "batch: 1819, loss: 0.3649550676345825\n",
      "batch: 1820, loss: 0.40225842595100403\n",
      "batch: 1821, loss: 0.29715496301651\n",
      "batch: 1822, loss: 0.2960502505302429\n",
      "batch: 1823, loss: 0.30220550298690796\n",
      "batch: 1824, loss: 0.33765119314193726\n",
      "batch: 1825, loss: 0.5094859600067139\n",
      "batch: 1826, loss: 0.18868595361709595\n",
      "batch: 1827, loss: 0.24988417327404022\n",
      "batch: 1828, loss: 0.1947280764579773\n",
      "batch: 1829, loss: 0.18659554421901703\n",
      "batch: 1830, loss: 0.40211784839630127\n",
      "batch: 1831, loss: 0.47892749309539795\n",
      "batch: 1832, loss: 0.3727089762687683\n",
      "batch: 1833, loss: 0.4233388900756836\n",
      "batch: 1834, loss: 0.3178824782371521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1835, loss: 0.44470518827438354\n",
      "batch: 1836, loss: 0.23679965734481812\n",
      "batch: 1837, loss: 0.11469889432191849\n",
      "batch: 1838, loss: 0.4297742247581482\n",
      "batch: 1839, loss: 0.3129068613052368\n",
      "batch: 1840, loss: 0.3525003492832184\n",
      "batch: 1841, loss: 0.2574625015258789\n",
      "batch: 1842, loss: 0.5321223735809326\n",
      "batch: 1843, loss: 0.22781717777252197\n",
      "batch: 1844, loss: 0.25854507088661194\n",
      "batch: 1845, loss: 0.28553831577301025\n",
      "batch: 1846, loss: 0.43411028385162354\n",
      "batch: 1847, loss: 0.3202386200428009\n",
      "batch: 1848, loss: 0.3180159032344818\n",
      "batch: 1849, loss: 0.38950473070144653\n",
      "batch: 1850, loss: 0.3287869691848755\n",
      "batch: 1851, loss: 0.2632668912410736\n",
      "batch: 1852, loss: 0.47647884488105774\n",
      "batch: 1853, loss: 0.3286556601524353\n",
      "batch: 1854, loss: 0.2526557445526123\n",
      "batch: 1855, loss: 0.3032698631286621\n",
      "batch: 1856, loss: 0.15842978656291962\n",
      "batch: 1857, loss: 0.1756897121667862\n",
      "batch: 1858, loss: 0.31190985441207886\n",
      "batch: 1859, loss: 0.4671735465526581\n",
      "batch: 1860, loss: 0.2627134323120117\n",
      "batch: 1861, loss: 0.30095696449279785\n",
      "batch: 1862, loss: 0.23511387407779694\n",
      "batch: 1863, loss: 0.18418720364570618\n",
      "batch: 1864, loss: 0.31349992752075195\n",
      "batch: 1865, loss: 0.1884852647781372\n",
      "batch: 1866, loss: 0.20748460292816162\n",
      "batch: 1867, loss: 0.26585477590560913\n",
      "batch: 1868, loss: 0.5135324597358704\n",
      "batch: 1869, loss: 0.4217185974121094\n",
      "batch: 1870, loss: 0.3850436508655548\n",
      "batch: 1871, loss: 0.29780763387680054\n",
      "batch: 1872, loss: 0.3287709355354309\n",
      "batch: 1873, loss: 0.2594073712825775\n",
      "batch: 1874, loss: 0.18314366042613983\n",
      "batch: 1875, loss: 0.25705453753471375\n",
      "batch: 1876, loss: 0.3414444327354431\n",
      "batch: 1877, loss: 0.23544752597808838\n",
      "batch: 1878, loss: 0.27242690324783325\n",
      "batch: 1879, loss: 0.19191330671310425\n",
      "batch: 1880, loss: 0.40343111753463745\n",
      "batch: 1881, loss: 0.2821912169456482\n",
      "batch: 1882, loss: 0.3621840476989746\n",
      "batch: 1883, loss: 0.23722785711288452\n",
      "batch: 1884, loss: 0.20253919064998627\n",
      "batch: 1885, loss: 0.32313814759254456\n",
      "batch: 1886, loss: 0.1921207308769226\n",
      "batch: 1887, loss: 0.3824530243873596\n",
      "batch: 1888, loss: 0.19238737225532532\n",
      "batch: 1889, loss: 0.32511818408966064\n",
      "batch: 1890, loss: 0.2958166003227234\n",
      "batch: 1891, loss: 0.27561238408088684\n",
      "batch: 1892, loss: 0.24833133816719055\n",
      "batch: 1893, loss: 0.3468744158744812\n",
      "batch: 1894, loss: 0.47693923115730286\n",
      "batch: 1895, loss: 0.18999718129634857\n",
      "batch: 1896, loss: 0.225197896361351\n",
      "batch: 1897, loss: 0.3147321343421936\n",
      "batch: 1898, loss: 0.21845827996730804\n",
      "batch: 1899, loss: 0.4870397746562958\n",
      "batch: 1900, loss: 0.34932148456573486\n",
      "model saved to ./saving/model.ckpt-19\n",
      "batch: 1901, loss: 0.30226975679397583\n",
      "batch: 1902, loss: 0.3292122483253479\n",
      "batch: 1903, loss: 0.29863497614860535\n",
      "batch: 1904, loss: 0.45723265409469604\n",
      "batch: 1905, loss: 0.30350589752197266\n",
      "batch: 1906, loss: 0.4148402214050293\n",
      "batch: 1907, loss: 0.20235778391361237\n",
      "batch: 1908, loss: 0.2633163332939148\n",
      "batch: 1909, loss: 0.2922738790512085\n",
      "batch: 1910, loss: 0.26627522706985474\n",
      "batch: 1911, loss: 0.3231934905052185\n",
      "batch: 1912, loss: 0.2812599837779999\n",
      "batch: 1913, loss: 0.34731388092041016\n",
      "batch: 1914, loss: 0.3182864785194397\n",
      "batch: 1915, loss: 0.3579394221305847\n",
      "batch: 1916, loss: 0.3867051601409912\n",
      "batch: 1917, loss: 0.2858425974845886\n",
      "batch: 1918, loss: 0.2858731150627136\n",
      "batch: 1919, loss: 0.3013705015182495\n",
      "batch: 1920, loss: 0.267622709274292\n",
      "batch: 1921, loss: 0.252336323261261\n",
      "batch: 1922, loss: 0.29898929595947266\n",
      "batch: 1923, loss: 0.361663281917572\n",
      "batch: 1924, loss: 0.3472937047481537\n",
      "batch: 1925, loss: 0.32236599922180176\n",
      "batch: 1926, loss: 0.3935850262641907\n",
      "batch: 1927, loss: 0.2917703092098236\n",
      "batch: 1928, loss: 0.22325244545936584\n",
      "batch: 1929, loss: 0.20792923867702484\n",
      "batch: 1930, loss: 0.5432841777801514\n",
      "batch: 1931, loss: 0.18502329289913177\n",
      "batch: 1932, loss: 0.33354657888412476\n",
      "batch: 1933, loss: 0.33041778206825256\n",
      "batch: 1934, loss: 0.3175092339515686\n",
      "batch: 1935, loss: 0.28781628608703613\n",
      "batch: 1936, loss: 0.2740848958492279\n",
      "batch: 1937, loss: 0.2754071056842804\n",
      "batch: 1938, loss: 0.5023430585861206\n",
      "batch: 1939, loss: 0.14321428537368774\n",
      "batch: 1940, loss: 0.2406199872493744\n",
      "batch: 1941, loss: 0.42889589071273804\n",
      "batch: 1942, loss: 0.362916499376297\n",
      "batch: 1943, loss: 0.18983203172683716\n",
      "batch: 1944, loss: 0.2969371974468231\n",
      "batch: 1945, loss: 0.38540616631507874\n",
      "batch: 1946, loss: 0.23174451291561127\n",
      "batch: 1947, loss: 0.2717786133289337\n",
      "batch: 1948, loss: 0.25661128759384155\n",
      "batch: 1949, loss: 0.30460125207901\n",
      "batch: 1950, loss: 0.1968519687652588\n",
      "batch: 1951, loss: 0.40132278203964233\n",
      "batch: 1952, loss: 0.3576353192329407\n",
      "batch: 1953, loss: 0.27170470356941223\n",
      "batch: 1954, loss: 0.20479856431484222\n",
      "batch: 1955, loss: 0.5186454653739929\n",
      "batch: 1956, loss: 0.40368980169296265\n",
      "batch: 1957, loss: 0.2780494689941406\n",
      "batch: 1958, loss: 0.4230610728263855\n",
      "batch: 1959, loss: 0.31261318922042847\n",
      "batch: 1960, loss: 0.2615140378475189\n",
      "batch: 1961, loss: 0.2765841782093048\n",
      "batch: 1962, loss: 0.3877246677875519\n",
      "batch: 1963, loss: 0.3087822198867798\n",
      "batch: 1964, loss: 0.5460488200187683\n",
      "batch: 1965, loss: 0.334276020526886\n",
      "batch: 1966, loss: 0.234353706240654\n",
      "batch: 1967, loss: 0.5137987732887268\n",
      "batch: 1968, loss: 0.3263474702835083\n",
      "batch: 1969, loss: 0.2580450177192688\n",
      "batch: 1970, loss: 0.22920125722885132\n",
      "batch: 1971, loss: 0.2113116979598999\n",
      "batch: 1972, loss: 0.282340407371521\n",
      "batch: 1973, loss: 0.4343542456626892\n",
      "batch: 1974, loss: 0.44176676869392395\n",
      "batch: 1975, loss: 0.33201566338539124\n",
      "batch: 1976, loss: 0.3251692056655884\n",
      "batch: 1977, loss: 0.39000657200813293\n",
      "batch: 1978, loss: 0.3566000461578369\n",
      "batch: 1979, loss: 0.25934672355651855\n",
      "batch: 1980, loss: 0.2795895040035248\n",
      "batch: 1981, loss: 0.2491973340511322\n",
      "batch: 1982, loss: 0.39287135004997253\n",
      "batch: 1983, loss: 0.25819653272628784\n",
      "batch: 1984, loss: 0.3256649672985077\n",
      "batch: 1985, loss: 0.36908775568008423\n",
      "batch: 1986, loss: 0.3009576201438904\n",
      "batch: 1987, loss: 0.23160997033119202\n",
      "batch: 1988, loss: 0.3063545525074005\n",
      "batch: 1989, loss: 0.42813241481781006\n",
      "batch: 1990, loss: 0.1667523980140686\n",
      "batch: 1991, loss: 0.4129183888435364\n",
      "batch: 1992, loss: 0.3868999779224396\n",
      "batch: 1993, loss: 0.4895039498806\n",
      "batch: 1994, loss: 0.4877912402153015\n",
      "batch: 1995, loss: 0.3794485628604889\n",
      "batch: 1996, loss: 0.24427273869514465\n",
      "batch: 1997, loss: 0.3166213631629944\n",
      "batch: 1998, loss: 0.461455374956131\n",
      "batch: 1999, loss: 0.2890928089618683\n",
      "batch: 2000, loss: 0.36201566457748413\n",
      "model saved to ./saving/model.ckpt-20\n",
      "batch: 2001, loss: 0.30415529012680054\n",
      "batch: 2002, loss: 0.33085358142852783\n",
      "batch: 2003, loss: 0.28438839316368103\n",
      "batch: 2004, loss: 0.2816600501537323\n",
      "batch: 2005, loss: 0.45257604122161865\n",
      "batch: 2006, loss: 0.34949445724487305\n",
      "batch: 2007, loss: 0.16720780730247498\n",
      "batch: 2008, loss: 0.2582724094390869\n",
      "batch: 2009, loss: 0.3138226270675659\n",
      "batch: 2010, loss: 0.38630834221839905\n",
      "batch: 2011, loss: 0.24004270136356354\n",
      "batch: 2012, loss: 0.2614111602306366\n",
      "batch: 2013, loss: 0.2168414294719696\n",
      "batch: 2014, loss: 0.3888465464115143\n",
      "batch: 2015, loss: 0.47565680742263794\n",
      "batch: 2016, loss: 0.23567216098308563\n",
      "batch: 2017, loss: 0.26373279094696045\n",
      "batch: 2018, loss: 0.2360592782497406\n",
      "batch: 2019, loss: 0.33171674609184265\n",
      "batch: 2020, loss: 0.3484518527984619\n",
      "batch: 2021, loss: 0.26567381620407104\n",
      "batch: 2022, loss: 0.2537382245063782\n",
      "batch: 2023, loss: 0.33907538652420044\n",
      "batch: 2024, loss: 0.24430784583091736\n",
      "batch: 2025, loss: 0.3615092635154724\n",
      "batch: 2026, loss: 0.2586108446121216\n",
      "batch: 2027, loss: 0.2509100139141083\n",
      "batch: 2028, loss: 0.25501731038093567\n",
      "batch: 2029, loss: 0.33122754096984863\n",
      "batch: 2030, loss: 0.31178438663482666\n",
      "batch: 2031, loss: 0.3154030740261078\n",
      "batch: 2032, loss: 0.3159273564815521\n",
      "batch: 2033, loss: 0.4800177216529846\n",
      "batch: 2034, loss: 0.24931150674819946\n",
      "batch: 2035, loss: 0.2654499113559723\n",
      "batch: 2036, loss: 0.21397872269153595\n",
      "batch: 2037, loss: 0.2482418715953827\n",
      "batch: 2038, loss: 0.4387265145778656\n",
      "batch: 2039, loss: 0.5125033855438232\n",
      "batch: 2040, loss: 0.18649885058403015\n",
      "batch: 2041, loss: 0.21941541135311127\n",
      "batch: 2042, loss: 0.3539554476737976\n",
      "batch: 2043, loss: 0.1343448907136917\n",
      "batch: 2044, loss: 0.3313564658164978\n",
      "batch: 2045, loss: 0.3637216091156006\n",
      "batch: 2046, loss: 0.19067689776420593\n",
      "batch: 2047, loss: 0.30990439653396606\n",
      "batch: 2048, loss: 0.45375555753707886\n",
      "batch: 2049, loss: 0.32327800989151\n",
      "batch: 2050, loss: 0.2444494217634201\n",
      "batch: 2051, loss: 0.20483741164207458\n",
      "batch: 2052, loss: 0.2749466300010681\n",
      "batch: 2053, loss: 0.3791797161102295\n",
      "batch: 2054, loss: 0.4662986397743225\n",
      "batch: 2055, loss: 0.4794650673866272\n",
      "batch: 2056, loss: 0.15998172760009766\n",
      "batch: 2057, loss: 0.4431971311569214\n",
      "batch: 2058, loss: 0.24706289172172546\n",
      "batch: 2059, loss: 0.41721266508102417\n",
      "batch: 2060, loss: 0.38577404618263245\n",
      "batch: 2061, loss: 0.32987791299819946\n",
      "batch: 2062, loss: 0.41410884261131287\n",
      "batch: 2063, loss: 0.34236055612564087\n",
      "batch: 2064, loss: 0.1861891895532608\n",
      "batch: 2065, loss: 0.19625303149223328\n",
      "batch: 2066, loss: 0.1611877828836441\n",
      "batch: 2067, loss: 0.28128594160079956\n",
      "batch: 2068, loss: 0.27128294110298157\n",
      "batch: 2069, loss: 0.3318217992782593\n",
      "batch: 2070, loss: 0.2572477459907532\n",
      "batch: 2071, loss: 0.2400464564561844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 2072, loss: 0.35054710507392883\n",
      "batch: 2073, loss: 0.3516124486923218\n",
      "batch: 2074, loss: 0.2007768750190735\n",
      "batch: 2075, loss: 0.38252127170562744\n",
      "batch: 2076, loss: 0.3683021664619446\n",
      "batch: 2077, loss: 0.24579690396785736\n",
      "batch: 2078, loss: 0.3066501319408417\n",
      "batch: 2079, loss: 0.32735514640808105\n",
      "batch: 2080, loss: 0.462340772151947\n",
      "batch: 2081, loss: 0.5329163670539856\n",
      "batch: 2082, loss: 0.3253195881843567\n",
      "batch: 2083, loss: 0.26146602630615234\n",
      "batch: 2084, loss: 0.2876339256763458\n",
      "batch: 2085, loss: 0.2495589256286621\n",
      "batch: 2086, loss: 0.24674907326698303\n",
      "batch: 2087, loss: 0.2632083296775818\n",
      "batch: 2088, loss: 0.2811657786369324\n",
      "batch: 2089, loss: 0.2194734513759613\n",
      "batch: 2090, loss: 0.6746086478233337\n",
      "batch: 2091, loss: 0.35214173793792725\n",
      "batch: 2092, loss: 0.362463116645813\n",
      "batch: 2093, loss: 0.26721447706222534\n",
      "batch: 2094, loss: 0.37853842973709106\n",
      "batch: 2095, loss: 0.24302680790424347\n",
      "batch: 2096, loss: 0.20802000164985657\n",
      "batch: 2097, loss: 0.3774805963039398\n",
      "batch: 2098, loss: 0.2639915645122528\n",
      "batch: 2099, loss: 0.20803573727607727\n",
      "batch: 2100, loss: 0.30121293663978577\n",
      "model saved to ./saving/model.ckpt-21\n",
      "batch: 2101, loss: 0.32196223735809326\n",
      "batch: 2102, loss: 0.31022071838378906\n",
      "batch: 2103, loss: 0.28602978587150574\n",
      "batch: 2104, loss: 0.29542437195777893\n",
      "batch: 2105, loss: 0.3550170660018921\n",
      "batch: 2106, loss: 0.17508500814437866\n",
      "batch: 2107, loss: 0.2845868468284607\n",
      "batch: 2108, loss: 0.4249505400657654\n",
      "batch: 2109, loss: 0.3143305778503418\n",
      "batch: 2110, loss: 0.28502118587493896\n",
      "batch: 2111, loss: 0.258099228143692\n",
      "batch: 2112, loss: 0.23651152849197388\n",
      "batch: 2113, loss: 0.26228880882263184\n",
      "batch: 2114, loss: 0.24905772507190704\n",
      "batch: 2115, loss: 0.3163163363933563\n",
      "batch: 2116, loss: 0.39914557337760925\n",
      "batch: 2117, loss: 0.3207233250141144\n",
      "batch: 2118, loss: 0.30938875675201416\n",
      "batch: 2119, loss: 0.41544777154922485\n",
      "batch: 2120, loss: 0.322119802236557\n",
      "batch: 2121, loss: 0.2458738535642624\n",
      "batch: 2122, loss: 0.41362372040748596\n",
      "batch: 2123, loss: 0.37305206060409546\n",
      "batch: 2124, loss: 0.185799241065979\n",
      "batch: 2125, loss: 0.321900337934494\n",
      "batch: 2126, loss: 0.21695345640182495\n",
      "batch: 2127, loss: 0.2589637339115143\n",
      "batch: 2128, loss: 0.282055139541626\n",
      "batch: 2129, loss: 0.24786138534545898\n",
      "batch: 2130, loss: 0.3857147693634033\n",
      "batch: 2131, loss: 0.35644158720970154\n",
      "batch: 2132, loss: 0.37210896611213684\n",
      "batch: 2133, loss: 0.2697926461696625\n",
      "batch: 2134, loss: 0.31728655099868774\n",
      "batch: 2135, loss: 0.20370402932167053\n",
      "batch: 2136, loss: 0.30381736159324646\n",
      "batch: 2137, loss: 0.2761619985103607\n",
      "batch: 2138, loss: 0.2295457422733307\n",
      "batch: 2139, loss: 0.21174010634422302\n",
      "batch: 2140, loss: 0.3468843102455139\n",
      "batch: 2141, loss: 0.17986592650413513\n",
      "batch: 2142, loss: 0.32351356744766235\n",
      "batch: 2143, loss: 0.3652524948120117\n",
      "batch: 2144, loss: 0.3224402666091919\n",
      "batch: 2145, loss: 0.33628079295158386\n",
      "batch: 2146, loss: 0.3338344097137451\n",
      "batch: 2147, loss: 0.230643630027771\n",
      "batch: 2148, loss: 0.4161897897720337\n",
      "batch: 2149, loss: 0.28682926297187805\n",
      "batch: 2150, loss: 0.40720364451408386\n",
      "batch: 2151, loss: 0.1880282461643219\n",
      "batch: 2152, loss: 0.2957465350627899\n",
      "batch: 2153, loss: 0.22284051775932312\n",
      "batch: 2154, loss: 0.3496665060520172\n",
      "batch: 2155, loss: 0.2142469882965088\n",
      "batch: 2156, loss: 0.21719138324260712\n",
      "batch: 2157, loss: 0.2175212800502777\n",
      "batch: 2158, loss: 0.20748111605644226\n",
      "batch: 2159, loss: 0.5608253479003906\n",
      "batch: 2160, loss: 0.45655661821365356\n",
      "batch: 2161, loss: 0.26166394352912903\n",
      "batch: 2162, loss: 0.30603426694869995\n",
      "batch: 2163, loss: 0.29245197772979736\n",
      "batch: 2164, loss: 0.2331348955631256\n",
      "batch: 2165, loss: 0.35719889402389526\n",
      "batch: 2166, loss: 0.43247973918914795\n",
      "batch: 2167, loss: 0.39674779772758484\n",
      "batch: 2168, loss: 0.24343745410442352\n",
      "batch: 2169, loss: 0.37854570150375366\n",
      "batch: 2170, loss: 0.23098914325237274\n",
      "batch: 2171, loss: 0.28935354948043823\n",
      "batch: 2172, loss: 0.38015618920326233\n",
      "batch: 2173, loss: 0.4310194253921509\n",
      "batch: 2174, loss: 0.28944578766822815\n",
      "batch: 2175, loss: 0.3239459991455078\n",
      "batch: 2176, loss: 0.18960504233837128\n",
      "batch: 2177, loss: 0.2591891884803772\n",
      "batch: 2178, loss: 0.488129198551178\n",
      "batch: 2179, loss: 0.398529052734375\n",
      "batch: 2180, loss: 0.3712029755115509\n",
      "batch: 2181, loss: 0.26543471217155457\n",
      "batch: 2182, loss: 0.4739883542060852\n",
      "batch: 2183, loss: 0.3147216737270355\n",
      "batch: 2184, loss: 0.13917411863803864\n",
      "batch: 2185, loss: 0.17667295038700104\n",
      "batch: 2186, loss: 0.26703503727912903\n",
      "batch: 2187, loss: 0.22166982293128967\n",
      "batch: 2188, loss: 0.4262191653251648\n",
      "batch: 2189, loss: 0.18689972162246704\n",
      "batch: 2190, loss: 0.3060051202774048\n",
      "batch: 2191, loss: 0.2960878908634186\n",
      "batch: 2192, loss: 0.24355173110961914\n",
      "batch: 2193, loss: 0.2479235678911209\n",
      "batch: 2194, loss: 0.23089565336704254\n",
      "batch: 2195, loss: 0.25751057267189026\n",
      "batch: 2196, loss: 0.38177356123924255\n",
      "batch: 2197, loss: 0.22345076501369476\n",
      "batch: 2198, loss: 0.19789601862430573\n",
      "batch: 2199, loss: 0.28495681285858154\n",
      "batch: 2200, loss: 0.27097922563552856\n",
      "model saved to ./saving/model.ckpt-22\n",
      "batch: 2201, loss: 0.2654002606868744\n",
      "batch: 2202, loss: 0.29905256628990173\n",
      "batch: 2203, loss: 0.2880394756793976\n",
      "batch: 2204, loss: 0.30191028118133545\n",
      "batch: 2205, loss: 0.23007351160049438\n",
      "batch: 2206, loss: 0.2223542183637619\n",
      "batch: 2207, loss: 0.3027038872241974\n",
      "batch: 2208, loss: 0.2001500427722931\n",
      "batch: 2209, loss: 0.3924436569213867\n",
      "batch: 2210, loss: 0.22589494287967682\n",
      "batch: 2211, loss: 0.23491525650024414\n",
      "batch: 2212, loss: 0.2543565034866333\n",
      "batch: 2213, loss: 0.22059965133666992\n",
      "batch: 2214, loss: 0.23299166560173035\n",
      "batch: 2215, loss: 0.25371673703193665\n",
      "batch: 2216, loss: 0.368423730134964\n",
      "batch: 2217, loss: 0.27566930651664734\n",
      "batch: 2218, loss: 0.4079040288925171\n",
      "batch: 2219, loss: 0.2771044373512268\n",
      "batch: 2220, loss: 0.3601318597793579\n",
      "batch: 2221, loss: 0.3497917652130127\n",
      "batch: 2222, loss: 0.16859546303749084\n",
      "batch: 2223, loss: 0.2763667404651642\n",
      "batch: 2224, loss: 0.3322441279888153\n",
      "batch: 2225, loss: 0.22274687886238098\n",
      "batch: 2226, loss: 0.3761317729949951\n",
      "batch: 2227, loss: 0.19606056809425354\n",
      "batch: 2228, loss: 0.312732994556427\n",
      "batch: 2229, loss: 0.17448247969150543\n",
      "batch: 2230, loss: 0.38730257749557495\n",
      "batch: 2231, loss: 0.2524840235710144\n",
      "batch: 2232, loss: 0.3014136552810669\n",
      "batch: 2233, loss: 0.24915778636932373\n",
      "batch: 2234, loss: 0.3113335072994232\n",
      "batch: 2235, loss: 0.3511229157447815\n",
      "batch: 2236, loss: 0.3110507130622864\n",
      "batch: 2237, loss: 0.10368306189775467\n",
      "batch: 2238, loss: 0.4317864179611206\n",
      "batch: 2239, loss: 0.3180743455886841\n",
      "batch: 2240, loss: 0.17921815812587738\n",
      "batch: 2241, loss: 0.31071463227272034\n",
      "batch: 2242, loss: 0.34354275465011597\n",
      "batch: 2243, loss: 0.2589954733848572\n",
      "batch: 2244, loss: 0.3712644875049591\n",
      "batch: 2245, loss: 0.2594827711582184\n",
      "batch: 2246, loss: 0.3773031532764435\n",
      "batch: 2247, loss: 0.21247079968452454\n",
      "batch: 2248, loss: 0.2270556092262268\n",
      "batch: 2249, loss: 0.36231011152267456\n",
      "batch: 2250, loss: 0.25629812479019165\n",
      "batch: 2251, loss: 0.1745225340127945\n",
      "batch: 2252, loss: 0.27035441994667053\n",
      "batch: 2253, loss: 0.4563673138618469\n",
      "batch: 2254, loss: 0.15657833218574524\n",
      "batch: 2255, loss: 0.271014928817749\n",
      "batch: 2256, loss: 0.3167009651660919\n",
      "batch: 2257, loss: 0.19045744836330414\n",
      "batch: 2258, loss: 0.3599718511104584\n",
      "batch: 2259, loss: 0.2517456114292145\n",
      "batch: 2260, loss: 0.27408623695373535\n",
      "batch: 2261, loss: 0.17132321000099182\n",
      "batch: 2262, loss: 0.34195467829704285\n",
      "batch: 2263, loss: 0.42008161544799805\n",
      "batch: 2264, loss: 0.39850255846977234\n",
      "batch: 2265, loss: 0.35678526759147644\n",
      "batch: 2266, loss: 0.3709855377674103\n",
      "batch: 2267, loss: 0.22614586353302002\n",
      "batch: 2268, loss: 0.20148418843746185\n",
      "batch: 2269, loss: 0.2432268112897873\n",
      "batch: 2270, loss: 0.18735754489898682\n",
      "batch: 2271, loss: 0.20804134011268616\n",
      "batch: 2272, loss: 0.40911388397216797\n",
      "batch: 2273, loss: 0.2591562867164612\n",
      "batch: 2274, loss: 0.17954276502132416\n",
      "batch: 2275, loss: 0.24475719034671783\n",
      "batch: 2276, loss: 0.36188799142837524\n",
      "batch: 2277, loss: 0.2037445604801178\n",
      "batch: 2278, loss: 0.16874253749847412\n",
      "batch: 2279, loss: 0.3182518482208252\n",
      "batch: 2280, loss: 0.4395855963230133\n",
      "batch: 2281, loss: 0.1582231968641281\n",
      "batch: 2282, loss: 0.37322333455085754\n",
      "batch: 2283, loss: 0.18581324815750122\n",
      "batch: 2284, loss: 0.15622344613075256\n",
      "batch: 2285, loss: 0.3543407917022705\n",
      "batch: 2286, loss: 0.37320318818092346\n",
      "batch: 2287, loss: 0.19615674018859863\n",
      "batch: 2288, loss: 0.21839319169521332\n",
      "batch: 2289, loss: 0.3181910514831543\n",
      "batch: 2290, loss: 0.2989511489868164\n",
      "batch: 2291, loss: 0.29682251811027527\n",
      "batch: 2292, loss: 0.08722137659788132\n",
      "batch: 2293, loss: 0.21036818623542786\n",
      "batch: 2294, loss: 0.4052690267562866\n",
      "batch: 2295, loss: 0.2790352404117584\n",
      "batch: 2296, loss: 0.2489689141511917\n",
      "batch: 2297, loss: 0.2383517622947693\n",
      "batch: 2298, loss: 0.2551259994506836\n",
      "batch: 2299, loss: 0.25358107686042786\n",
      "batch: 2300, loss: 0.43582165241241455\n",
      "model saved to ./saving/model.ckpt-23\n",
      "batch: 2301, loss: 0.19341598451137543\n",
      "batch: 2302, loss: 0.203302800655365\n",
      "batch: 2303, loss: 0.3615550994873047\n",
      "batch: 2304, loss: 0.1891985833644867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 2305, loss: 0.3217175006866455\n",
      "batch: 2306, loss: 0.3436700105667114\n",
      "batch: 2307, loss: 0.23656688630580902\n",
      "batch: 2308, loss: 0.2993053197860718\n",
      "batch: 2309, loss: 0.2312716245651245\n",
      "batch: 2310, loss: 0.29982858896255493\n",
      "batch: 2311, loss: 0.3448296785354614\n",
      "batch: 2312, loss: 0.30931949615478516\n",
      "batch: 2313, loss: 0.3246552050113678\n",
      "batch: 2314, loss: 0.2700040340423584\n",
      "batch: 2315, loss: 0.49458155035972595\n",
      "batch: 2316, loss: 0.4107496440410614\n",
      "batch: 2317, loss: 0.35841166973114014\n",
      "batch: 2318, loss: 0.21197444200515747\n",
      "batch: 2319, loss: 0.29090654850006104\n",
      "batch: 2320, loss: 0.2602847218513489\n",
      "batch: 2321, loss: 0.2469237893819809\n",
      "batch: 2322, loss: 0.4195203185081482\n",
      "batch: 2323, loss: 0.16792044043540955\n",
      "batch: 2324, loss: 0.24913187325000763\n",
      "batch: 2325, loss: 0.1705826073884964\n",
      "batch: 2326, loss: 0.10891830176115036\n",
      "batch: 2327, loss: 0.32011085748672485\n",
      "batch: 2328, loss: 0.14345622062683105\n",
      "batch: 2329, loss: 0.3615524470806122\n",
      "batch: 2330, loss: 0.2516954839229584\n",
      "batch: 2331, loss: 0.35355663299560547\n",
      "batch: 2332, loss: 0.2341398298740387\n",
      "batch: 2333, loss: 0.1957245171070099\n",
      "batch: 2334, loss: 0.286821573972702\n",
      "batch: 2335, loss: 0.248416006565094\n",
      "batch: 2336, loss: 0.16296640038490295\n",
      "batch: 2337, loss: 0.3539372682571411\n",
      "batch: 2338, loss: 0.525539219379425\n",
      "batch: 2339, loss: 0.3032805919647217\n",
      "batch: 2340, loss: 0.3207108974456787\n",
      "batch: 2341, loss: 0.4540283679962158\n",
      "batch: 2342, loss: 0.32411104440689087\n",
      "batch: 2343, loss: 0.2946693003177643\n",
      "batch: 2344, loss: 0.4154776930809021\n",
      "batch: 2345, loss: 0.4091049134731293\n",
      "batch: 2346, loss: 0.29459184408187866\n",
      "batch: 2347, loss: 0.18867072463035583\n",
      "batch: 2348, loss: 0.2575194239616394\n",
      "batch: 2349, loss: 0.23874077200889587\n",
      "batch: 2350, loss: 0.32047975063323975\n",
      "batch: 2351, loss: 0.420084685087204\n",
      "batch: 2352, loss: 0.26912450790405273\n",
      "batch: 2353, loss: 0.24007272720336914\n",
      "batch: 2354, loss: 0.5303913354873657\n",
      "batch: 2355, loss: 0.26762253046035767\n",
      "batch: 2356, loss: 0.36345374584198\n",
      "batch: 2357, loss: 0.2939155697822571\n",
      "batch: 2358, loss: 0.24149096012115479\n",
      "batch: 2359, loss: 0.21767306327819824\n",
      "batch: 2360, loss: 0.26093876361846924\n",
      "batch: 2361, loss: 0.27426081895828247\n",
      "batch: 2362, loss: 0.4985073208808899\n",
      "batch: 2363, loss: 0.33541610836982727\n",
      "batch: 2364, loss: 0.23698237538337708\n",
      "batch: 2365, loss: 0.30544620752334595\n",
      "batch: 2366, loss: 0.3339571952819824\n",
      "batch: 2367, loss: 0.3111438453197479\n",
      "batch: 2368, loss: 0.33012163639068604\n",
      "batch: 2369, loss: 0.32710108160972595\n",
      "batch: 2370, loss: 0.3375484347343445\n",
      "batch: 2371, loss: 0.3211771547794342\n",
      "batch: 2372, loss: 0.3398241400718689\n",
      "batch: 2373, loss: 0.6544740796089172\n",
      "batch: 2374, loss: 0.19712963700294495\n",
      "batch: 2375, loss: 0.2184952199459076\n",
      "batch: 2376, loss: 0.20165280997753143\n",
      "batch: 2377, loss: 0.3699258267879486\n",
      "batch: 2378, loss: 0.18342456221580505\n",
      "batch: 2379, loss: 0.38405346870422363\n",
      "batch: 2380, loss: 0.37103864550590515\n",
      "batch: 2381, loss: 0.34612154960632324\n",
      "batch: 2382, loss: 0.24543672800064087\n",
      "batch: 2383, loss: 0.3094249367713928\n",
      "batch: 2384, loss: 0.17149481177330017\n",
      "batch: 2385, loss: 0.3890811800956726\n",
      "batch: 2386, loss: 0.4605258107185364\n",
      "batch: 2387, loss: 0.3205394446849823\n",
      "batch: 2388, loss: 0.2189738154411316\n",
      "batch: 2389, loss: 0.20444124937057495\n",
      "batch: 2390, loss: 0.2514117956161499\n",
      "batch: 2391, loss: 0.1942552626132965\n",
      "batch: 2392, loss: 0.3416934311389923\n",
      "batch: 2393, loss: 0.47060009837150574\n",
      "batch: 2394, loss: 0.22878298163414001\n",
      "batch: 2395, loss: 0.27902156114578247\n",
      "batch: 2396, loss: 0.2956465482711792\n",
      "batch: 2397, loss: 0.3307938575744629\n",
      "batch: 2398, loss: 0.4312629699707031\n",
      "batch: 2399, loss: 0.45565393567085266\n",
      "batch: 2400, loss: 0.15030911564826965\n",
      "model saved to ./saving/model.ckpt-24\n",
      "batch: 2401, loss: 0.24190810322761536\n",
      "batch: 2402, loss: 0.15586334466934204\n",
      "batch: 2403, loss: 0.3488003611564636\n",
      "batch: 2404, loss: 0.29936718940734863\n",
      "batch: 2405, loss: 0.23132412135601044\n",
      "batch: 2406, loss: 0.17656445503234863\n",
      "batch: 2407, loss: 0.12681987881660461\n",
      "batch: 2408, loss: 0.3999941647052765\n",
      "batch: 2409, loss: 0.19362090528011322\n",
      "batch: 2410, loss: 0.455628365278244\n",
      "batch: 2411, loss: 0.25937992334365845\n",
      "batch: 2412, loss: 0.286638081073761\n",
      "batch: 2413, loss: 0.26924556493759155\n",
      "batch: 2414, loss: 0.12819015979766846\n",
      "batch: 2415, loss: 0.3263683319091797\n",
      "batch: 2416, loss: 0.420916348695755\n",
      "batch: 2417, loss: 0.2845522165298462\n",
      "batch: 2418, loss: 0.23468685150146484\n",
      "batch: 2419, loss: 0.23236115276813507\n",
      "batch: 2420, loss: 0.29085198044776917\n",
      "batch: 2421, loss: 0.3052588105201721\n",
      "batch: 2422, loss: 0.2979598045349121\n",
      "batch: 2423, loss: 0.20237310230731964\n",
      "batch: 2424, loss: 0.19858437776565552\n",
      "batch: 2425, loss: 0.36101728677749634\n",
      "batch: 2426, loss: 0.2620547115802765\n",
      "batch: 2427, loss: 0.4101177453994751\n",
      "batch: 2428, loss: 0.22964243590831757\n",
      "batch: 2429, loss: 0.38916918635368347\n",
      "batch: 2430, loss: 0.23885749280452728\n",
      "batch: 2431, loss: 0.26465871930122375\n",
      "batch: 2432, loss: 0.2529046833515167\n",
      "batch: 2433, loss: 0.2718363106250763\n",
      "batch: 2434, loss: 0.24257241189479828\n",
      "batch: 2435, loss: 0.28426694869995117\n",
      "batch: 2436, loss: 0.1941482126712799\n",
      "batch: 2437, loss: 0.34677213430404663\n",
      "batch: 2438, loss: 0.45417261123657227\n",
      "batch: 2439, loss: 0.24038437008857727\n",
      "batch: 2440, loss: 0.31588906049728394\n",
      "batch: 2441, loss: 0.32283836603164673\n",
      "batch: 2442, loss: 0.2805655002593994\n",
      "batch: 2443, loss: 0.24045641720294952\n",
      "batch: 2444, loss: 0.3677150309085846\n",
      "batch: 2445, loss: 0.21745920181274414\n",
      "batch: 2446, loss: 0.281955748796463\n",
      "batch: 2447, loss: 0.16794848442077637\n",
      "batch: 2448, loss: 0.2430478185415268\n",
      "batch: 2449, loss: 0.25928425788879395\n",
      "batch: 2450, loss: 0.3260791599750519\n",
      "batch: 2451, loss: 0.15271791815757751\n",
      "batch: 2452, loss: 0.3188195526599884\n",
      "batch: 2453, loss: 0.4854672849178314\n",
      "batch: 2454, loss: 0.17320984601974487\n",
      "batch: 2455, loss: 0.29325786232948303\n",
      "batch: 2456, loss: 0.577470600605011\n",
      "batch: 2457, loss: 0.33761918544769287\n",
      "batch: 2458, loss: 0.2419351041316986\n",
      "batch: 2459, loss: 0.40594327449798584\n",
      "batch: 2460, loss: 0.25057393312454224\n",
      "batch: 2461, loss: 0.35629352927207947\n",
      "batch: 2462, loss: 0.39364948868751526\n",
      "batch: 2463, loss: 0.30483323335647583\n",
      "batch: 2464, loss: 0.26356595754623413\n",
      "batch: 2465, loss: 0.36378443241119385\n",
      "batch: 2466, loss: 0.24028843641281128\n",
      "batch: 2467, loss: 0.19955343008041382\n",
      "batch: 2468, loss: 0.37764543294906616\n",
      "batch: 2469, loss: 0.32899990677833557\n",
      "batch: 2470, loss: 0.5154458284378052\n",
      "batch: 2471, loss: 0.36910516023635864\n",
      "batch: 2472, loss: 0.44943171739578247\n",
      "batch: 2473, loss: 0.24123801290988922\n",
      "batch: 2474, loss: 0.30505791306495667\n",
      "batch: 2475, loss: 0.31091344356536865\n",
      "batch: 2476, loss: 0.2973822057247162\n",
      "batch: 2477, loss: 0.3201395273208618\n",
      "batch: 2478, loss: 0.25522929430007935\n",
      "batch: 2479, loss: 0.38524994254112244\n",
      "batch: 2480, loss: 0.34215086698532104\n",
      "batch: 2481, loss: 0.24228624999523163\n",
      "batch: 2482, loss: 0.36467990279197693\n",
      "batch: 2483, loss: 0.48904508352279663\n",
      "batch: 2484, loss: 0.23904870450496674\n",
      "batch: 2485, loss: 0.3649117946624756\n",
      "batch: 2486, loss: 0.33266782760620117\n",
      "batch: 2487, loss: 0.19131715595722198\n",
      "batch: 2488, loss: 0.18373480439186096\n",
      "batch: 2489, loss: 0.3282047510147095\n",
      "batch: 2490, loss: 0.22359247505664825\n",
      "batch: 2491, loss: 0.21788233518600464\n",
      "batch: 2492, loss: 0.3051357865333557\n",
      "batch: 2493, loss: 0.2673727869987488\n",
      "batch: 2494, loss: 0.2233053743839264\n",
      "batch: 2495, loss: 0.4500419497489929\n",
      "batch: 2496, loss: 0.3400513827800751\n",
      "batch: 2497, loss: 0.36298954486846924\n",
      "batch: 2498, loss: 0.194766566157341\n",
      "batch: 2499, loss: 0.17510497570037842\n",
      "batch: 2500, loss: 0.2316868156194687\n",
      "model saved to ./saving/model.ckpt-25\n",
      "batch: 2501, loss: 0.17351356148719788\n",
      "batch: 2502, loss: 0.30657702684402466\n",
      "batch: 2503, loss: 0.26799440383911133\n",
      "batch: 2504, loss: 0.3664383888244629\n",
      "batch: 2505, loss: 0.22110620141029358\n",
      "batch: 2506, loss: 0.20947790145874023\n",
      "batch: 2507, loss: 0.2731689214706421\n",
      "batch: 2508, loss: 0.2200547754764557\n",
      "batch: 2509, loss: 0.32401567697525024\n",
      "batch: 2510, loss: 0.19766473770141602\n",
      "batch: 2511, loss: 0.30457165837287903\n",
      "batch: 2512, loss: 0.2406485378742218\n",
      "batch: 2513, loss: 0.4143405556678772\n",
      "batch: 2514, loss: 0.14221139252185822\n",
      "batch: 2515, loss: 0.2786141633987427\n",
      "batch: 2516, loss: 0.27384522557258606\n",
      "batch: 2517, loss: 0.3584100902080536\n",
      "batch: 2518, loss: 0.2793899178504944\n",
      "batch: 2519, loss: 0.2769407033920288\n",
      "batch: 2520, loss: 0.15438072383403778\n",
      "batch: 2521, loss: 0.20389913022518158\n",
      "batch: 2522, loss: 0.3016096353530884\n",
      "batch: 2523, loss: 0.18515318632125854\n",
      "batch: 2524, loss: 0.24209259450435638\n",
      "batch: 2525, loss: 0.1567552089691162\n",
      "batch: 2526, loss: 0.335806280374527\n",
      "batch: 2527, loss: 0.22647321224212646\n",
      "batch: 2528, loss: 0.4270264506340027\n",
      "batch: 2529, loss: 0.3337959349155426\n",
      "batch: 2530, loss: 0.28882962465286255\n",
      "batch: 2531, loss: 0.1823604255914688\n",
      "batch: 2532, loss: 0.2436905801296234\n",
      "batch: 2533, loss: 0.2493661791086197\n",
      "batch: 2534, loss: 0.21675938367843628\n",
      "batch: 2535, loss: 0.30470988154411316\n",
      "batch: 2536, loss: 0.2478291243314743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 2537, loss: 0.4586019814014435\n",
      "batch: 2538, loss: 0.09680219739675522\n",
      "batch: 2539, loss: 0.3968152403831482\n",
      "batch: 2540, loss: 0.22684696316719055\n",
      "batch: 2541, loss: 0.35715389251708984\n",
      "batch: 2542, loss: 0.23747187852859497\n",
      "batch: 2543, loss: 0.17173635959625244\n",
      "batch: 2544, loss: 0.2901594936847687\n",
      "batch: 2545, loss: 0.25909268856048584\n",
      "batch: 2546, loss: 0.3026227355003357\n",
      "batch: 2547, loss: 0.21034057438373566\n",
      "batch: 2548, loss: 0.27037757635116577\n",
      "batch: 2549, loss: 0.15493153035640717\n",
      "batch: 2550, loss: 0.22295284271240234\n",
      "batch: 2551, loss: 0.16749531030654907\n",
      "batch: 2552, loss: 0.25370505452156067\n",
      "batch: 2553, loss: 0.18970806896686554\n",
      "batch: 2554, loss: 0.30339884757995605\n",
      "batch: 2555, loss: 0.28771770000457764\n",
      "batch: 2556, loss: 0.2897109389305115\n",
      "batch: 2557, loss: 0.19593986868858337\n",
      "batch: 2558, loss: 0.32175061106681824\n",
      "batch: 2559, loss: 0.42550355195999146\n",
      "batch: 2560, loss: 0.297673761844635\n",
      "batch: 2561, loss: 0.19829781353473663\n",
      "batch: 2562, loss: 0.22654837369918823\n",
      "batch: 2563, loss: 0.20665819942951202\n",
      "batch: 2564, loss: 0.19868044555187225\n",
      "batch: 2565, loss: 0.3763652443885803\n",
      "batch: 2566, loss: 0.23835420608520508\n",
      "batch: 2567, loss: 0.33304789662361145\n",
      "batch: 2568, loss: 0.4228869080543518\n",
      "batch: 2569, loss: 0.3615490794181824\n",
      "batch: 2570, loss: 0.2772064208984375\n",
      "batch: 2571, loss: 0.29219937324523926\n",
      "batch: 2572, loss: 0.32270634174346924\n",
      "batch: 2573, loss: 0.326100617647171\n",
      "batch: 2574, loss: 0.22520941495895386\n",
      "batch: 2575, loss: 0.6171428561210632\n",
      "batch: 2576, loss: 0.2948533892631531\n",
      "batch: 2577, loss: 0.28842100501060486\n",
      "batch: 2578, loss: 0.2875322699546814\n",
      "batch: 2579, loss: 0.26825645565986633\n",
      "batch: 2580, loss: 0.39594408869743347\n",
      "batch: 2581, loss: 0.26742440462112427\n",
      "batch: 2582, loss: 0.16289278864860535\n",
      "batch: 2583, loss: 0.318351686000824\n",
      "batch: 2584, loss: 0.3430172801017761\n",
      "batch: 2585, loss: 0.2955934703350067\n",
      "batch: 2586, loss: 0.2946147620677948\n",
      "batch: 2587, loss: 0.29177436232566833\n",
      "batch: 2588, loss: 0.1297638714313507\n",
      "batch: 2589, loss: 0.2981007993221283\n",
      "batch: 2590, loss: 0.19817328453063965\n",
      "batch: 2591, loss: 0.34277617931365967\n",
      "batch: 2592, loss: 0.3721073269844055\n",
      "batch: 2593, loss: 0.2729650139808655\n",
      "batch: 2594, loss: 0.22033078968524933\n",
      "batch: 2595, loss: 0.26310062408447266\n",
      "batch: 2596, loss: 0.19318024814128876\n",
      "batch: 2597, loss: 0.3143611550331116\n",
      "batch: 2598, loss: 0.4140874743461609\n",
      "batch: 2599, loss: 0.3604671061038971\n",
      "batch: 2600, loss: 0.2249595820903778\n",
      "model saved to ./saving/model.ckpt-26\n",
      "batch: 2601, loss: 0.31502556800842285\n",
      "batch: 2602, loss: 0.17316578328609467\n",
      "batch: 2603, loss: 0.15993547439575195\n",
      "batch: 2604, loss: 0.5886094570159912\n",
      "batch: 2605, loss: 0.34191182255744934\n",
      "batch: 2606, loss: 0.19732549786567688\n",
      "batch: 2607, loss: 0.16794371604919434\n",
      "batch: 2608, loss: 0.23620599508285522\n",
      "batch: 2609, loss: 0.2718944549560547\n",
      "batch: 2610, loss: 0.22374355792999268\n",
      "batch: 2611, loss: 0.3485187590122223\n",
      "batch: 2612, loss: 0.2221892923116684\n",
      "batch: 2613, loss: 0.2084045112133026\n",
      "batch: 2614, loss: 0.24506917595863342\n",
      "batch: 2615, loss: 0.3109360337257385\n",
      "batch: 2616, loss: 0.2285335212945938\n",
      "batch: 2617, loss: 0.2621188461780548\n",
      "batch: 2618, loss: 0.2482924461364746\n",
      "batch: 2619, loss: 0.28490543365478516\n",
      "batch: 2620, loss: 0.2631286680698395\n",
      "batch: 2621, loss: 0.252920925617218\n",
      "batch: 2622, loss: 0.29494187235832214\n",
      "batch: 2623, loss: 0.204254150390625\n",
      "batch: 2624, loss: 0.24193337559700012\n",
      "batch: 2625, loss: 0.26609793305397034\n",
      "batch: 2626, loss: 0.21160206198692322\n",
      "batch: 2627, loss: 0.28933775424957275\n",
      "batch: 2628, loss: 0.21539902687072754\n",
      "batch: 2629, loss: 0.446972519159317\n",
      "batch: 2630, loss: 0.3023129105567932\n",
      "batch: 2631, loss: 0.15800760686397552\n",
      "batch: 2632, loss: 0.14191824197769165\n",
      "batch: 2633, loss: 0.25092795491218567\n",
      "batch: 2634, loss: 0.14996860921382904\n",
      "batch: 2635, loss: 0.432747483253479\n",
      "batch: 2636, loss: 0.17758110165596008\n",
      "batch: 2637, loss: 0.24440497159957886\n",
      "batch: 2638, loss: 0.23230081796646118\n",
      "batch: 2639, loss: 0.3188552260398865\n",
      "batch: 2640, loss: 0.34945905208587646\n",
      "batch: 2641, loss: 0.3093148469924927\n",
      "batch: 2642, loss: 0.3095353841781616\n",
      "batch: 2643, loss: 0.26174384355545044\n",
      "batch: 2644, loss: 0.2569785714149475\n",
      "batch: 2645, loss: 0.27931758761405945\n",
      "batch: 2646, loss: 0.330993115901947\n",
      "batch: 2647, loss: 0.22489111125469208\n",
      "batch: 2648, loss: 0.36362147331237793\n",
      "batch: 2649, loss: 0.5027285814285278\n",
      "batch: 2650, loss: 0.24619394540786743\n",
      "batch: 2651, loss: 0.31281208992004395\n",
      "batch: 2652, loss: 0.20946049690246582\n",
      "batch: 2653, loss: 0.2882886826992035\n",
      "batch: 2654, loss: 0.26064324378967285\n",
      "batch: 2655, loss: 0.3174745738506317\n",
      "batch: 2656, loss: 0.23717933893203735\n",
      "batch: 2657, loss: 0.1683751940727234\n",
      "batch: 2658, loss: 0.219471275806427\n",
      "batch: 2659, loss: 0.40428346395492554\n",
      "batch: 2660, loss: 0.2847244143486023\n",
      "batch: 2661, loss: 0.23354896903038025\n",
      "batch: 2662, loss: 0.5151626467704773\n",
      "batch: 2663, loss: 0.385306179523468\n",
      "batch: 2664, loss: 0.2744731903076172\n",
      "batch: 2665, loss: 0.1506430208683014\n",
      "batch: 2666, loss: 0.3921857178211212\n",
      "batch: 2667, loss: 0.17797616124153137\n",
      "batch: 2668, loss: 0.32461655139923096\n",
      "batch: 2669, loss: 0.20737938582897186\n",
      "batch: 2670, loss: 0.2651367485523224\n",
      "batch: 2671, loss: 0.27963390946388245\n",
      "batch: 2672, loss: 0.16656793653964996\n",
      "batch: 2673, loss: 0.18996363878250122\n",
      "batch: 2674, loss: 0.30814027786254883\n",
      "batch: 2675, loss: 0.15024617314338684\n",
      "batch: 2676, loss: 0.2158854603767395\n",
      "batch: 2677, loss: 0.26392191648483276\n",
      "batch: 2678, loss: 0.2193675935268402\n",
      "batch: 2679, loss: 0.2297460436820984\n",
      "batch: 2680, loss: 0.22201742231845856\n",
      "batch: 2681, loss: 0.21597005426883698\n",
      "batch: 2682, loss: 0.25432685017585754\n",
      "batch: 2683, loss: 0.26932984590530396\n",
      "batch: 2684, loss: 0.18850581347942352\n",
      "batch: 2685, loss: 0.265206903219223\n",
      "batch: 2686, loss: 0.23969709873199463\n",
      "batch: 2687, loss: 0.5200345516204834\n",
      "batch: 2688, loss: 0.41162270307540894\n",
      "batch: 2689, loss: 0.33861732482910156\n",
      "batch: 2690, loss: 0.2821092903614044\n",
      "batch: 2691, loss: 0.1844957172870636\n",
      "batch: 2692, loss: 0.19384458661079407\n",
      "batch: 2693, loss: 0.2730149030685425\n",
      "batch: 2694, loss: 0.22117014229297638\n",
      "batch: 2695, loss: 0.15908870100975037\n",
      "batch: 2696, loss: 0.2696378231048584\n",
      "batch: 2697, loss: 0.2842562794685364\n",
      "batch: 2698, loss: 0.27576908469200134\n",
      "batch: 2699, loss: 0.3604665696620941\n",
      "batch: 2700, loss: 0.2290610373020172\n",
      "model saved to ./saving/model.ckpt-27\n",
      "batch: 2701, loss: 0.1962866634130478\n",
      "batch: 2702, loss: 0.12485820800065994\n",
      "batch: 2703, loss: 0.27885448932647705\n",
      "batch: 2704, loss: 0.1792452037334442\n",
      "batch: 2705, loss: 0.3659786880016327\n",
      "batch: 2706, loss: 0.29159867763519287\n",
      "batch: 2707, loss: 0.20275601744651794\n",
      "batch: 2708, loss: 0.2883789539337158\n",
      "batch: 2709, loss: 0.26440608501434326\n",
      "batch: 2710, loss: 0.30604875087738037\n",
      "batch: 2711, loss: 0.16362780332565308\n",
      "batch: 2712, loss: 0.15224045515060425\n",
      "batch: 2713, loss: 0.20587843656539917\n",
      "batch: 2714, loss: 0.2103578895330429\n",
      "batch: 2715, loss: 0.4521278738975525\n",
      "batch: 2716, loss: 0.4421423375606537\n",
      "batch: 2717, loss: 0.17902225255966187\n",
      "batch: 2718, loss: 0.34282398223876953\n",
      "batch: 2719, loss: 0.332472562789917\n",
      "batch: 2720, loss: 0.1824606955051422\n",
      "batch: 2721, loss: 0.30477166175842285\n",
      "batch: 2722, loss: 0.1331358700990677\n",
      "batch: 2723, loss: 0.16614316403865814\n",
      "batch: 2724, loss: 0.21744419634342194\n",
      "batch: 2725, loss: 0.16051828861236572\n",
      "batch: 2726, loss: 0.2726072669029236\n",
      "batch: 2727, loss: 0.3733801543712616\n",
      "batch: 2728, loss: 0.16400174796581268\n",
      "batch: 2729, loss: 0.2798842191696167\n",
      "batch: 2730, loss: 0.39543670415878296\n",
      "batch: 2731, loss: 0.2199087291955948\n",
      "batch: 2732, loss: 0.2371983528137207\n",
      "batch: 2733, loss: 0.21826425194740295\n",
      "batch: 2734, loss: 0.15090829133987427\n",
      "batch: 2735, loss: 0.17260918021202087\n",
      "batch: 2736, loss: 0.2735668420791626\n",
      "batch: 2737, loss: 0.12158390134572983\n",
      "batch: 2738, loss: 0.3562730848789215\n",
      "batch: 2739, loss: 0.3359335660934448\n",
      "batch: 2740, loss: 0.44663888216018677\n",
      "batch: 2741, loss: 0.24607183039188385\n",
      "batch: 2742, loss: 0.42039138078689575\n",
      "batch: 2743, loss: 0.2904492914676666\n",
      "batch: 2744, loss: 0.17292910814285278\n",
      "batch: 2745, loss: 0.27935415506362915\n",
      "batch: 2746, loss: 0.16941820085048676\n",
      "batch: 2747, loss: 0.2672944664955139\n",
      "batch: 2748, loss: 0.25076162815093994\n",
      "batch: 2749, loss: 0.2513626217842102\n",
      "batch: 2750, loss: 0.36272570490837097\n",
      "batch: 2751, loss: 0.2385067343711853\n",
      "batch: 2752, loss: 0.16504672169685364\n",
      "batch: 2753, loss: 0.2919102907180786\n",
      "batch: 2754, loss: 0.20218876004219055\n",
      "batch: 2755, loss: 0.1903155893087387\n",
      "batch: 2756, loss: 0.27473002672195435\n",
      "batch: 2757, loss: 0.22470343112945557\n",
      "batch: 2758, loss: 0.2877843379974365\n",
      "batch: 2759, loss: 0.2739102244377136\n",
      "batch: 2760, loss: 0.28868645429611206\n",
      "batch: 2761, loss: 0.22350385785102844\n",
      "batch: 2762, loss: 0.18404974043369293\n",
      "batch: 2763, loss: 0.4365069568157196\n",
      "batch: 2764, loss: 0.3438226878643036\n",
      "batch: 2765, loss: 0.18195001780986786\n",
      "batch: 2766, loss: 0.17384694516658783\n",
      "batch: 2767, loss: 0.19709512591362\n",
      "batch: 2768, loss: 0.27089253067970276\n",
      "batch: 2769, loss: 0.29616743326187134\n",
      "batch: 2770, loss: 0.2760332226753235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 2771, loss: 0.278891384601593\n",
      "batch: 2772, loss: 0.17905163764953613\n",
      "batch: 2773, loss: 0.207527756690979\n",
      "batch: 2774, loss: 0.392242431640625\n",
      "batch: 2775, loss: 0.222307488322258\n",
      "batch: 2776, loss: 0.3901023268699646\n",
      "batch: 2777, loss: 0.17077529430389404\n",
      "batch: 2778, loss: 0.19963741302490234\n",
      "batch: 2779, loss: 0.24419161677360535\n",
      "batch: 2780, loss: 0.26668673753738403\n",
      "batch: 2781, loss: 0.17210474610328674\n",
      "batch: 2782, loss: 0.27459967136383057\n",
      "batch: 2783, loss: 0.25432467460632324\n",
      "batch: 2784, loss: 0.23003312945365906\n",
      "batch: 2785, loss: 0.20053710043430328\n",
      "batch: 2786, loss: 0.3762759566307068\n",
      "batch: 2787, loss: 0.23679503798484802\n",
      "batch: 2788, loss: 0.3857583999633789\n",
      "batch: 2789, loss: 0.2613176107406616\n",
      "batch: 2790, loss: 0.2578117847442627\n",
      "batch: 2791, loss: 0.3244665265083313\n",
      "batch: 2792, loss: 0.25939691066741943\n",
      "batch: 2793, loss: 0.275776743888855\n",
      "batch: 2794, loss: 0.1336117535829544\n",
      "batch: 2795, loss: 0.2548918128013611\n",
      "batch: 2796, loss: 0.22834280133247375\n",
      "batch: 2797, loss: 0.3655078411102295\n",
      "batch: 2798, loss: 0.33422529697418213\n",
      "batch: 2799, loss: 0.11271059513092041\n",
      "batch: 2800, loss: 0.23166730999946594\n",
      "model saved to ./saving/model.ckpt-28\n",
      "batch: 2801, loss: 0.23023712635040283\n",
      "batch: 2802, loss: 0.38092365860939026\n",
      "batch: 2803, loss: 0.34321653842926025\n",
      "batch: 2804, loss: 0.3718358278274536\n",
      "batch: 2805, loss: 0.23331137001514435\n",
      "batch: 2806, loss: 0.26808804273605347\n",
      "batch: 2807, loss: 0.28458651900291443\n",
      "batch: 2808, loss: 0.19782443344593048\n",
      "batch: 2809, loss: 0.3124692440032959\n",
      "batch: 2810, loss: 0.3492305278778076\n",
      "batch: 2811, loss: 0.46272367238998413\n",
      "batch: 2812, loss: 0.3015640377998352\n",
      "batch: 2813, loss: 0.3513156771659851\n",
      "batch: 2814, loss: 0.17920415103435516\n",
      "batch: 2815, loss: 0.45124536752700806\n",
      "batch: 2816, loss: 0.2968989908695221\n",
      "batch: 2817, loss: 0.2612954378128052\n",
      "batch: 2818, loss: 0.24542421102523804\n",
      "batch: 2819, loss: 0.24654749035835266\n",
      "batch: 2820, loss: 0.21277214586734772\n",
      "batch: 2821, loss: 0.3208923935890198\n",
      "batch: 2822, loss: 0.23513123393058777\n",
      "batch: 2823, loss: 0.3090399205684662\n",
      "batch: 2824, loss: 0.2617631256580353\n",
      "batch: 2825, loss: 0.34849172830581665\n",
      "batch: 2826, loss: 0.16497869789600372\n",
      "batch: 2827, loss: 0.22171607613563538\n",
      "batch: 2828, loss: 0.2886546850204468\n",
      "batch: 2829, loss: 0.2528251111507416\n",
      "batch: 2830, loss: 0.24720671772956848\n",
      "batch: 2831, loss: 0.132016122341156\n",
      "batch: 2832, loss: 0.36150264739990234\n",
      "batch: 2833, loss: 0.2398257702589035\n",
      "batch: 2834, loss: 0.3432157635688782\n",
      "batch: 2835, loss: 0.1893753856420517\n",
      "batch: 2836, loss: 0.2033233344554901\n",
      "batch: 2837, loss: 0.2694264054298401\n",
      "batch: 2838, loss: 0.21265867352485657\n",
      "batch: 2839, loss: 0.2025250792503357\n",
      "batch: 2840, loss: 0.37559622526168823\n",
      "batch: 2841, loss: 0.2756028473377228\n",
      "batch: 2842, loss: 0.2058975100517273\n",
      "batch: 2843, loss: 0.2243008017539978\n",
      "batch: 2844, loss: 0.2216334044933319\n",
      "batch: 2845, loss: 0.3830539584159851\n",
      "batch: 2846, loss: 0.31220972537994385\n",
      "batch: 2847, loss: 0.44654107093811035\n",
      "batch: 2848, loss: 0.11007753014564514\n",
      "batch: 2849, loss: 0.1765873283147812\n",
      "batch: 2850, loss: 0.16822044551372528\n",
      "batch: 2851, loss: 0.1993490606546402\n",
      "batch: 2852, loss: 0.24358680844306946\n",
      "batch: 2853, loss: 0.3357647657394409\n",
      "batch: 2854, loss: 0.16223251819610596\n",
      "batch: 2855, loss: 0.22520317137241364\n",
      "batch: 2856, loss: 0.33062607049942017\n",
      "batch: 2857, loss: 0.25422099232673645\n",
      "batch: 2858, loss: 0.3754410743713379\n",
      "batch: 2859, loss: 0.27222123742103577\n",
      "batch: 2860, loss: 0.22136718034744263\n",
      "batch: 2861, loss: 0.2120802253484726\n",
      "batch: 2862, loss: 0.2379072904586792\n",
      "batch: 2863, loss: 0.16190695762634277\n",
      "batch: 2864, loss: 0.22535592317581177\n",
      "batch: 2865, loss: 0.19742947816848755\n",
      "batch: 2866, loss: 0.2905818223953247\n",
      "batch: 2867, loss: 0.34532445669174194\n",
      "batch: 2868, loss: 0.2943044900894165\n",
      "batch: 2869, loss: 0.2724420726299286\n",
      "batch: 2870, loss: 0.21095946431159973\n",
      "batch: 2871, loss: 0.31727129220962524\n",
      "batch: 2872, loss: 0.23466713726520538\n",
      "batch: 2873, loss: 0.24044661223888397\n",
      "batch: 2874, loss: 0.20702122151851654\n",
      "batch: 2875, loss: 0.25784164667129517\n",
      "batch: 2876, loss: 0.2033998966217041\n",
      "batch: 2877, loss: 0.22630774974822998\n",
      "batch: 2878, loss: 0.34176379442214966\n",
      "batch: 2879, loss: 0.47467291355133057\n",
      "batch: 2880, loss: 0.38871192932128906\n",
      "batch: 2881, loss: 0.14574271440505981\n",
      "batch: 2882, loss: 0.12042389810085297\n",
      "batch: 2883, loss: 0.1760241836309433\n",
      "batch: 2884, loss: 0.22801142930984497\n",
      "batch: 2885, loss: 0.3477970063686371\n",
      "batch: 2886, loss: 0.17697389423847198\n",
      "batch: 2887, loss: 0.3239327669143677\n",
      "batch: 2888, loss: 0.33789634704589844\n",
      "batch: 2889, loss: 0.23591096699237823\n",
      "batch: 2890, loss: 0.2183075249195099\n",
      "batch: 2891, loss: 0.31047898530960083\n",
      "batch: 2892, loss: 0.1283779740333557\n",
      "batch: 2893, loss: 0.2662492096424103\n",
      "batch: 2894, loss: 0.369089275598526\n",
      "batch: 2895, loss: 0.44536587595939636\n",
      "batch: 2896, loss: 0.3872170150279999\n",
      "batch: 2897, loss: 0.2753860354423523\n",
      "batch: 2898, loss: 0.2711312472820282\n",
      "batch: 2899, loss: 0.16361433267593384\n",
      "batch: 2900, loss: 0.38842302560806274\n",
      "model saved to ./saving/model.ckpt-29\n",
      "batch: 2901, loss: 0.3560601472854614\n",
      "batch: 2902, loss: 0.3045751750469208\n",
      "batch: 2903, loss: 0.11316319555044174\n",
      "batch: 2904, loss: 0.31280410289764404\n",
      "batch: 2905, loss: 0.1832374930381775\n",
      "batch: 2906, loss: 0.35930728912353516\n",
      "batch: 2907, loss: 0.3501511812210083\n",
      "batch: 2908, loss: 0.18932591378688812\n",
      "batch: 2909, loss: 0.24855011701583862\n",
      "batch: 2910, loss: 0.18822132050991058\n",
      "batch: 2911, loss: 0.29473239183425903\n",
      "batch: 2912, loss: 0.19410967826843262\n",
      "batch: 2913, loss: 0.42538440227508545\n",
      "batch: 2914, loss: 0.20829951763153076\n",
      "batch: 2915, loss: 0.15172237157821655\n",
      "batch: 2916, loss: 0.43717247247695923\n",
      "batch: 2917, loss: 0.25926947593688965\n",
      "batch: 2918, loss: 0.19921469688415527\n",
      "batch: 2919, loss: 0.3422914147377014\n",
      "batch: 2920, loss: 0.28297916054725647\n",
      "batch: 2921, loss: 0.17562302947044373\n",
      "batch: 2922, loss: 0.312811017036438\n",
      "batch: 2923, loss: 0.2564772665500641\n",
      "batch: 2924, loss: 0.27093687653541565\n",
      "batch: 2925, loss: 0.25107696652412415\n",
      "batch: 2926, loss: 0.13784322142601013\n",
      "batch: 2927, loss: 0.5151495933532715\n",
      "batch: 2928, loss: 0.27695658802986145\n",
      "batch: 2929, loss: 0.2447250485420227\n",
      "batch: 2930, loss: 0.1962367296218872\n",
      "batch: 2931, loss: 0.33441925048828125\n",
      "batch: 2932, loss: 0.32977330684661865\n",
      "batch: 2933, loss: 0.42667317390441895\n",
      "batch: 2934, loss: 0.33795100450515747\n",
      "batch: 2935, loss: 0.16913726925849915\n",
      "batch: 2936, loss: 0.2293383777141571\n",
      "batch: 2937, loss: 0.22389572858810425\n",
      "batch: 2938, loss: 0.3621790111064911\n",
      "batch: 2939, loss: 0.2527749240398407\n",
      "batch: 2940, loss: 0.27894699573516846\n",
      "batch: 2941, loss: 0.35452669858932495\n",
      "batch: 2942, loss: 0.30596622824668884\n",
      "batch: 2943, loss: 0.32933762669563293\n",
      "batch: 2944, loss: 0.29016977548599243\n",
      "batch: 2945, loss: 0.28184378147125244\n",
      "batch: 2946, loss: 0.3357519507408142\n",
      "batch: 2947, loss: 0.3484842777252197\n",
      "batch: 2948, loss: 0.1607423573732376\n",
      "batch: 2949, loss: 0.46480220556259155\n",
      "batch: 2950, loss: 0.30767446756362915\n",
      "batch: 2951, loss: 0.22521083056926727\n",
      "batch: 2952, loss: 0.32715001702308655\n",
      "batch: 2953, loss: 0.17459522187709808\n",
      "batch: 2954, loss: 0.4024115204811096\n",
      "batch: 2955, loss: 0.34069085121154785\n",
      "batch: 2956, loss: 0.19889208674430847\n",
      "batch: 2957, loss: 0.43629616498947144\n",
      "batch: 2958, loss: 0.21040958166122437\n",
      "batch: 2959, loss: 0.32527875900268555\n",
      "batch: 2960, loss: 0.3126639127731323\n",
      "batch: 2961, loss: 0.19409798085689545\n",
      "batch: 2962, loss: 0.22466503083705902\n",
      "batch: 2963, loss: 0.174406498670578\n",
      "batch: 2964, loss: 0.17906367778778076\n",
      "batch: 2965, loss: 0.3067778944969177\n",
      "batch: 2966, loss: 0.20051944255828857\n",
      "batch: 2967, loss: 0.1732243001461029\n",
      "batch: 2968, loss: 0.3034723699092865\n",
      "batch: 2969, loss: 0.2652125358581543\n",
      "batch: 2970, loss: 0.21899709105491638\n",
      "batch: 2971, loss: 0.3483038544654846\n",
      "batch: 2972, loss: 0.2757686376571655\n",
      "batch: 2973, loss: 0.3838544487953186\n",
      "batch: 2974, loss: 0.13119597733020782\n",
      "batch: 2975, loss: 0.19463790953159332\n",
      "batch: 2976, loss: 0.3261932134628296\n",
      "batch: 2977, loss: 0.23555266857147217\n",
      "batch: 2978, loss: 0.30329662561416626\n",
      "batch: 2979, loss: 0.3670479357242584\n",
      "batch: 2980, loss: 0.3241155743598938\n",
      "batch: 2981, loss: 0.407190203666687\n",
      "batch: 2982, loss: 0.26389700174331665\n",
      "batch: 2983, loss: 0.29599589109420776\n",
      "batch: 2984, loss: 0.2745957374572754\n",
      "batch: 2985, loss: 0.22916898131370544\n",
      "batch: 2986, loss: 0.21138623356819153\n",
      "batch: 2987, loss: 0.2725867033004761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 2988, loss: 0.259324312210083\n",
      "batch: 2989, loss: 0.47576776146888733\n",
      "batch: 2990, loss: 0.28705742955207825\n",
      "batch: 2991, loss: 0.25310564041137695\n",
      "batch: 2992, loss: 0.2720400094985962\n",
      "batch: 2993, loss: 0.23094075918197632\n",
      "batch: 2994, loss: 0.16838869452476501\n",
      "batch: 2995, loss: 0.2931753993034363\n",
      "batch: 2996, loss: 0.3301936984062195\n",
      "batch: 2997, loss: 0.2847937345504761\n",
      "batch: 2998, loss: 0.242509126663208\n",
      "batch: 2999, loss: 0.2923208475112915\n",
      "batch: 3000, loss: 0.11960867047309875\n",
      "model saved to ./saving/model.ckpt-30\n",
      "batch: 3001, loss: 0.2246484011411667\n",
      "batch: 3002, loss: 0.12183451652526855\n",
      "batch: 3003, loss: 0.21591493487358093\n",
      "batch: 3004, loss: 0.41203945875167847\n",
      "batch: 3005, loss: 0.147917702794075\n",
      "batch: 3006, loss: 0.20896992087364197\n",
      "batch: 3007, loss: 0.21382580697536469\n",
      "batch: 3008, loss: 0.16515013575553894\n",
      "batch: 3009, loss: 0.45736244320869446\n",
      "batch: 3010, loss: 0.27165529131889343\n",
      "batch: 3011, loss: 0.2789115309715271\n",
      "batch: 3012, loss: 0.2839798927307129\n",
      "batch: 3013, loss: 0.16020722687244415\n",
      "batch: 3014, loss: 0.36402061581611633\n",
      "batch: 3015, loss: 0.22800114750862122\n",
      "batch: 3016, loss: 0.1611267328262329\n",
      "batch: 3017, loss: 0.18309524655342102\n",
      "batch: 3018, loss: 0.15465199947357178\n",
      "batch: 3019, loss: 0.153815358877182\n",
      "batch: 3020, loss: 0.24401332437992096\n",
      "batch: 3021, loss: 0.2410311996936798\n",
      "batch: 3022, loss: 0.22010871767997742\n",
      "batch: 3023, loss: 0.12143324315547943\n",
      "batch: 3024, loss: 0.26525089144706726\n",
      "batch: 3025, loss: 0.3577602207660675\n",
      "batch: 3026, loss: 0.26146772503852844\n",
      "batch: 3027, loss: 0.18313787877559662\n",
      "batch: 3028, loss: 0.21966883540153503\n",
      "batch: 3029, loss: 0.32132941484451294\n",
      "batch: 3030, loss: 0.18383154273033142\n",
      "batch: 3031, loss: 0.23317831754684448\n",
      "batch: 3032, loss: 0.5243754982948303\n",
      "batch: 3033, loss: 0.2682381868362427\n",
      "batch: 3034, loss: 0.2575283646583557\n",
      "batch: 3035, loss: 0.09038572013378143\n",
      "batch: 3036, loss: 0.14676563441753387\n",
      "batch: 3037, loss: 0.20321862399578094\n",
      "batch: 3038, loss: 0.22226408123970032\n",
      "batch: 3039, loss: 0.18827274441719055\n",
      "batch: 3040, loss: 0.20026731491088867\n",
      "batch: 3041, loss: 0.21389922499656677\n",
      "batch: 3042, loss: 0.3559836745262146\n",
      "batch: 3043, loss: 0.21924912929534912\n",
      "batch: 3044, loss: 0.3988000750541687\n",
      "batch: 3045, loss: 0.3719077706336975\n",
      "batch: 3046, loss: 0.3859662115573883\n",
      "batch: 3047, loss: 0.3121667802333832\n",
      "batch: 3048, loss: 0.15359070897102356\n",
      "batch: 3049, loss: 0.16791899502277374\n",
      "batch: 3050, loss: 0.27701687812805176\n",
      "batch: 3051, loss: 0.4012821912765503\n",
      "batch: 3052, loss: 0.22357752919197083\n",
      "batch: 3053, loss: 0.32011014223098755\n",
      "batch: 3054, loss: 0.15731662511825562\n",
      "batch: 3055, loss: 0.2838382124900818\n",
      "batch: 3056, loss: 0.2754238545894623\n",
      "batch: 3057, loss: 0.13253730535507202\n",
      "batch: 3058, loss: 0.16135674715042114\n",
      "batch: 3059, loss: 0.2829163074493408\n",
      "batch: 3060, loss: 0.40010136365890503\n",
      "batch: 3061, loss: 0.20657414197921753\n",
      "batch: 3062, loss: 0.2790849804878235\n",
      "batch: 3063, loss: 0.10753114521503448\n",
      "batch: 3064, loss: 0.17447052896022797\n",
      "batch: 3065, loss: 0.36614394187927246\n",
      "batch: 3066, loss: 0.20903411507606506\n",
      "batch: 3067, loss: 0.3030323386192322\n",
      "batch: 3068, loss: 0.20588260889053345\n",
      "batch: 3069, loss: 0.3472168445587158\n",
      "batch: 3070, loss: 0.25027284026145935\n",
      "batch: 3071, loss: 0.24818196892738342\n",
      "batch: 3072, loss: 0.29307103157043457\n",
      "batch: 3073, loss: 0.25352099537849426\n",
      "batch: 3074, loss: 0.2827460467815399\n",
      "batch: 3075, loss: 0.3250362277030945\n",
      "batch: 3076, loss: 0.38362306356430054\n",
      "batch: 3077, loss: 0.1785554140806198\n",
      "batch: 3078, loss: 0.21205481886863708\n",
      "batch: 3079, loss: 0.2595517635345459\n",
      "batch: 3080, loss: 0.28946182131767273\n",
      "batch: 3081, loss: 0.4121095538139343\n",
      "batch: 3082, loss: 0.20538237690925598\n",
      "batch: 3083, loss: 0.24583196640014648\n",
      "batch: 3084, loss: 0.19630685448646545\n",
      "batch: 3085, loss: 0.34934890270233154\n",
      "batch: 3086, loss: 0.18806135654449463\n",
      "batch: 3087, loss: 0.3200641870498657\n",
      "batch: 3088, loss: 0.2511260509490967\n",
      "batch: 3089, loss: 0.28053581714630127\n",
      "batch: 3090, loss: 0.1986585110425949\n",
      "batch: 3091, loss: 0.32461076974868774\n",
      "batch: 3092, loss: 0.1540193259716034\n",
      "batch: 3093, loss: 0.25022009015083313\n",
      "batch: 3094, loss: 0.2414913922548294\n",
      "batch: 3095, loss: 0.23180022835731506\n",
      "batch: 3096, loss: 0.27828362584114075\n",
      "batch: 3097, loss: 0.3134532570838928\n",
      "batch: 3098, loss: 0.23290087282657623\n",
      "batch: 3099, loss: 0.3271638751029968\n",
      "batch: 3100, loss: 0.15950243175029755\n",
      "model saved to ./saving/model.ckpt-31\n",
      "batch: 3101, loss: 0.19589808583259583\n",
      "batch: 3102, loss: 0.16203737258911133\n",
      "batch: 3103, loss: 0.37078234553337097\n",
      "batch: 3104, loss: 0.16934868693351746\n",
      "batch: 3105, loss: 0.314619243144989\n",
      "batch: 3106, loss: 0.3257918953895569\n",
      "batch: 3107, loss: 0.15168902277946472\n",
      "batch: 3108, loss: 0.20232389867305756\n",
      "batch: 3109, loss: 0.3615313172340393\n",
      "batch: 3110, loss: 0.16861122846603394\n",
      "batch: 3111, loss: 0.18134371936321259\n",
      "batch: 3112, loss: 0.3281406760215759\n",
      "batch: 3113, loss: 0.22589001059532166\n",
      "batch: 3114, loss: 0.24890369176864624\n",
      "batch: 3115, loss: 0.12694495916366577\n",
      "batch: 3116, loss: 0.22250482439994812\n",
      "batch: 3117, loss: 0.2503886818885803\n",
      "batch: 3118, loss: 0.1172490119934082\n",
      "batch: 3119, loss: 0.42748531699180603\n",
      "batch: 3120, loss: 0.16997525095939636\n",
      "batch: 3121, loss: 0.43229711055755615\n",
      "batch: 3122, loss: 0.24326303601264954\n",
      "batch: 3123, loss: 0.20750215649604797\n",
      "batch: 3124, loss: 0.23227277398109436\n",
      "batch: 3125, loss: 0.22740326821804047\n",
      "batch: 3126, loss: 0.19095462560653687\n",
      "batch: 3127, loss: 0.28760212659835815\n",
      "batch: 3128, loss: 0.324307382106781\n",
      "batch: 3129, loss: 0.21644704043865204\n",
      "batch: 3130, loss: 0.19162365794181824\n",
      "batch: 3131, loss: 0.21227508783340454\n",
      "batch: 3132, loss: 0.26020556688308716\n",
      "batch: 3133, loss: 0.20172417163848877\n",
      "batch: 3134, loss: 0.20464864373207092\n",
      "batch: 3135, loss: 0.11817081272602081\n",
      "batch: 3136, loss: 0.4137037992477417\n",
      "batch: 3137, loss: 0.24288302659988403\n",
      "batch: 3138, loss: 0.17822660505771637\n",
      "batch: 3139, loss: 0.4204522371292114\n",
      "batch: 3140, loss: 0.1823829710483551\n",
      "batch: 3141, loss: 0.3527253568172455\n",
      "batch: 3142, loss: 0.2113020122051239\n",
      "batch: 3143, loss: 0.1466614454984665\n",
      "batch: 3144, loss: 0.35177791118621826\n",
      "batch: 3145, loss: 0.3246198296546936\n",
      "batch: 3146, loss: 0.17179469764232635\n",
      "batch: 3147, loss: 0.3384849429130554\n",
      "batch: 3148, loss: 0.13729295134544373\n",
      "batch: 3149, loss: 0.2518933117389679\n",
      "batch: 3150, loss: 0.26732078194618225\n",
      "batch: 3151, loss: 0.28411388397216797\n",
      "batch: 3152, loss: 0.24724960327148438\n",
      "batch: 3153, loss: 0.18036088347434998\n",
      "batch: 3154, loss: 0.2126443237066269\n",
      "batch: 3155, loss: 0.18511632084846497\n",
      "batch: 3156, loss: 0.2080172747373581\n",
      "batch: 3157, loss: 0.37589332461357117\n",
      "batch: 3158, loss: 0.37424570322036743\n",
      "batch: 3159, loss: 0.15856537222862244\n",
      "batch: 3160, loss: 0.1849944144487381\n",
      "batch: 3161, loss: 0.2530607283115387\n",
      "batch: 3162, loss: 0.2646380662918091\n",
      "batch: 3163, loss: 0.17762678861618042\n",
      "batch: 3164, loss: 0.24110133945941925\n",
      "batch: 3165, loss: 0.24687841534614563\n",
      "batch: 3166, loss: 0.3751755654811859\n",
      "batch: 3167, loss: 0.24445170164108276\n",
      "batch: 3168, loss: 0.22865518927574158\n",
      "batch: 3169, loss: 0.11808386445045471\n",
      "batch: 3170, loss: 0.3218404948711395\n",
      "batch: 3171, loss: 0.19549840688705444\n",
      "batch: 3172, loss: 0.26275500655174255\n",
      "batch: 3173, loss: 0.18333004415035248\n",
      "batch: 3174, loss: 0.3351389765739441\n",
      "batch: 3175, loss: 0.1889437884092331\n",
      "batch: 3176, loss: 0.17088991403579712\n",
      "batch: 3177, loss: 0.23185986280441284\n",
      "batch: 3178, loss: 0.31443676352500916\n",
      "batch: 3179, loss: 0.18768978118896484\n",
      "batch: 3180, loss: 0.3566962480545044\n",
      "batch: 3181, loss: 0.106550432741642\n",
      "batch: 3182, loss: 0.321586012840271\n",
      "batch: 3183, loss: 0.2527415156364441\n",
      "batch: 3184, loss: 0.27205392718315125\n",
      "batch: 3185, loss: 0.2625572681427002\n",
      "batch: 3186, loss: 0.2237498015165329\n",
      "batch: 3187, loss: 0.27343830466270447\n",
      "batch: 3188, loss: 0.13833053410053253\n",
      "batch: 3189, loss: 0.33652740716934204\n",
      "batch: 3190, loss: 0.09919473528862\n",
      "batch: 3191, loss: 0.2755559980869293\n",
      "batch: 3192, loss: 0.15559729933738708\n",
      "batch: 3193, loss: 0.3441810607910156\n",
      "batch: 3194, loss: 0.3471899628639221\n",
      "batch: 3195, loss: 0.41386598348617554\n",
      "batch: 3196, loss: 0.22260001301765442\n",
      "batch: 3197, loss: 0.2684972584247589\n",
      "batch: 3198, loss: 0.16341593861579895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 3199, loss: 0.24737662076950073\n",
      "batch: 3200, loss: 0.19664137065410614\n",
      "model saved to ./saving/model.ckpt-32\n",
      "batch: 3201, loss: 0.2595599591732025\n",
      "batch: 3202, loss: 0.3918686509132385\n",
      "batch: 3203, loss: 0.24252726137638092\n",
      "batch: 3204, loss: 0.34710103273391724\n",
      "batch: 3205, loss: 0.3709973692893982\n",
      "batch: 3206, loss: 0.14819572865962982\n",
      "batch: 3207, loss: 0.33529549837112427\n",
      "batch: 3208, loss: 0.26667895913124084\n",
      "batch: 3209, loss: 0.17318379878997803\n",
      "batch: 3210, loss: 0.41103875637054443\n",
      "batch: 3211, loss: 0.2816035747528076\n",
      "batch: 3212, loss: 0.12529820203781128\n",
      "batch: 3213, loss: 0.44362685084342957\n",
      "batch: 3214, loss: 0.17581161856651306\n",
      "batch: 3215, loss: 0.1476742923259735\n",
      "batch: 3216, loss: 0.31260526180267334\n",
      "batch: 3217, loss: 0.2661226987838745\n",
      "batch: 3218, loss: 0.207773357629776\n",
      "batch: 3219, loss: 0.28383326530456543\n",
      "batch: 3220, loss: 0.36403217911720276\n",
      "batch: 3221, loss: 0.2880089282989502\n",
      "batch: 3222, loss: 0.24028751254081726\n",
      "batch: 3223, loss: 0.14884772896766663\n",
      "batch: 3224, loss: 0.29913145303726196\n",
      "batch: 3225, loss: 0.36115363240242004\n",
      "batch: 3226, loss: 0.2977977991104126\n",
      "batch: 3227, loss: 0.23125793039798737\n",
      "batch: 3228, loss: 0.19650596380233765\n",
      "batch: 3229, loss: 0.4712570309638977\n",
      "batch: 3230, loss: 0.2990420162677765\n",
      "batch: 3231, loss: 0.11582578718662262\n",
      "batch: 3232, loss: 0.29445379972457886\n",
      "batch: 3233, loss: 0.2084072232246399\n",
      "batch: 3234, loss: 0.19199493527412415\n",
      "batch: 3235, loss: 0.32034218311309814\n",
      "batch: 3236, loss: 0.20587515830993652\n",
      "batch: 3237, loss: 0.4083784818649292\n",
      "batch: 3238, loss: 0.3235754668712616\n",
      "batch: 3239, loss: 0.18099577724933624\n",
      "batch: 3240, loss: 0.2670978605747223\n",
      "batch: 3241, loss: 0.3495190143585205\n",
      "batch: 3242, loss: 0.30329060554504395\n",
      "batch: 3243, loss: 0.19436290860176086\n",
      "batch: 3244, loss: 0.26367706060409546\n",
      "batch: 3245, loss: 0.2142716646194458\n",
      "batch: 3246, loss: 0.25521841645240784\n",
      "batch: 3247, loss: 0.192561537027359\n",
      "batch: 3248, loss: 0.33912360668182373\n",
      "batch: 3249, loss: 0.16613692045211792\n",
      "batch: 3250, loss: 0.2932899296283722\n",
      "batch: 3251, loss: 0.21883629262447357\n",
      "batch: 3252, loss: 0.1781846284866333\n",
      "batch: 3253, loss: 0.33624646067619324\n",
      "batch: 3254, loss: 0.2859848737716675\n",
      "batch: 3255, loss: 0.30423736572265625\n",
      "batch: 3256, loss: 0.23960930109024048\n",
      "batch: 3257, loss: 0.16643010079860687\n",
      "batch: 3258, loss: 0.4259115159511566\n",
      "batch: 3259, loss: 0.2627296447753906\n",
      "batch: 3260, loss: 0.19299691915512085\n",
      "batch: 3261, loss: 0.21986129879951477\n",
      "batch: 3262, loss: 0.37165147066116333\n",
      "batch: 3263, loss: 0.11406321823596954\n",
      "batch: 3264, loss: 0.2096167951822281\n",
      "batch: 3265, loss: 0.2037975788116455\n",
      "batch: 3266, loss: 0.3200824558734894\n",
      "batch: 3267, loss: 0.3002523183822632\n",
      "batch: 3268, loss: 0.33786508440971375\n",
      "batch: 3269, loss: 0.18545718491077423\n",
      "batch: 3270, loss: 0.15420560538768768\n",
      "batch: 3271, loss: 0.18491196632385254\n",
      "batch: 3272, loss: 0.24864783883094788\n",
      "batch: 3273, loss: 0.17616552114486694\n",
      "batch: 3274, loss: 0.3592587113380432\n",
      "batch: 3275, loss: 0.2555932104587555\n",
      "batch: 3276, loss: 0.29823848605155945\n",
      "batch: 3277, loss: 0.3094557523727417\n",
      "batch: 3278, loss: 0.11842605471611023\n",
      "batch: 3279, loss: 0.18498322367668152\n",
      "batch: 3280, loss: 0.35385382175445557\n",
      "batch: 3281, loss: 0.2768140733242035\n",
      "batch: 3282, loss: 0.17023642361164093\n",
      "batch: 3283, loss: 0.16180238127708435\n",
      "batch: 3284, loss: 0.23553788661956787\n",
      "batch: 3285, loss: 0.24194034934043884\n",
      "batch: 3286, loss: 0.40053611993789673\n",
      "batch: 3287, loss: 0.24976836144924164\n",
      "batch: 3288, loss: 0.2774713635444641\n",
      "batch: 3289, loss: 0.30409783124923706\n",
      "batch: 3290, loss: 0.20007357001304626\n",
      "batch: 3291, loss: 0.34558266401290894\n",
      "batch: 3292, loss: 0.3412656784057617\n",
      "batch: 3293, loss: 0.21775393187999725\n",
      "batch: 3294, loss: 0.17674732208251953\n",
      "batch: 3295, loss: 0.3180040717124939\n",
      "batch: 3296, loss: 0.25460395216941833\n",
      "batch: 3297, loss: 0.30570241808891296\n",
      "batch: 3298, loss: 0.15764698386192322\n",
      "batch: 3299, loss: 0.26236259937286377\n",
      "batch: 3300, loss: 0.30923157930374146\n",
      "model saved to ./saving/model.ckpt-33\n",
      "batch: 3301, loss: 0.19945456087589264\n",
      "batch: 3302, loss: 0.26356104016304016\n",
      "batch: 3303, loss: 0.38167375326156616\n",
      "batch: 3304, loss: 0.44126081466674805\n",
      "batch: 3305, loss: 0.2807422876358032\n",
      "batch: 3306, loss: 0.23805704712867737\n",
      "batch: 3307, loss: 0.12941938638687134\n",
      "batch: 3308, loss: 0.18470421433448792\n",
      "batch: 3309, loss: 0.107998326420784\n",
      "batch: 3310, loss: 0.29139018058776855\n",
      "batch: 3311, loss: 0.21534809470176697\n",
      "batch: 3312, loss: 0.28488296270370483\n",
      "batch: 3313, loss: 0.13795216381549835\n",
      "batch: 3314, loss: 0.2388882040977478\n",
      "batch: 3315, loss: 0.17163804173469543\n",
      "batch: 3316, loss: 0.163099005818367\n",
      "batch: 3317, loss: 0.19496330618858337\n",
      "batch: 3318, loss: 0.16731616854667664\n",
      "batch: 3319, loss: 0.2934502363204956\n",
      "batch: 3320, loss: 0.34467774629592896\n",
      "batch: 3321, loss: 0.3184434175491333\n",
      "batch: 3322, loss: 0.17963391542434692\n",
      "batch: 3323, loss: 0.25158441066741943\n",
      "batch: 3324, loss: 0.30841392278671265\n",
      "batch: 3325, loss: 0.3151494264602661\n",
      "batch: 3326, loss: 0.2807115614414215\n",
      "batch: 3327, loss: 0.14740043878555298\n",
      "batch: 3328, loss: 0.24023263156414032\n",
      "batch: 3329, loss: 0.10941097140312195\n",
      "batch: 3330, loss: 0.3324708938598633\n",
      "batch: 3331, loss: 0.2848747968673706\n",
      "batch: 3332, loss: 0.30070367455482483\n",
      "batch: 3333, loss: 0.29688218235969543\n",
      "batch: 3334, loss: 0.26072967052459717\n",
      "batch: 3335, loss: 0.28139129281044006\n",
      "batch: 3336, loss: 0.17338180541992188\n",
      "batch: 3337, loss: 0.23282378911972046\n",
      "batch: 3338, loss: 0.1807338297367096\n",
      "batch: 3339, loss: 0.10785878449678421\n",
      "batch: 3340, loss: 0.25394001603126526\n",
      "batch: 3341, loss: 0.21755331754684448\n",
      "batch: 3342, loss: 0.13426446914672852\n",
      "batch: 3343, loss: 0.24110227823257446\n",
      "batch: 3344, loss: 0.45311814546585083\n",
      "batch: 3345, loss: 0.28584185242652893\n",
      "batch: 3346, loss: 0.3864606022834778\n",
      "batch: 3347, loss: 0.19786623120307922\n",
      "batch: 3348, loss: 0.1787678450345993\n",
      "batch: 3349, loss: 0.18598374724388123\n",
      "batch: 3350, loss: 0.2924419045448303\n",
      "batch: 3351, loss: 0.19219675660133362\n",
      "batch: 3352, loss: 0.2588312029838562\n",
      "batch: 3353, loss: 0.1384696215391159\n",
      "batch: 3354, loss: 0.09796185791492462\n",
      "batch: 3355, loss: 0.23711369931697845\n",
      "batch: 3356, loss: 0.20475545525550842\n",
      "batch: 3357, loss: 0.2053946554660797\n",
      "batch: 3358, loss: 0.2846771776676178\n",
      "batch: 3359, loss: 0.24350781738758087\n",
      "batch: 3360, loss: 0.23972764611244202\n",
      "batch: 3361, loss: 0.15710195899009705\n",
      "batch: 3362, loss: 0.2551800608634949\n",
      "batch: 3363, loss: 0.268069326877594\n",
      "batch: 3364, loss: 0.19274497032165527\n",
      "batch: 3365, loss: 0.2835584878921509\n",
      "batch: 3366, loss: 0.27715423703193665\n",
      "batch: 3367, loss: 0.18103528022766113\n",
      "batch: 3368, loss: 0.39424294233322144\n",
      "batch: 3369, loss: 0.3663075268268585\n",
      "batch: 3370, loss: 0.25955086946487427\n",
      "batch: 3371, loss: 0.23421728610992432\n",
      "batch: 3372, loss: 0.4318205714225769\n",
      "batch: 3373, loss: 0.1734790802001953\n",
      "batch: 3374, loss: 0.1907026469707489\n",
      "batch: 3375, loss: 0.2042587399482727\n",
      "batch: 3376, loss: 0.24378687143325806\n",
      "batch: 3377, loss: 0.3255462050437927\n",
      "batch: 3378, loss: 0.16310717165470123\n",
      "batch: 3379, loss: 0.26991450786590576\n",
      "batch: 3380, loss: 0.2621128559112549\n",
      "batch: 3381, loss: 0.16169604659080505\n",
      "batch: 3382, loss: 0.20533889532089233\n",
      "batch: 3383, loss: 0.19801071286201477\n",
      "batch: 3384, loss: 0.18343999981880188\n",
      "batch: 3385, loss: 0.3189413547515869\n",
      "batch: 3386, loss: 0.2656731605529785\n",
      "batch: 3387, loss: 0.3332306146621704\n",
      "batch: 3388, loss: 0.15385062992572784\n",
      "batch: 3389, loss: 0.13695697486400604\n",
      "batch: 3390, loss: 0.21726512908935547\n",
      "batch: 3391, loss: 0.17121942341327667\n",
      "batch: 3392, loss: 0.2589075267314911\n",
      "batch: 3393, loss: 0.3874460458755493\n",
      "batch: 3394, loss: 0.23130516707897186\n",
      "batch: 3395, loss: 0.34024229645729065\n",
      "batch: 3396, loss: 0.20391599833965302\n",
      "batch: 3397, loss: 0.23177511990070343\n",
      "batch: 3398, loss: 0.27868422865867615\n",
      "batch: 3399, loss: 0.16347478330135345\n",
      "batch: 3400, loss: 0.23914092779159546\n",
      "model saved to ./saving/model.ckpt-34\n",
      "batch: 3401, loss: 0.3745943307876587\n",
      "batch: 3402, loss: 0.24350762367248535\n",
      "batch: 3403, loss: 0.3491445779800415\n",
      "batch: 3404, loss: 0.2220589518547058\n",
      "batch: 3405, loss: 0.18027836084365845\n",
      "batch: 3406, loss: 0.1101694256067276\n",
      "batch: 3407, loss: 0.2341667264699936\n",
      "batch: 3408, loss: 0.26633119583129883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 3409, loss: 0.38534805178642273\n",
      "batch: 3410, loss: 0.37980714440345764\n",
      "batch: 3411, loss: 0.18912209570407867\n",
      "batch: 3412, loss: 0.19333484768867493\n",
      "batch: 3413, loss: 0.22102034091949463\n",
      "batch: 3414, loss: 0.24390849471092224\n",
      "batch: 3415, loss: 0.12118133902549744\n",
      "batch: 3416, loss: 0.21057508885860443\n",
      "batch: 3417, loss: 0.24892184138298035\n",
      "batch: 3418, loss: 0.23785941302776337\n",
      "batch: 3419, loss: 0.27311351895332336\n",
      "batch: 3420, loss: 0.1920059621334076\n",
      "batch: 3421, loss: 0.34266534447669983\n",
      "batch: 3422, loss: 0.2528217136859894\n",
      "batch: 3423, loss: 0.4102845788002014\n",
      "batch: 3424, loss: 0.2640652358531952\n",
      "batch: 3425, loss: 0.10447844117879868\n",
      "batch: 3426, loss: 0.27923691272735596\n",
      "batch: 3427, loss: 0.2638114392757416\n",
      "batch: 3428, loss: 0.3011236786842346\n",
      "batch: 3429, loss: 0.21690741181373596\n",
      "batch: 3430, loss: 0.2049316018819809\n",
      "batch: 3431, loss: 0.28399398922920227\n",
      "batch: 3432, loss: 0.2910776138305664\n",
      "batch: 3433, loss: 0.17186161875724792\n",
      "batch: 3434, loss: 0.38961726427078247\n",
      "batch: 3435, loss: 0.21307262778282166\n",
      "batch: 3436, loss: 0.12046946585178375\n",
      "batch: 3437, loss: 0.24657182395458221\n",
      "batch: 3438, loss: 0.2568124234676361\n",
      "batch: 3439, loss: 0.3791443705558777\n",
      "batch: 3440, loss: 0.1816372126340866\n",
      "batch: 3441, loss: 0.1467636525630951\n",
      "batch: 3442, loss: 0.21224626898765564\n",
      "batch: 3443, loss: 0.12490953505039215\n",
      "batch: 3444, loss: 0.2248269021511078\n",
      "batch: 3445, loss: 0.33321475982666016\n",
      "batch: 3446, loss: 0.20260144770145416\n",
      "batch: 3447, loss: 0.19477766752243042\n",
      "batch: 3448, loss: 0.18740418553352356\n",
      "batch: 3449, loss: 0.21955882012844086\n",
      "batch: 3450, loss: 0.21229258179664612\n",
      "batch: 3451, loss: 0.3869468569755554\n",
      "batch: 3452, loss: 0.34807464480400085\n",
      "batch: 3453, loss: 0.16351845860481262\n",
      "batch: 3454, loss: 0.27311059832572937\n",
      "batch: 3455, loss: 0.36157870292663574\n",
      "batch: 3456, loss: 0.30903133749961853\n",
      "batch: 3457, loss: 0.18419548869132996\n",
      "batch: 3458, loss: 0.21759077906608582\n",
      "batch: 3459, loss: 0.29414087533950806\n",
      "batch: 3460, loss: 0.1665305197238922\n",
      "batch: 3461, loss: 0.25402694940567017\n",
      "batch: 3462, loss: 0.18094269931316376\n",
      "batch: 3463, loss: 0.2866293489933014\n",
      "batch: 3464, loss: 0.22734612226486206\n",
      "batch: 3465, loss: 0.1878223717212677\n",
      "batch: 3466, loss: 0.3113735616207123\n",
      "batch: 3467, loss: 0.26595374941825867\n",
      "batch: 3468, loss: 0.14013920724391937\n",
      "batch: 3469, loss: 0.1717347502708435\n",
      "batch: 3470, loss: 0.18554918467998505\n",
      "batch: 3471, loss: 0.4146941900253296\n",
      "batch: 3472, loss: 0.4697488844394684\n",
      "batch: 3473, loss: 0.23251605033874512\n",
      "batch: 3474, loss: 0.25902050733566284\n",
      "batch: 3475, loss: 0.17601144313812256\n",
      "batch: 3476, loss: 0.39470016956329346\n",
      "batch: 3477, loss: 0.17416223883628845\n",
      "batch: 3478, loss: 0.2777697443962097\n",
      "batch: 3479, loss: 0.1966286301612854\n",
      "batch: 3480, loss: 0.23989379405975342\n",
      "batch: 3481, loss: 0.2494736760854721\n",
      "batch: 3482, loss: 0.3331034779548645\n",
      "batch: 3483, loss: 0.2331266701221466\n",
      "batch: 3484, loss: 0.37365710735321045\n",
      "batch: 3485, loss: 0.20535966753959656\n",
      "batch: 3486, loss: 0.20652970671653748\n",
      "batch: 3487, loss: 0.37950822710990906\n",
      "batch: 3488, loss: 0.2131718397140503\n",
      "batch: 3489, loss: 0.1846514791250229\n",
      "batch: 3490, loss: 0.22891095280647278\n",
      "batch: 3491, loss: 0.19358107447624207\n",
      "batch: 3492, loss: 0.33685991168022156\n",
      "batch: 3493, loss: 0.4581530690193176\n",
      "batch: 3494, loss: 0.28359103202819824\n",
      "batch: 3495, loss: 0.1193903312087059\n",
      "batch: 3496, loss: 0.22315752506256104\n",
      "batch: 3497, loss: 0.4286758005619049\n",
      "batch: 3498, loss: 0.20865222811698914\n",
      "batch: 3499, loss: 0.2734985947608948\n",
      "batch: 3500, loss: 0.27217578887939453\n",
      "model saved to ./saving/model.ckpt-35\n",
      "batch: 3501, loss: 0.19768470525741577\n",
      "batch: 3502, loss: 0.2987586259841919\n",
      "batch: 3503, loss: 0.24754492938518524\n",
      "batch: 3504, loss: 0.19824029505252838\n",
      "batch: 3505, loss: 0.2022424042224884\n",
      "batch: 3506, loss: 0.16553020477294922\n",
      "batch: 3507, loss: 0.14036208391189575\n",
      "batch: 3508, loss: 0.11701902747154236\n",
      "batch: 3509, loss: 0.38965705037117004\n",
      "batch: 3510, loss: 0.27913984656333923\n",
      "batch: 3511, loss: 0.43771570920944214\n",
      "batch: 3512, loss: 0.18929621577262878\n",
      "batch: 3513, loss: 0.320936918258667\n",
      "batch: 3514, loss: 0.27890485525131226\n",
      "batch: 3515, loss: 0.11891639232635498\n",
      "batch: 3516, loss: 0.25529325008392334\n",
      "batch: 3517, loss: 0.3109629154205322\n",
      "batch: 3518, loss: 0.23135143518447876\n",
      "batch: 3519, loss: 0.16080260276794434\n",
      "batch: 3520, loss: 0.25661274790763855\n",
      "batch: 3521, loss: 0.17746618390083313\n",
      "batch: 3522, loss: 0.2828434407711029\n",
      "batch: 3523, loss: 0.12016740441322327\n",
      "batch: 3524, loss: 0.21258726716041565\n",
      "batch: 3525, loss: 0.1679701954126358\n",
      "batch: 3526, loss: 0.17581704258918762\n",
      "batch: 3527, loss: 0.2603175938129425\n",
      "batch: 3528, loss: 0.23134014010429382\n",
      "batch: 3529, loss: 0.19978466629981995\n",
      "batch: 3530, loss: 0.1420821249485016\n",
      "batch: 3531, loss: 0.1769011914730072\n",
      "batch: 3532, loss: 0.15980492532253265\n",
      "batch: 3533, loss: 0.22157219052314758\n",
      "batch: 3534, loss: 0.28342363238334656\n",
      "batch: 3535, loss: 0.38691389560699463\n",
      "batch: 3536, loss: 0.30470922589302063\n",
      "batch: 3537, loss: 0.29238784313201904\n",
      "batch: 3538, loss: 0.18069100379943848\n",
      "batch: 3539, loss: 0.18709871172904968\n",
      "batch: 3540, loss: 0.25021934509277344\n",
      "batch: 3541, loss: 0.27698925137519836\n",
      "batch: 3542, loss: 0.377336323261261\n",
      "batch: 3543, loss: 0.19975236058235168\n",
      "batch: 3544, loss: 0.26031064987182617\n",
      "batch: 3545, loss: 0.17267408967018127\n",
      "batch: 3546, loss: 0.16705073416233063\n",
      "batch: 3547, loss: 0.23946568369865417\n",
      "batch: 3548, loss: 0.18947403132915497\n",
      "batch: 3549, loss: 0.2676624059677124\n",
      "batch: 3550, loss: 0.1792297065258026\n",
      "batch: 3551, loss: 0.1491532325744629\n",
      "batch: 3552, loss: 0.17068834602832794\n",
      "batch: 3553, loss: 0.27285200357437134\n",
      "batch: 3554, loss: 0.16986581683158875\n",
      "batch: 3555, loss: 0.18462760746479034\n",
      "batch: 3556, loss: 0.16305512189865112\n",
      "batch: 3557, loss: 0.2679198980331421\n",
      "batch: 3558, loss: 0.23323655128479004\n",
      "batch: 3559, loss: 0.42258432507514954\n",
      "batch: 3560, loss: 0.16629502177238464\n",
      "batch: 3561, loss: 0.12571027874946594\n",
      "batch: 3562, loss: 0.2709093987941742\n",
      "batch: 3563, loss: 0.2662011384963989\n",
      "batch: 3564, loss: 0.15372315049171448\n",
      "batch: 3565, loss: 0.22096356749534607\n",
      "batch: 3566, loss: 0.16288039088249207\n",
      "batch: 3567, loss: 0.1693519949913025\n",
      "batch: 3568, loss: 0.23309949040412903\n",
      "batch: 3569, loss: 0.1958812028169632\n",
      "batch: 3570, loss: 0.23743563890457153\n",
      "batch: 3571, loss: 0.2327263504266739\n",
      "batch: 3572, loss: 0.21434539556503296\n",
      "batch: 3573, loss: 0.5300628542900085\n",
      "batch: 3574, loss: 0.46062737703323364\n",
      "batch: 3575, loss: 0.12221437692642212\n",
      "batch: 3576, loss: 0.17746561765670776\n",
      "batch: 3577, loss: 0.1847301721572876\n",
      "batch: 3578, loss: 0.1633714735507965\n",
      "batch: 3579, loss: 0.27780023217201233\n",
      "batch: 3580, loss: 0.18964049220085144\n",
      "batch: 3581, loss: 0.48905128240585327\n",
      "batch: 3582, loss: 0.23480463027954102\n",
      "batch: 3583, loss: 0.3862902522087097\n",
      "batch: 3584, loss: 0.19141113758087158\n",
      "batch: 3585, loss: 0.3652690052986145\n",
      "batch: 3586, loss: 0.25735849142074585\n",
      "batch: 3587, loss: 0.2562326192855835\n",
      "batch: 3588, loss: 0.23548460006713867\n",
      "batch: 3589, loss: 0.17847415804862976\n",
      "batch: 3590, loss: 0.3231883645057678\n",
      "batch: 3591, loss: 0.15994015336036682\n",
      "batch: 3592, loss: 0.2799089550971985\n",
      "batch: 3593, loss: 0.36232686042785645\n",
      "batch: 3594, loss: 0.232076957821846\n",
      "batch: 3595, loss: 0.30835825204849243\n",
      "batch: 3596, loss: 0.2877817749977112\n",
      "batch: 3597, loss: 0.3849955201148987\n",
      "batch: 3598, loss: 0.2525402307510376\n",
      "batch: 3599, loss: 0.2201979160308838\n",
      "batch: 3600, loss: 0.1430157870054245\n",
      "model saved to ./saving/model.ckpt-36\n",
      "batch: 3601, loss: 0.2516416311264038\n",
      "batch: 3602, loss: 0.21950094401836395\n",
      "batch: 3603, loss: 0.3031444549560547\n",
      "batch: 3604, loss: 0.2867910861968994\n",
      "batch: 3605, loss: 0.21935364603996277\n",
      "batch: 3606, loss: 0.22955147922039032\n",
      "batch: 3607, loss: 0.2683775722980499\n",
      "batch: 3608, loss: 0.191910058259964\n",
      "batch: 3609, loss: 0.29911625385284424\n",
      "batch: 3610, loss: 0.19290339946746826\n",
      "batch: 3611, loss: 0.17818689346313477\n",
      "batch: 3612, loss: 0.15117192268371582\n",
      "batch: 3613, loss: 0.2033335566520691\n",
      "batch: 3614, loss: 0.2834305167198181\n",
      "batch: 3615, loss: 0.2354811728000641\n",
      "batch: 3616, loss: 0.42122000455856323\n",
      "batch: 3617, loss: 0.19709941744804382\n",
      "batch: 3618, loss: 0.2791203260421753\n",
      "batch: 3619, loss: 0.25434398651123047\n",
      "batch: 3620, loss: 0.34615403413772583\n",
      "batch: 3621, loss: 0.32969290018081665\n",
      "batch: 3622, loss: 0.38573774695396423\n",
      "batch: 3623, loss: 0.20548754930496216\n",
      "batch: 3624, loss: 0.2304820567369461\n",
      "batch: 3625, loss: 0.2324962317943573\n",
      "batch: 3626, loss: 0.1646796315908432\n",
      "batch: 3627, loss: 0.2652384042739868\n",
      "batch: 3628, loss: 0.21342997252941132\n",
      "batch: 3629, loss: 0.3122299313545227\n",
      "batch: 3630, loss: 0.25661468505859375\n",
      "batch: 3631, loss: 0.300175279378891\n",
      "batch: 3632, loss: 0.13132932782173157\n",
      "batch: 3633, loss: 0.1599673479795456\n",
      "batch: 3634, loss: 0.18522843718528748\n",
      "batch: 3635, loss: 0.19170638918876648\n",
      "batch: 3636, loss: 0.5144737958908081\n",
      "batch: 3637, loss: 0.1851881891489029\n",
      "batch: 3638, loss: 0.29252225160598755\n",
      "batch: 3639, loss: 0.28699034452438354\n",
      "batch: 3640, loss: 0.42808017134666443\n",
      "batch: 3641, loss: 0.2740683853626251\n",
      "batch: 3642, loss: 0.13212643563747406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 3643, loss: 0.35139942169189453\n",
      "batch: 3644, loss: 0.19300386309623718\n",
      "batch: 3645, loss: 0.15022405982017517\n",
      "batch: 3646, loss: 0.11678719520568848\n",
      "batch: 3647, loss: 0.1675332486629486\n",
      "batch: 3648, loss: 0.23792511224746704\n",
      "batch: 3649, loss: 0.2558947205543518\n",
      "batch: 3650, loss: 0.16037976741790771\n",
      "batch: 3651, loss: 0.7315360903739929\n",
      "batch: 3652, loss: 0.2275470793247223\n",
      "batch: 3653, loss: 0.278469055891037\n",
      "batch: 3654, loss: 0.43475884199142456\n",
      "batch: 3655, loss: 0.2216508388519287\n",
      "batch: 3656, loss: 0.268008828163147\n",
      "batch: 3657, loss: 0.22711500525474548\n",
      "batch: 3658, loss: 0.2495354861021042\n",
      "batch: 3659, loss: 0.4317481815814972\n",
      "batch: 3660, loss: 0.17154856026172638\n",
      "batch: 3661, loss: 0.2013196051120758\n",
      "batch: 3662, loss: 0.28067317605018616\n",
      "batch: 3663, loss: 0.231637105345726\n",
      "batch: 3664, loss: 0.19469457864761353\n",
      "batch: 3665, loss: 0.358745276927948\n",
      "batch: 3666, loss: 0.1585691124200821\n",
      "batch: 3667, loss: 0.19985038042068481\n",
      "batch: 3668, loss: 0.23664066195487976\n",
      "batch: 3669, loss: 0.10828050971031189\n",
      "batch: 3670, loss: 0.20970317721366882\n",
      "batch: 3671, loss: 0.0848676934838295\n",
      "batch: 3672, loss: 0.20542261004447937\n",
      "batch: 3673, loss: 0.39600712060928345\n",
      "batch: 3674, loss: 0.17129600048065186\n",
      "batch: 3675, loss: 0.33147525787353516\n",
      "batch: 3676, loss: 0.12212194502353668\n",
      "batch: 3677, loss: 0.1485423892736435\n",
      "batch: 3678, loss: 0.33166900277137756\n",
      "batch: 3679, loss: 0.2964969277381897\n",
      "batch: 3680, loss: 0.18135201930999756\n",
      "batch: 3681, loss: 0.14636069536209106\n",
      "batch: 3682, loss: 0.2510364055633545\n",
      "batch: 3683, loss: 0.34655439853668213\n",
      "batch: 3684, loss: 0.2258301079273224\n",
      "batch: 3685, loss: 0.12739808857440948\n",
      "batch: 3686, loss: 0.23201553523540497\n",
      "batch: 3687, loss: 0.17746880650520325\n",
      "batch: 3688, loss: 0.21104663610458374\n",
      "batch: 3689, loss: 0.1978125423192978\n",
      "batch: 3690, loss: 0.17187777161598206\n",
      "batch: 3691, loss: 0.17672137916088104\n",
      "batch: 3692, loss: 0.1803349107503891\n",
      "batch: 3693, loss: 0.41541165113449097\n",
      "batch: 3694, loss: 0.25390180945396423\n",
      "batch: 3695, loss: 0.15679821372032166\n",
      "batch: 3696, loss: 0.11564268171787262\n",
      "batch: 3697, loss: 0.1803063452243805\n",
      "batch: 3698, loss: 0.16866229474544525\n",
      "batch: 3699, loss: 0.23490625619888306\n",
      "batch: 3700, loss: 0.24735771119594574\n",
      "model saved to ./saving/model.ckpt-37\n",
      "batch: 3701, loss: 0.2382124960422516\n",
      "batch: 3702, loss: 0.2554052472114563\n",
      "batch: 3703, loss: 0.3237219750881195\n",
      "batch: 3704, loss: 0.23180653154850006\n",
      "batch: 3705, loss: 0.20345892012119293\n",
      "batch: 3706, loss: 0.21863169968128204\n",
      "batch: 3707, loss: 0.20601975917816162\n",
      "batch: 3708, loss: 0.366723895072937\n",
      "batch: 3709, loss: 0.3318990468978882\n",
      "batch: 3710, loss: 0.3021402359008789\n",
      "batch: 3711, loss: 0.27514657378196716\n",
      "batch: 3712, loss: 0.1936829537153244\n",
      "batch: 3713, loss: 0.15538005530834198\n",
      "batch: 3714, loss: 0.19999299943447113\n",
      "batch: 3715, loss: 0.2565203905105591\n",
      "batch: 3716, loss: 0.1995254009962082\n",
      "batch: 3717, loss: 0.20817551016807556\n",
      "batch: 3718, loss: 0.23898625373840332\n",
      "batch: 3719, loss: 0.13751348853111267\n",
      "batch: 3720, loss: 0.36276108026504517\n",
      "batch: 3721, loss: 0.1900743842124939\n",
      "batch: 3722, loss: 0.31943726539611816\n",
      "batch: 3723, loss: 0.5362647771835327\n",
      "batch: 3724, loss: 0.2642134726047516\n",
      "batch: 3725, loss: 0.1893133521080017\n",
      "batch: 3726, loss: 0.33269938826560974\n",
      "batch: 3727, loss: 0.20990467071533203\n",
      "batch: 3728, loss: 0.26310083270072937\n",
      "batch: 3729, loss: 0.23282797634601593\n",
      "batch: 3730, loss: 0.17764176428318024\n",
      "batch: 3731, loss: 0.13590861856937408\n",
      "batch: 3732, loss: 0.24835661053657532\n",
      "batch: 3733, loss: 0.17449045181274414\n",
      "batch: 3734, loss: 0.187638059258461\n",
      "batch: 3735, loss: 0.26215195655822754\n",
      "batch: 3736, loss: 0.25036466121673584\n",
      "batch: 3737, loss: 0.1967940777540207\n",
      "batch: 3738, loss: 0.24050460755825043\n",
      "batch: 3739, loss: 0.1528046727180481\n",
      "batch: 3740, loss: 0.20219118893146515\n",
      "batch: 3741, loss: 0.1441996544599533\n",
      "batch: 3742, loss: 0.300801545381546\n",
      "batch: 3743, loss: 0.15754632651805878\n",
      "batch: 3744, loss: 0.270600825548172\n",
      "batch: 3745, loss: 0.26193180680274963\n",
      "batch: 3746, loss: 0.406036913394928\n",
      "batch: 3747, loss: 0.34832143783569336\n",
      "batch: 3748, loss: 0.2226993292570114\n",
      "batch: 3749, loss: 0.27083253860473633\n",
      "batch: 3750, loss: 0.2978895902633667\n",
      "batch: 3751, loss: 0.3864324390888214\n",
      "batch: 3752, loss: 0.3386170268058777\n",
      "batch: 3753, loss: 0.3507861793041229\n",
      "batch: 3754, loss: 0.23839087784290314\n",
      "batch: 3755, loss: 0.10273933410644531\n",
      "batch: 3756, loss: 0.2712792754173279\n",
      "batch: 3757, loss: 0.19549286365509033\n",
      "batch: 3758, loss: 0.21196654438972473\n",
      "batch: 3759, loss: 0.31540048122406006\n",
      "batch: 3760, loss: 0.24787572026252747\n",
      "batch: 3761, loss: 0.26322141289711\n",
      "batch: 3762, loss: 0.15418097376823425\n",
      "batch: 3763, loss: 0.16506829857826233\n",
      "batch: 3764, loss: 0.31768086552619934\n",
      "batch: 3765, loss: 0.26606979966163635\n",
      "batch: 3766, loss: 0.20846354961395264\n",
      "batch: 3767, loss: 0.21643796563148499\n",
      "batch: 3768, loss: 0.42055612802505493\n",
      "batch: 3769, loss: 0.1547352522611618\n",
      "batch: 3770, loss: 0.34614306688308716\n",
      "batch: 3771, loss: 0.08207925409078598\n",
      "batch: 3772, loss: 0.2693819999694824\n",
      "batch: 3773, loss: 0.21892400085926056\n",
      "batch: 3774, loss: 0.1670818030834198\n",
      "batch: 3775, loss: 0.4051319658756256\n",
      "batch: 3776, loss: 0.29651132225990295\n",
      "batch: 3777, loss: 0.17979148030281067\n",
      "batch: 3778, loss: 0.129270538687706\n",
      "batch: 3779, loss: 0.2158890962600708\n",
      "batch: 3780, loss: 0.34140992164611816\n",
      "batch: 3781, loss: 0.4271252155303955\n",
      "batch: 3782, loss: 0.246412456035614\n",
      "batch: 3783, loss: 0.2264050990343094\n",
      "batch: 3784, loss: 0.21015137434005737\n",
      "batch: 3785, loss: 0.22799944877624512\n",
      "batch: 3786, loss: 0.13201558589935303\n",
      "batch: 3787, loss: 0.21094921231269836\n",
      "batch: 3788, loss: 0.08303764462471008\n",
      "batch: 3789, loss: 0.16582655906677246\n",
      "batch: 3790, loss: 0.18611764907836914\n",
      "batch: 3791, loss: 0.2477549910545349\n",
      "batch: 3792, loss: 0.22709575295448303\n",
      "batch: 3793, loss: 0.4071967303752899\n",
      "batch: 3794, loss: 0.2737578749656677\n",
      "batch: 3795, loss: 0.2755414545536041\n",
      "batch: 3796, loss: 0.24676355719566345\n",
      "batch: 3797, loss: 0.372058629989624\n",
      "batch: 3798, loss: 0.14256945252418518\n",
      "batch: 3799, loss: 0.26546305418014526\n",
      "batch: 3800, loss: 0.21933718025684357\n",
      "model saved to ./saving/model.ckpt-38\n",
      "batch: 3801, loss: 0.19654491543769836\n",
      "batch: 3802, loss: 0.31715115904808044\n",
      "batch: 3803, loss: 0.2339886873960495\n",
      "batch: 3804, loss: 0.2338801771402359\n",
      "batch: 3805, loss: 0.1948641538619995\n",
      "batch: 3806, loss: 0.18205122649669647\n",
      "batch: 3807, loss: 0.20683526992797852\n",
      "batch: 3808, loss: 0.1717260777950287\n",
      "batch: 3809, loss: 0.14735674858093262\n",
      "batch: 3810, loss: 0.1907062530517578\n",
      "batch: 3811, loss: 0.214517742395401\n",
      "batch: 3812, loss: 0.17597892880439758\n",
      "batch: 3813, loss: 0.18689271807670593\n",
      "batch: 3814, loss: 0.26938700675964355\n",
      "batch: 3815, loss: 0.14277416467666626\n",
      "batch: 3816, loss: 0.4406747817993164\n",
      "batch: 3817, loss: 0.17672616243362427\n",
      "batch: 3818, loss: 0.268473744392395\n",
      "batch: 3819, loss: 0.3325357735157013\n",
      "batch: 3820, loss: 0.19048386812210083\n",
      "batch: 3821, loss: 0.11224120110273361\n",
      "batch: 3822, loss: 0.24933871626853943\n",
      "batch: 3823, loss: 0.11564093828201294\n",
      "batch: 3824, loss: 0.1995537281036377\n",
      "batch: 3825, loss: 0.19872769713401794\n",
      "batch: 3826, loss: 0.15929144620895386\n",
      "batch: 3827, loss: 0.25684231519699097\n",
      "batch: 3828, loss: 0.2978200316429138\n",
      "batch: 3829, loss: 0.29800382256507874\n",
      "batch: 3830, loss: 0.0633799135684967\n",
      "batch: 3831, loss: 0.21970947086811066\n",
      "batch: 3832, loss: 0.28627362847328186\n",
      "batch: 3833, loss: 0.12711310386657715\n",
      "batch: 3834, loss: 0.16887281835079193\n",
      "batch: 3835, loss: 0.3176482617855072\n",
      "batch: 3836, loss: 0.42334628105163574\n",
      "batch: 3837, loss: 0.26517704129219055\n",
      "batch: 3838, loss: 0.2890775203704834\n",
      "batch: 3839, loss: 0.18406718969345093\n",
      "batch: 3840, loss: 0.32641923427581787\n",
      "batch: 3841, loss: 0.17558756470680237\n",
      "batch: 3842, loss: 0.13315455615520477\n",
      "batch: 3843, loss: 0.41379255056381226\n",
      "batch: 3844, loss: 0.1293814480304718\n",
      "batch: 3845, loss: 0.1889847218990326\n",
      "batch: 3846, loss: 0.19051463901996613\n",
      "batch: 3847, loss: 0.16842900216579437\n",
      "batch: 3848, loss: 0.23440754413604736\n",
      "batch: 3849, loss: 0.18690042197704315\n",
      "batch: 3850, loss: 0.20399004220962524\n",
      "batch: 3851, loss: 0.2478407323360443\n",
      "batch: 3852, loss: 0.1661541610956192\n",
      "batch: 3853, loss: 0.27091705799102783\n",
      "batch: 3854, loss: 0.20630478858947754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 3855, loss: 0.14266812801361084\n",
      "batch: 3856, loss: 0.1741732954978943\n",
      "batch: 3857, loss: 0.2364281266927719\n",
      "batch: 3858, loss: 0.1532326191663742\n",
      "batch: 3859, loss: 0.25820469856262207\n",
      "batch: 3860, loss: 0.13386288285255432\n",
      "batch: 3861, loss: 0.1697288453578949\n",
      "batch: 3862, loss: 0.20231366157531738\n",
      "batch: 3863, loss: 0.29204511642456055\n",
      "batch: 3864, loss: 0.24635322391986847\n",
      "batch: 3865, loss: 0.35144513845443726\n",
      "batch: 3866, loss: 0.2523941993713379\n",
      "batch: 3867, loss: 0.15576216578483582\n",
      "batch: 3868, loss: 0.377471923828125\n",
      "batch: 3869, loss: 0.10202142596244812\n",
      "batch: 3870, loss: 0.3625395894050598\n",
      "batch: 3871, loss: 0.1436527967453003\n",
      "batch: 3872, loss: 0.18287964165210724\n",
      "batch: 3873, loss: 0.3326237201690674\n",
      "batch: 3874, loss: 0.1571066677570343\n",
      "batch: 3875, loss: 0.25569185614585876\n",
      "batch: 3876, loss: 0.13461537659168243\n",
      "batch: 3877, loss: 0.20162934064865112\n",
      "batch: 3878, loss: 0.12485805153846741\n",
      "batch: 3879, loss: 0.16820821166038513\n",
      "batch: 3880, loss: 0.23879483342170715\n",
      "batch: 3881, loss: 0.14358165860176086\n",
      "batch: 3882, loss: 0.1741969883441925\n",
      "batch: 3883, loss: 0.23265305161476135\n",
      "batch: 3884, loss: 0.31005850434303284\n",
      "batch: 3885, loss: 0.18708589673042297\n",
      "batch: 3886, loss: 0.2014046460390091\n",
      "batch: 3887, loss: 0.3495013117790222\n",
      "batch: 3888, loss: 0.11165503412485123\n",
      "batch: 3889, loss: 0.2737252116203308\n",
      "batch: 3890, loss: 0.12196756899356842\n",
      "batch: 3891, loss: 0.11154147982597351\n",
      "batch: 3892, loss: 0.3351471424102783\n",
      "batch: 3893, loss: 0.32276636362075806\n",
      "batch: 3894, loss: 0.24587573111057281\n",
      "batch: 3895, loss: 0.1944005936384201\n",
      "batch: 3896, loss: 0.23870036005973816\n",
      "batch: 3897, loss: 0.15127281844615936\n",
      "batch: 3898, loss: 0.33001163601875305\n",
      "batch: 3899, loss: 0.24359671771526337\n",
      "batch: 3900, loss: 0.30030572414398193\n",
      "model saved to ./saving/model.ckpt-39\n",
      "batch: 3901, loss: 0.20131109654903412\n",
      "batch: 3902, loss: 0.23626162111759186\n",
      "batch: 3903, loss: 0.35310131311416626\n",
      "batch: 3904, loss: 0.21105274558067322\n",
      "batch: 3905, loss: 0.2200101763010025\n",
      "batch: 3906, loss: 0.12823036313056946\n",
      "batch: 3907, loss: 0.23212261497974396\n",
      "batch: 3908, loss: 0.19793984293937683\n",
      "batch: 3909, loss: 0.1156773567199707\n",
      "batch: 3910, loss: 0.17894485592842102\n",
      "batch: 3911, loss: 0.2211993932723999\n",
      "batch: 3912, loss: 0.16737809777259827\n",
      "batch: 3913, loss: 0.3464317321777344\n",
      "batch: 3914, loss: 0.1696704626083374\n",
      "batch: 3915, loss: 0.14993926882743835\n",
      "batch: 3916, loss: 0.23001477122306824\n",
      "batch: 3917, loss: 0.11419878900051117\n",
      "batch: 3918, loss: 0.23247426748275757\n",
      "batch: 3919, loss: 0.539124608039856\n",
      "batch: 3920, loss: 0.2825744152069092\n",
      "batch: 3921, loss: 0.16111260652542114\n",
      "batch: 3922, loss: 0.1766974925994873\n",
      "batch: 3923, loss: 0.21953991055488586\n",
      "batch: 3924, loss: 0.2471942901611328\n",
      "batch: 3925, loss: 0.23086541891098022\n",
      "batch: 3926, loss: 0.20193704962730408\n",
      "batch: 3927, loss: 0.1817084550857544\n",
      "batch: 3928, loss: 0.1412222981452942\n",
      "batch: 3929, loss: 0.19393983483314514\n",
      "batch: 3930, loss: 0.12857267260551453\n",
      "batch: 3931, loss: 0.3145908713340759\n",
      "batch: 3932, loss: 0.31850337982177734\n",
      "batch: 3933, loss: 0.29775315523147583\n",
      "batch: 3934, loss: 0.32056987285614014\n",
      "batch: 3935, loss: 0.14692723751068115\n",
      "batch: 3936, loss: 0.18611475825309753\n",
      "batch: 3937, loss: 0.1602170765399933\n",
      "batch: 3938, loss: 0.3295290172100067\n",
      "batch: 3939, loss: 0.15600302815437317\n",
      "batch: 3940, loss: 0.2015402615070343\n",
      "batch: 3941, loss: 0.27602821588516235\n",
      "batch: 3942, loss: 0.14344953000545502\n",
      "batch: 3943, loss: 0.3463931679725647\n",
      "batch: 3944, loss: 0.23424455523490906\n",
      "batch: 3945, loss: 0.2587948441505432\n",
      "batch: 3946, loss: 0.1641460359096527\n",
      "batch: 3947, loss: 0.30261000990867615\n",
      "batch: 3948, loss: 0.3980221748352051\n",
      "batch: 3949, loss: 0.23519495129585266\n",
      "batch: 3950, loss: 0.10146598517894745\n",
      "batch: 3951, loss: 0.22094769775867462\n",
      "batch: 3952, loss: 0.13416916131973267\n",
      "batch: 3953, loss: 0.2469855546951294\n",
      "batch: 3954, loss: 0.2722492218017578\n",
      "batch: 3955, loss: 0.21297410130500793\n",
      "batch: 3956, loss: 0.18497619032859802\n",
      "batch: 3957, loss: 0.320686399936676\n",
      "batch: 3958, loss: 0.18379086256027222\n",
      "batch: 3959, loss: 0.17233380675315857\n",
      "batch: 3960, loss: 0.15446054935455322\n",
      "batch: 3961, loss: 0.3454228639602661\n",
      "batch: 3962, loss: 0.22968389093875885\n",
      "batch: 3963, loss: 0.11101751029491425\n",
      "batch: 3964, loss: 0.3636014759540558\n",
      "batch: 3965, loss: 0.43817734718322754\n",
      "batch: 3966, loss: 0.2312101274728775\n",
      "batch: 3967, loss: 0.2038334608078003\n",
      "batch: 3968, loss: 0.2788519859313965\n",
      "batch: 3969, loss: 0.18772448599338531\n",
      "batch: 3970, loss: 0.14016982913017273\n",
      "batch: 3971, loss: 0.20437419414520264\n",
      "batch: 3972, loss: 0.1538720279932022\n",
      "batch: 3973, loss: 0.24281299114227295\n",
      "batch: 3974, loss: 0.15341249108314514\n",
      "batch: 3975, loss: 0.2868804931640625\n",
      "batch: 3976, loss: 0.2532136142253876\n",
      "batch: 3977, loss: 0.31027233600616455\n",
      "batch: 3978, loss: 0.20888027548789978\n",
      "batch: 3979, loss: 0.377730131149292\n",
      "batch: 3980, loss: 0.10038979351520538\n",
      "batch: 3981, loss: 0.17798689007759094\n",
      "batch: 3982, loss: 0.12364146113395691\n",
      "batch: 3983, loss: 0.3422216773033142\n",
      "batch: 3984, loss: 0.2027239352464676\n",
      "batch: 3985, loss: 0.2956845164299011\n",
      "batch: 3986, loss: 0.16004526615142822\n",
      "batch: 3987, loss: 0.3549875020980835\n",
      "batch: 3988, loss: 0.16470307111740112\n",
      "batch: 3989, loss: 0.3257331848144531\n",
      "batch: 3990, loss: 0.19833114743232727\n",
      "batch: 3991, loss: 0.271779865026474\n",
      "batch: 3992, loss: 0.17125554382801056\n",
      "batch: 3993, loss: 0.18503612279891968\n",
      "batch: 3994, loss: 0.30030515789985657\n",
      "batch: 3995, loss: 0.23701412975788116\n",
      "batch: 3996, loss: 0.348747193813324\n",
      "batch: 3997, loss: 0.22305074334144592\n",
      "batch: 3998, loss: 0.33362120389938354\n",
      "batch: 3999, loss: 0.243305966258049\n",
      "batch: 4000, loss: 0.07445508241653442\n",
      "model saved to ./saving/model.ckpt-40\n",
      "batch: 4001, loss: 0.21003279089927673\n",
      "batch: 4002, loss: 0.26426729559898376\n",
      "batch: 4003, loss: 0.16105207800865173\n",
      "batch: 4004, loss: 0.28227829933166504\n",
      "batch: 4005, loss: 0.1702929139137268\n",
      "batch: 4006, loss: 0.26402655243873596\n",
      "batch: 4007, loss: 0.15255337953567505\n",
      "batch: 4008, loss: 0.22058430314064026\n",
      "batch: 4009, loss: 0.2530576288700104\n",
      "batch: 4010, loss: 0.14900653064250946\n",
      "batch: 4011, loss: 0.2153254598379135\n",
      "batch: 4012, loss: 0.25249966979026794\n",
      "batch: 4013, loss: 0.1476566642522812\n",
      "batch: 4014, loss: 0.26265645027160645\n",
      "batch: 4015, loss: 0.1864343285560608\n",
      "batch: 4016, loss: 0.15398094058036804\n",
      "batch: 4017, loss: 0.3331458270549774\n",
      "batch: 4018, loss: 0.16183477640151978\n",
      "batch: 4019, loss: 0.24043485522270203\n",
      "batch: 4020, loss: 0.19800817966461182\n",
      "batch: 4021, loss: 0.2725503444671631\n",
      "batch: 4022, loss: 0.3560389280319214\n",
      "batch: 4023, loss: 0.23170602321624756\n",
      "batch: 4024, loss: 0.13781824707984924\n",
      "batch: 4025, loss: 0.28216397762298584\n",
      "batch: 4026, loss: 0.33730348944664\n",
      "batch: 4027, loss: 0.08507395535707474\n",
      "batch: 4028, loss: 0.18617627024650574\n",
      "batch: 4029, loss: 0.17928987741470337\n",
      "batch: 4030, loss: 0.085821233689785\n",
      "batch: 4031, loss: 0.25633251667022705\n",
      "batch: 4032, loss: 0.14252042770385742\n",
      "batch: 4033, loss: 0.2200372815132141\n",
      "batch: 4034, loss: 0.3892178237438202\n",
      "batch: 4035, loss: 0.14066699147224426\n",
      "batch: 4036, loss: 0.2570074796676636\n",
      "batch: 4037, loss: 0.22760166227817535\n",
      "batch: 4038, loss: 0.14965344965457916\n",
      "batch: 4039, loss: 0.17651210725307465\n",
      "batch: 4040, loss: 0.18673285841941833\n",
      "batch: 4041, loss: 0.17943967878818512\n",
      "batch: 4042, loss: 0.3366003930568695\n",
      "batch: 4043, loss: 0.288266122341156\n",
      "batch: 4044, loss: 0.20470118522644043\n",
      "batch: 4045, loss: 0.22387373447418213\n",
      "batch: 4046, loss: 0.3113734722137451\n",
      "batch: 4047, loss: 0.15920045971870422\n",
      "batch: 4048, loss: 0.3490968644618988\n",
      "batch: 4049, loss: 0.29959484934806824\n",
      "batch: 4050, loss: 0.16454607248306274\n",
      "batch: 4051, loss: 0.20810633897781372\n",
      "batch: 4052, loss: 0.17516902089118958\n",
      "batch: 4053, loss: 0.26603400707244873\n",
      "batch: 4054, loss: 0.2050989270210266\n",
      "batch: 4055, loss: 0.3952709436416626\n",
      "batch: 4056, loss: 0.1868891417980194\n",
      "batch: 4057, loss: 0.2306332141160965\n",
      "batch: 4058, loss: 0.25552648305892944\n",
      "batch: 4059, loss: 0.24729084968566895\n",
      "batch: 4060, loss: 0.1669962853193283\n",
      "batch: 4061, loss: 0.1726253628730774\n",
      "batch: 4062, loss: 0.1992076337337494\n",
      "batch: 4063, loss: 0.2732471525669098\n",
      "batch: 4064, loss: 0.2414545714855194\n",
      "batch: 4065, loss: 0.22489602863788605\n",
      "batch: 4066, loss: 0.1497488170862198\n",
      "batch: 4067, loss: 0.12362655997276306\n",
      "batch: 4068, loss: 0.20496223866939545\n",
      "batch: 4069, loss: 0.16492706537246704\n",
      "batch: 4070, loss: 0.35796454548835754\n",
      "batch: 4071, loss: 0.2973635196685791\n",
      "batch: 4072, loss: 0.24171853065490723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 4073, loss: 0.21048389375209808\n",
      "batch: 4074, loss: 0.1728338599205017\n",
      "batch: 4075, loss: 0.14644396305084229\n",
      "batch: 4076, loss: 0.1787201166152954\n",
      "batch: 4077, loss: 0.06467457860708237\n",
      "batch: 4078, loss: 0.24704426527023315\n",
      "batch: 4079, loss: 0.26442644000053406\n",
      "batch: 4080, loss: 0.14495407044887543\n",
      "batch: 4081, loss: 0.3030429780483246\n",
      "batch: 4082, loss: 0.18791604042053223\n",
      "batch: 4083, loss: 0.37194597721099854\n",
      "batch: 4084, loss: 0.23204609751701355\n",
      "batch: 4085, loss: 0.2614991068840027\n",
      "batch: 4086, loss: 0.1219189465045929\n",
      "batch: 4087, loss: 0.21287330985069275\n",
      "batch: 4088, loss: 0.33872780203819275\n",
      "batch: 4089, loss: 0.2116648256778717\n",
      "batch: 4090, loss: 0.3684167265892029\n",
      "batch: 4091, loss: 0.23254446685314178\n",
      "batch: 4092, loss: 0.22243715822696686\n",
      "batch: 4093, loss: 0.15115106105804443\n",
      "batch: 4094, loss: 0.30053699016571045\n",
      "batch: 4095, loss: 0.16535347700119019\n",
      "batch: 4096, loss: 0.25193899869918823\n",
      "batch: 4097, loss: 0.14195869863033295\n",
      "batch: 4098, loss: 0.16280195116996765\n",
      "batch: 4099, loss: 0.15615057945251465\n",
      "batch: 4100, loss: 0.24216054379940033\n",
      "model saved to ./saving/model.ckpt-41\n",
      "batch: 4101, loss: 0.15855351090431213\n",
      "batch: 4102, loss: 0.3341849446296692\n",
      "batch: 4103, loss: 0.22593116760253906\n",
      "batch: 4104, loss: 0.2418464571237564\n",
      "batch: 4105, loss: 0.13866819441318512\n",
      "batch: 4106, loss: 0.22961005568504333\n",
      "batch: 4107, loss: 0.3053867220878601\n",
      "batch: 4108, loss: 0.38460490107536316\n",
      "batch: 4109, loss: 0.2223256528377533\n",
      "batch: 4110, loss: 0.16989600658416748\n",
      "batch: 4111, loss: 0.25188642740249634\n",
      "batch: 4112, loss: 0.1903497874736786\n",
      "batch: 4113, loss: 0.11245451867580414\n",
      "batch: 4114, loss: 0.25337299704551697\n",
      "batch: 4115, loss: 0.23865103721618652\n",
      "batch: 4116, loss: 0.13975924253463745\n",
      "batch: 4117, loss: 0.2596780061721802\n",
      "batch: 4118, loss: 0.12946519255638123\n",
      "batch: 4119, loss: 0.2418244183063507\n",
      "batch: 4120, loss: 0.15638063848018646\n",
      "batch: 4121, loss: 0.28294119238853455\n",
      "batch: 4122, loss: 0.30059757828712463\n",
      "batch: 4123, loss: 0.16194742918014526\n",
      "batch: 4124, loss: 0.1732088029384613\n",
      "batch: 4125, loss: 0.16411808133125305\n",
      "batch: 4126, loss: 0.34124425053596497\n",
      "batch: 4127, loss: 0.24888259172439575\n",
      "batch: 4128, loss: 0.2419356405735016\n",
      "batch: 4129, loss: 0.3002638816833496\n",
      "batch: 4130, loss: 0.45488008856773376\n",
      "batch: 4131, loss: 0.12051677703857422\n",
      "batch: 4132, loss: 0.23220479488372803\n",
      "batch: 4133, loss: 0.2864537239074707\n",
      "batch: 4134, loss: 0.17747852206230164\n",
      "batch: 4135, loss: 0.25085464119911194\n",
      "batch: 4136, loss: 0.1378389298915863\n",
      "batch: 4137, loss: 0.21867431700229645\n",
      "batch: 4138, loss: 0.10311625897884369\n",
      "batch: 4139, loss: 0.10324175655841827\n",
      "batch: 4140, loss: 0.20448237657546997\n",
      "batch: 4141, loss: 0.2533389925956726\n",
      "batch: 4142, loss: 0.162227600812912\n",
      "batch: 4143, loss: 0.21537475287914276\n",
      "batch: 4144, loss: 0.31133216619491577\n",
      "batch: 4145, loss: 0.36326318979263306\n",
      "batch: 4146, loss: 0.2738025188446045\n",
      "batch: 4147, loss: 0.30363768339157104\n",
      "batch: 4148, loss: 0.3020421266555786\n",
      "batch: 4149, loss: 0.19493864476680756\n",
      "batch: 4150, loss: 0.3441101312637329\n",
      "batch: 4151, loss: 0.19055289030075073\n",
      "batch: 4152, loss: 0.1866922676563263\n",
      "batch: 4153, loss: 0.11815501749515533\n",
      "batch: 4154, loss: 0.20265047252178192\n",
      "batch: 4155, loss: 0.2645338475704193\n",
      "batch: 4156, loss: 0.23479211330413818\n",
      "batch: 4157, loss: 0.21820583939552307\n",
      "batch: 4158, loss: 0.24397996068000793\n",
      "batch: 4159, loss: 0.12587526440620422\n",
      "batch: 4160, loss: 0.1648518592119217\n",
      "batch: 4161, loss: 0.2018246352672577\n",
      "batch: 4162, loss: 0.22053733468055725\n",
      "batch: 4163, loss: 0.2664541006088257\n",
      "batch: 4164, loss: 0.2132091522216797\n",
      "batch: 4165, loss: 0.33617934584617615\n",
      "batch: 4166, loss: 0.22950485348701477\n",
      "batch: 4167, loss: 0.16096849739551544\n",
      "batch: 4168, loss: 0.2039705514907837\n",
      "batch: 4169, loss: 0.10135282576084137\n",
      "batch: 4170, loss: 0.4086586534976959\n",
      "batch: 4171, loss: 0.16173794865608215\n",
      "batch: 4172, loss: 0.48568373918533325\n",
      "batch: 4173, loss: 0.23966582119464874\n",
      "batch: 4174, loss: 0.3330889940261841\n",
      "batch: 4175, loss: 0.14794602990150452\n",
      "batch: 4176, loss: 0.20427243411540985\n",
      "batch: 4177, loss: 0.2238319218158722\n",
      "batch: 4178, loss: 0.10258223116397858\n",
      "batch: 4179, loss: 0.18748226761817932\n",
      "batch: 4180, loss: 0.23394465446472168\n",
      "batch: 4181, loss: 0.2713161110877991\n",
      "batch: 4182, loss: 0.16503497958183289\n",
      "batch: 4183, loss: 0.17840197682380676\n",
      "batch: 4184, loss: 0.31635260581970215\n",
      "batch: 4185, loss: 0.18290163576602936\n",
      "batch: 4186, loss: 0.26477521657943726\n",
      "batch: 4187, loss: 0.15413495898246765\n",
      "batch: 4188, loss: 0.30504950881004333\n",
      "batch: 4189, loss: 0.1511072814464569\n",
      "batch: 4190, loss: 0.19577577710151672\n",
      "batch: 4191, loss: 0.13021114468574524\n",
      "batch: 4192, loss: 0.1144951581954956\n",
      "batch: 4193, loss: 0.28293633460998535\n",
      "batch: 4194, loss: 0.35089194774627686\n",
      "batch: 4195, loss: 0.1902601271867752\n",
      "batch: 4196, loss: 0.14837363362312317\n",
      "batch: 4197, loss: 0.11549244821071625\n",
      "batch: 4198, loss: 0.3719480633735657\n",
      "batch: 4199, loss: 0.19664376974105835\n",
      "batch: 4200, loss: 0.20014899969100952\n",
      "model saved to ./saving/model.ckpt-42\n",
      "batch: 4201, loss: 0.23426222801208496\n",
      "batch: 4202, loss: 0.1796605885028839\n",
      "batch: 4203, loss: 0.16348940134048462\n",
      "batch: 4204, loss: 0.25906461477279663\n",
      "batch: 4205, loss: 0.15180152654647827\n",
      "batch: 4206, loss: 0.30779311060905457\n",
      "batch: 4207, loss: 0.13673460483551025\n",
      "batch: 4208, loss: 0.34719952940940857\n",
      "batch: 4209, loss: 0.27912694215774536\n",
      "batch: 4210, loss: 0.16638517379760742\n",
      "batch: 4211, loss: 0.23963019251823425\n",
      "batch: 4212, loss: 0.2502121925354004\n",
      "batch: 4213, loss: 0.16200847923755646\n",
      "batch: 4214, loss: 0.17878907918930054\n",
      "batch: 4215, loss: 0.1691754162311554\n",
      "batch: 4216, loss: 0.23170232772827148\n",
      "batch: 4217, loss: 0.1977842152118683\n",
      "batch: 4218, loss: 0.4313170313835144\n",
      "batch: 4219, loss: 0.30253034830093384\n",
      "batch: 4220, loss: 0.18668095767498016\n",
      "batch: 4221, loss: 0.27156686782836914\n",
      "batch: 4222, loss: 0.1454806923866272\n",
      "batch: 4223, loss: 0.22654767334461212\n",
      "batch: 4224, loss: 0.4182288944721222\n",
      "batch: 4225, loss: 0.35697460174560547\n",
      "batch: 4226, loss: 0.36451607942581177\n",
      "batch: 4227, loss: 0.3369016945362091\n",
      "batch: 4228, loss: 0.1237037181854248\n",
      "batch: 4229, loss: 0.2932027280330658\n",
      "batch: 4230, loss: 0.26069188117980957\n",
      "batch: 4231, loss: 0.18584948778152466\n",
      "batch: 4232, loss: 0.1546868532896042\n",
      "batch: 4233, loss: 0.22053679823875427\n",
      "batch: 4234, loss: 0.23062291741371155\n",
      "batch: 4235, loss: 0.20629779994487762\n",
      "batch: 4236, loss: 0.16097556054592133\n",
      "batch: 4237, loss: 0.1978876292705536\n",
      "batch: 4238, loss: 0.19947147369384766\n",
      "batch: 4239, loss: 0.2754210829734802\n",
      "batch: 4240, loss: 0.29981452226638794\n",
      "batch: 4241, loss: 0.2592482566833496\n",
      "batch: 4242, loss: 0.18610632419586182\n",
      "batch: 4243, loss: 0.19843268394470215\n",
      "batch: 4244, loss: 0.21835312247276306\n",
      "batch: 4245, loss: 0.2169860303401947\n",
      "batch: 4246, loss: 0.3191003203392029\n",
      "batch: 4247, loss: 0.23672515153884888\n",
      "batch: 4248, loss: 0.20640359818935394\n",
      "batch: 4249, loss: 0.3174683153629303\n",
      "batch: 4250, loss: 0.12041711807250977\n",
      "batch: 4251, loss: 0.21598660945892334\n",
      "batch: 4252, loss: 0.11793637275695801\n",
      "batch: 4253, loss: 0.08732796460390091\n",
      "batch: 4254, loss: 0.2522101402282715\n",
      "batch: 4255, loss: 0.1796436309814453\n",
      "batch: 4256, loss: 0.17464876174926758\n",
      "batch: 4257, loss: 0.23183663189411163\n",
      "batch: 4258, loss: 0.2765173017978668\n",
      "batch: 4259, loss: 0.37250572443008423\n",
      "batch: 4260, loss: 0.13874706625938416\n",
      "batch: 4261, loss: 0.2907993197441101\n",
      "batch: 4262, loss: 0.3646659851074219\n",
      "batch: 4263, loss: 0.12840735912322998\n",
      "batch: 4264, loss: 0.33654308319091797\n",
      "batch: 4265, loss: 0.35483890771865845\n",
      "batch: 4266, loss: 0.14854927361011505\n",
      "batch: 4267, loss: 0.22939851880073547\n",
      "batch: 4268, loss: 0.3845449984073639\n",
      "batch: 4269, loss: 0.1592371165752411\n",
      "batch: 4270, loss: 0.42271289229393005\n",
      "batch: 4271, loss: 0.22532829642295837\n",
      "batch: 4272, loss: 0.332345187664032\n",
      "batch: 4273, loss: 0.16090844571590424\n",
      "batch: 4274, loss: 0.34419047832489014\n",
      "batch: 4275, loss: 0.33692461252212524\n",
      "batch: 4276, loss: 0.23059698939323425\n",
      "batch: 4277, loss: 0.12473386526107788\n",
      "batch: 4278, loss: 0.26153111457824707\n",
      "batch: 4279, loss: 0.25423771142959595\n",
      "batch: 4280, loss: 0.1421251893043518\n",
      "batch: 4281, loss: 0.17787468433380127\n",
      "batch: 4282, loss: 0.1411781907081604\n",
      "batch: 4283, loss: 0.3295179307460785\n",
      "batch: 4284, loss: 0.19707177579402924\n",
      "batch: 4285, loss: 0.19117572903633118\n",
      "batch: 4286, loss: 0.20738865435123444\n",
      "batch: 4287, loss: 0.25775277614593506\n",
      "batch: 4288, loss: 0.2360544353723526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 4289, loss: 0.25270020961761475\n",
      "batch: 4290, loss: 0.17752599716186523\n",
      "batch: 4291, loss: 0.16633503139019012\n",
      "batch: 4292, loss: 0.19587114453315735\n",
      "batch: 4293, loss: 0.16283994913101196\n",
      "batch: 4294, loss: 0.28603193163871765\n",
      "batch: 4295, loss: 0.30465996265411377\n",
      "batch: 4296, loss: 0.28080615401268005\n",
      "batch: 4297, loss: 0.2737894058227539\n",
      "batch: 4298, loss: 0.15577386319637299\n",
      "batch: 4299, loss: 0.24291442334651947\n",
      "batch: 4300, loss: 0.23899149894714355\n",
      "model saved to ./saving/model.ckpt-43\n",
      "batch: 4301, loss: 0.14867819845676422\n",
      "batch: 4302, loss: 0.20512528717517853\n",
      "batch: 4303, loss: 0.14431752264499664\n",
      "batch: 4304, loss: 0.308040052652359\n",
      "batch: 4305, loss: 0.15610966086387634\n",
      "batch: 4306, loss: 0.3072322607040405\n",
      "batch: 4307, loss: 0.24535691738128662\n",
      "batch: 4308, loss: 0.1617080569267273\n",
      "batch: 4309, loss: 0.16168387234210968\n",
      "batch: 4310, loss: 0.27608731389045715\n",
      "batch: 4311, loss: 0.14220860600471497\n",
      "batch: 4312, loss: 0.24466879665851593\n",
      "batch: 4313, loss: 0.3748950958251953\n",
      "batch: 4314, loss: 0.3048640787601471\n",
      "batch: 4315, loss: 0.262930303812027\n",
      "batch: 4316, loss: 0.3819415867328644\n",
      "batch: 4317, loss: 0.1829373687505722\n",
      "batch: 4318, loss: 0.29132604598999023\n",
      "batch: 4319, loss: 0.18355508148670197\n",
      "batch: 4320, loss: 0.2085677981376648\n",
      "batch: 4321, loss: 0.3782525658607483\n",
      "batch: 4322, loss: 0.2174406349658966\n",
      "batch: 4323, loss: 0.31617552042007446\n",
      "batch: 4324, loss: 0.30054527521133423\n",
      "batch: 4325, loss: 0.2367793321609497\n",
      "batch: 4326, loss: 0.1897744983434677\n",
      "batch: 4327, loss: 0.1963389813899994\n",
      "batch: 4328, loss: 0.18233656883239746\n",
      "batch: 4329, loss: 0.375294953584671\n",
      "batch: 4330, loss: 0.2558063268661499\n",
      "batch: 4331, loss: 0.34293171763420105\n",
      "batch: 4332, loss: 0.13796886801719666\n",
      "batch: 4333, loss: 0.24773520231246948\n",
      "batch: 4334, loss: 0.3850744962692261\n",
      "batch: 4335, loss: 0.4245511293411255\n",
      "batch: 4336, loss: 0.2718597650527954\n",
      "batch: 4337, loss: 0.1745532602071762\n",
      "batch: 4338, loss: 0.10409985482692719\n",
      "batch: 4339, loss: 0.2910500168800354\n",
      "batch: 4340, loss: 0.29860079288482666\n",
      "batch: 4341, loss: 0.15817448496818542\n",
      "batch: 4342, loss: 0.18224729597568512\n",
      "batch: 4343, loss: 0.14617423713207245\n",
      "batch: 4344, loss: 0.3969588279724121\n",
      "batch: 4345, loss: 0.2286144644021988\n",
      "batch: 4346, loss: 0.14235930144786835\n",
      "batch: 4347, loss: 0.15085405111312866\n",
      "batch: 4348, loss: 0.4320722222328186\n",
      "batch: 4349, loss: 0.3054749369621277\n",
      "batch: 4350, loss: 0.20350822806358337\n",
      "batch: 4351, loss: 0.35682541131973267\n",
      "batch: 4352, loss: 0.24658650159835815\n",
      "batch: 4353, loss: 0.13516952097415924\n",
      "batch: 4354, loss: 0.19392220675945282\n",
      "batch: 4355, loss: 0.1337248980998993\n",
      "batch: 4356, loss: 0.13543012738227844\n",
      "batch: 4357, loss: 0.22485420107841492\n",
      "batch: 4358, loss: 0.24647364020347595\n",
      "batch: 4359, loss: 0.18268485367298126\n",
      "batch: 4360, loss: 0.1900162398815155\n",
      "batch: 4361, loss: 0.2322978377342224\n",
      "batch: 4362, loss: 0.1858452409505844\n",
      "batch: 4363, loss: 0.32710516452789307\n",
      "batch: 4364, loss: 0.15029919147491455\n",
      "batch: 4365, loss: 0.31722503900527954\n",
      "batch: 4366, loss: 0.24972540140151978\n",
      "batch: 4367, loss: 0.3105567991733551\n",
      "batch: 4368, loss: 0.1366206556558609\n",
      "batch: 4369, loss: 0.20641931891441345\n",
      "batch: 4370, loss: 0.26663339138031006\n",
      "batch: 4371, loss: 0.10409075021743774\n",
      "batch: 4372, loss: 0.20247578620910645\n",
      "batch: 4373, loss: 0.28255146741867065\n",
      "batch: 4374, loss: 0.22115816175937653\n",
      "batch: 4375, loss: 0.23188114166259766\n",
      "batch: 4376, loss: 0.23392213881015778\n",
      "batch: 4377, loss: 0.22542278468608856\n",
      "batch: 4378, loss: 0.11300645768642426\n",
      "batch: 4379, loss: 0.16239039599895477\n",
      "batch: 4380, loss: 0.23217077553272247\n",
      "batch: 4381, loss: 0.2035146951675415\n",
      "batch: 4382, loss: 0.22311151027679443\n",
      "batch: 4383, loss: 0.3966928720474243\n",
      "batch: 4384, loss: 0.29466524720191956\n",
      "batch: 4385, loss: 0.11948825418949127\n",
      "batch: 4386, loss: 0.31687498092651367\n",
      "batch: 4387, loss: 0.3179389536380768\n",
      "batch: 4388, loss: 0.3495821952819824\n",
      "batch: 4389, loss: 0.2000419795513153\n",
      "batch: 4390, loss: 0.13620485365390778\n",
      "batch: 4391, loss: 0.3608390688896179\n",
      "batch: 4392, loss: 0.167606383562088\n",
      "batch: 4393, loss: 0.31929051876068115\n",
      "batch: 4394, loss: 0.11508992314338684\n",
      "batch: 4395, loss: 0.15256276726722717\n",
      "batch: 4396, loss: 0.22747910022735596\n",
      "batch: 4397, loss: 0.21547868847846985\n",
      "batch: 4398, loss: 0.26368391513824463\n",
      "batch: 4399, loss: 0.22204609215259552\n",
      "batch: 4400, loss: 0.16419684886932373\n",
      "model saved to ./saving/model.ckpt-44\n",
      "batch: 4401, loss: 0.35529279708862305\n",
      "batch: 4402, loss: 0.10970105230808258\n",
      "batch: 4403, loss: 0.19968962669372559\n",
      "batch: 4404, loss: 0.15612028539180756\n",
      "batch: 4405, loss: 0.35946232080459595\n",
      "batch: 4406, loss: 0.25497135519981384\n",
      "batch: 4407, loss: 0.36836016178131104\n",
      "batch: 4408, loss: 0.21632090210914612\n",
      "batch: 4409, loss: 0.223899245262146\n",
      "batch: 4410, loss: 0.3695335388183594\n",
      "batch: 4411, loss: 0.13620254397392273\n",
      "batch: 4412, loss: 0.3115694522857666\n",
      "batch: 4413, loss: 0.2165561020374298\n",
      "batch: 4414, loss: 0.1381833851337433\n",
      "batch: 4415, loss: 0.315250039100647\n",
      "batch: 4416, loss: 0.22911016643047333\n",
      "batch: 4417, loss: 0.3899194598197937\n",
      "batch: 4418, loss: 0.25968044996261597\n",
      "batch: 4419, loss: 0.23626911640167236\n",
      "batch: 4420, loss: 0.14619165658950806\n",
      "batch: 4421, loss: 0.1811276376247406\n",
      "batch: 4422, loss: 0.24079838395118713\n",
      "batch: 4423, loss: 0.343635618686676\n",
      "batch: 4424, loss: 0.23430484533309937\n",
      "batch: 4425, loss: 0.22490838170051575\n",
      "batch: 4426, loss: 0.24182918667793274\n",
      "batch: 4427, loss: 0.22221903502941132\n",
      "batch: 4428, loss: 0.30908721685409546\n",
      "batch: 4429, loss: 0.22252583503723145\n",
      "batch: 4430, loss: 0.21816270053386688\n",
      "batch: 4431, loss: 0.11579049378633499\n",
      "batch: 4432, loss: 0.570820689201355\n",
      "batch: 4433, loss: 0.26169055700302124\n",
      "batch: 4434, loss: 0.2054484486579895\n",
      "batch: 4435, loss: 0.3569015860557556\n",
      "batch: 4436, loss: 0.3306437134742737\n",
      "batch: 4437, loss: 0.12694787979125977\n",
      "batch: 4438, loss: 0.08877637982368469\n",
      "batch: 4439, loss: 0.20118072628974915\n",
      "batch: 4440, loss: 0.11981108784675598\n",
      "batch: 4441, loss: 0.1774931102991104\n",
      "batch: 4442, loss: 0.32716700434684753\n",
      "batch: 4443, loss: 0.1819334328174591\n",
      "batch: 4444, loss: 0.1482963114976883\n",
      "batch: 4445, loss: 0.30345749855041504\n",
      "batch: 4446, loss: 0.2595398426055908\n",
      "batch: 4447, loss: 0.23330287635326385\n",
      "batch: 4448, loss: 0.1712760329246521\n",
      "batch: 4449, loss: 0.18905770778656006\n",
      "batch: 4450, loss: 0.2945847809314728\n",
      "batch: 4451, loss: 0.23532895743846893\n",
      "batch: 4452, loss: 0.24482738971710205\n",
      "batch: 4453, loss: 0.2926299273967743\n",
      "batch: 4454, loss: 0.19053396582603455\n",
      "batch: 4455, loss: 0.12208346277475357\n",
      "batch: 4456, loss: 0.21565504372119904\n",
      "batch: 4457, loss: 0.2166275680065155\n",
      "batch: 4458, loss: 0.17851752042770386\n",
      "batch: 4459, loss: 0.23476752638816833\n",
      "batch: 4460, loss: 0.2142394483089447\n",
      "batch: 4461, loss: 0.21688690781593323\n",
      "batch: 4462, loss: 0.20878297090530396\n",
      "batch: 4463, loss: 0.15653082728385925\n",
      "batch: 4464, loss: 0.3634601831436157\n",
      "batch: 4465, loss: 0.1678902506828308\n",
      "batch: 4466, loss: 0.3579341173171997\n",
      "batch: 4467, loss: 0.20554310083389282\n",
      "batch: 4468, loss: 0.19715121388435364\n",
      "batch: 4469, loss: 0.23436954617500305\n",
      "batch: 4470, loss: 0.29695022106170654\n",
      "batch: 4471, loss: 0.21336570382118225\n",
      "batch: 4472, loss: 0.2860812246799469\n",
      "batch: 4473, loss: 0.228005051612854\n",
      "batch: 4474, loss: 0.23070654273033142\n",
      "batch: 4475, loss: 0.16134338080883026\n",
      "batch: 4476, loss: 0.2515154480934143\n",
      "batch: 4477, loss: 0.27962708473205566\n",
      "batch: 4478, loss: 0.23984748125076294\n",
      "batch: 4479, loss: 0.24256570637226105\n",
      "batch: 4480, loss: 0.27427157759666443\n",
      "batch: 4481, loss: 0.25506332516670227\n",
      "batch: 4482, loss: 0.11300214380025864\n",
      "batch: 4483, loss: 0.10878650844097137\n",
      "batch: 4484, loss: 0.18668624758720398\n",
      "batch: 4485, loss: 0.13817918300628662\n",
      "batch: 4486, loss: 0.17559903860092163\n",
      "batch: 4487, loss: 0.41280755400657654\n",
      "batch: 4488, loss: 0.363714337348938\n",
      "batch: 4489, loss: 0.2164200246334076\n",
      "batch: 4490, loss: 0.1567113697528839\n",
      "batch: 4491, loss: 0.19607335329055786\n",
      "batch: 4492, loss: 0.21477508544921875\n",
      "batch: 4493, loss: 0.4631081819534302\n",
      "batch: 4494, loss: 0.13884560763835907\n",
      "batch: 4495, loss: 0.21300601959228516\n",
      "batch: 4496, loss: 0.4124870300292969\n",
      "batch: 4497, loss: 0.15040068328380585\n",
      "batch: 4498, loss: 0.15335234999656677\n",
      "batch: 4499, loss: 0.17817029356956482\n",
      "batch: 4500, loss: 0.18468034267425537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved to ./saving/model.ckpt-45\n",
      "batch: 4501, loss: 0.23821532726287842\n",
      "batch: 4502, loss: 0.2253998965024948\n",
      "batch: 4503, loss: 0.20508427917957306\n",
      "batch: 4504, loss: 0.3346537947654724\n",
      "batch: 4505, loss: 0.32728779315948486\n",
      "batch: 4506, loss: 0.395719975233078\n",
      "batch: 4507, loss: 0.17814719676971436\n",
      "batch: 4508, loss: 0.1564370095729828\n",
      "batch: 4509, loss: 0.29201740026474\n",
      "batch: 4510, loss: 0.13546377420425415\n",
      "batch: 4511, loss: 0.2222495675086975\n",
      "batch: 4512, loss: 0.13009938597679138\n",
      "batch: 4513, loss: 0.1925051510334015\n",
      "batch: 4514, loss: 0.23747429251670837\n",
      "batch: 4515, loss: 0.1655011922121048\n",
      "batch: 4516, loss: 0.2521095275878906\n",
      "batch: 4517, loss: 0.2769278883934021\n",
      "batch: 4518, loss: 0.19924455881118774\n",
      "batch: 4519, loss: 0.18306952714920044\n",
      "batch: 4520, loss: 0.2799485921859741\n",
      "batch: 4521, loss: 0.24091702699661255\n",
      "batch: 4522, loss: 0.4387176036834717\n",
      "batch: 4523, loss: 0.15744096040725708\n",
      "batch: 4524, loss: 0.10149051249027252\n",
      "batch: 4525, loss: 0.23208998143672943\n",
      "batch: 4526, loss: 0.271401971578598\n",
      "batch: 4527, loss: 0.15518513321876526\n",
      "batch: 4528, loss: 0.08123840391635895\n",
      "batch: 4529, loss: 0.2557647228240967\n",
      "batch: 4530, loss: 0.18319809436798096\n",
      "batch: 4531, loss: 0.3867183327674866\n",
      "batch: 4532, loss: 0.22306928038597107\n",
      "batch: 4533, loss: 0.15536823868751526\n",
      "batch: 4534, loss: 0.2047957181930542\n",
      "batch: 4535, loss: 0.23966272175312042\n",
      "batch: 4536, loss: 0.2872704267501831\n",
      "batch: 4537, loss: 0.27183985710144043\n",
      "batch: 4538, loss: 0.19135017693042755\n",
      "batch: 4539, loss: 0.18402929604053497\n",
      "batch: 4540, loss: 0.2102222889661789\n",
      "batch: 4541, loss: 0.3298807740211487\n",
      "batch: 4542, loss: 0.0852045789361\n",
      "batch: 4543, loss: 0.2848902940750122\n",
      "batch: 4544, loss: 0.37469571828842163\n",
      "batch: 4545, loss: 0.19262781739234924\n",
      "batch: 4546, loss: 0.14656059443950653\n",
      "batch: 4547, loss: 0.16905930638313293\n",
      "batch: 4548, loss: 0.20779265463352203\n",
      "batch: 4549, loss: 0.1957562267780304\n",
      "batch: 4550, loss: 0.24180030822753906\n",
      "batch: 4551, loss: 0.2095540165901184\n",
      "batch: 4552, loss: 0.12479222565889359\n",
      "batch: 4553, loss: 0.2293095886707306\n",
      "batch: 4554, loss: 0.12521015107631683\n",
      "batch: 4555, loss: 0.12382999062538147\n",
      "batch: 4556, loss: 0.23292280733585358\n",
      "batch: 4557, loss: 0.19963571429252625\n",
      "batch: 4558, loss: 0.1360969990491867\n",
      "batch: 4559, loss: 0.14397308230400085\n",
      "batch: 4560, loss: 0.2089948207139969\n",
      "batch: 4561, loss: 0.1474069356918335\n",
      "batch: 4562, loss: 0.21376383304595947\n",
      "batch: 4563, loss: 0.3424409031867981\n",
      "batch: 4564, loss: 0.17008504271507263\n",
      "batch: 4565, loss: 0.1665302962064743\n",
      "batch: 4566, loss: 0.38039952516555786\n",
      "batch: 4567, loss: 0.280756413936615\n",
      "batch: 4568, loss: 0.11773949861526489\n",
      "batch: 4569, loss: 0.31923720240592957\n",
      "batch: 4570, loss: 0.24485374987125397\n",
      "batch: 4571, loss: 0.1350507140159607\n",
      "batch: 4572, loss: 0.11465530842542648\n",
      "batch: 4573, loss: 0.32152533531188965\n",
      "batch: 4574, loss: 0.14223125576972961\n",
      "batch: 4575, loss: 0.25849971175193787\n",
      "batch: 4576, loss: 0.14993661642074585\n",
      "batch: 4577, loss: 0.21038612723350525\n",
      "batch: 4578, loss: 0.32245689630508423\n",
      "batch: 4579, loss: 0.09994950145483017\n",
      "batch: 4580, loss: 0.19111225008964539\n",
      "batch: 4581, loss: 0.29425638914108276\n",
      "batch: 4582, loss: 0.18409420549869537\n",
      "batch: 4583, loss: 0.14835792779922485\n",
      "batch: 4584, loss: 0.17605507373809814\n",
      "batch: 4585, loss: 0.27252766489982605\n",
      "batch: 4586, loss: 0.13585485517978668\n",
      "batch: 4587, loss: 0.32178640365600586\n",
      "batch: 4588, loss: 0.17774540185928345\n",
      "batch: 4589, loss: 0.10598573833703995\n",
      "batch: 4590, loss: 0.22748485207557678\n",
      "batch: 4591, loss: 0.17054645717144012\n",
      "batch: 4592, loss: 0.09997820854187012\n",
      "batch: 4593, loss: 0.2543352246284485\n",
      "batch: 4594, loss: 0.2629619538784027\n",
      "batch: 4595, loss: 0.2513763904571533\n",
      "batch: 4596, loss: 0.3178848624229431\n",
      "batch: 4597, loss: 0.27463915944099426\n",
      "batch: 4598, loss: 0.1842191219329834\n",
      "batch: 4599, loss: 0.15750612318515778\n",
      "batch: 4600, loss: 0.2160005122423172\n",
      "model saved to ./saving/model.ckpt-46\n",
      "batch: 4601, loss: 0.3597884774208069\n",
      "batch: 4602, loss: 0.2983129024505615\n",
      "batch: 4603, loss: 0.31501734256744385\n",
      "batch: 4604, loss: 0.2095489203929901\n",
      "batch: 4605, loss: 0.3046467900276184\n",
      "batch: 4606, loss: 0.11205606162548065\n",
      "batch: 4607, loss: 0.22682926058769226\n",
      "batch: 4608, loss: 0.13966284692287445\n",
      "batch: 4609, loss: 0.35268914699554443\n",
      "batch: 4610, loss: 0.35982292890548706\n",
      "batch: 4611, loss: 0.28164300322532654\n",
      "batch: 4612, loss: 0.23251762986183167\n",
      "batch: 4613, loss: 0.2679256796836853\n",
      "batch: 4614, loss: 0.10835886746644974\n",
      "batch: 4615, loss: 0.3691768944263458\n",
      "batch: 4616, loss: 0.16096745431423187\n",
      "batch: 4617, loss: 0.26271122694015503\n",
      "batch: 4618, loss: 0.22356022894382477\n",
      "batch: 4619, loss: 0.22352571785449982\n",
      "batch: 4620, loss: 0.19298680126667023\n",
      "batch: 4621, loss: 0.24453376233577728\n",
      "batch: 4622, loss: 0.19431492686271667\n",
      "batch: 4623, loss: 0.16314563155174255\n",
      "batch: 4624, loss: 0.23783709108829498\n",
      "batch: 4625, loss: 0.12373840063810349\n",
      "batch: 4626, loss: 0.31081467866897583\n",
      "batch: 4627, loss: 0.1795506626367569\n",
      "batch: 4628, loss: 0.21795469522476196\n",
      "batch: 4629, loss: 0.5179482698440552\n",
      "batch: 4630, loss: 0.2000550925731659\n",
      "batch: 4631, loss: 0.15789346396923065\n",
      "batch: 4632, loss: 0.1434350162744522\n",
      "batch: 4633, loss: 0.1783190816640854\n",
      "batch: 4634, loss: 0.2965666353702545\n",
      "batch: 4635, loss: 0.11647878587245941\n",
      "batch: 4636, loss: 0.15253472328186035\n",
      "batch: 4637, loss: 0.19768071174621582\n",
      "batch: 4638, loss: 0.22221580147743225\n",
      "batch: 4639, loss: 0.18292340636253357\n",
      "batch: 4640, loss: 0.32309216260910034\n",
      "batch: 4641, loss: 0.3311840295791626\n",
      "batch: 4642, loss: 0.3299519419670105\n",
      "batch: 4643, loss: 0.17024941742420197\n",
      "batch: 4644, loss: 0.1743689328432083\n",
      "batch: 4645, loss: 0.5153478384017944\n",
      "batch: 4646, loss: 0.37773656845092773\n",
      "batch: 4647, loss: 0.16582491993904114\n",
      "batch: 4648, loss: 0.13223867118358612\n",
      "batch: 4649, loss: 0.19133886694908142\n",
      "batch: 4650, loss: 0.21729695796966553\n",
      "batch: 4651, loss: 0.2254866361618042\n",
      "batch: 4652, loss: 0.11235259473323822\n",
      "batch: 4653, loss: 0.09237571060657501\n",
      "batch: 4654, loss: 0.27486681938171387\n",
      "batch: 4655, loss: 0.1947595179080963\n",
      "batch: 4656, loss: 0.3333028256893158\n",
      "batch: 4657, loss: 0.11124148219823837\n",
      "batch: 4658, loss: 0.15134404599666595\n",
      "batch: 4659, loss: 0.20191910862922668\n",
      "batch: 4660, loss: 0.3071114122867584\n",
      "batch: 4661, loss: 0.45823222398757935\n",
      "batch: 4662, loss: 0.15299968421459198\n",
      "batch: 4663, loss: 0.09171493351459503\n",
      "batch: 4664, loss: 0.18143171072006226\n",
      "batch: 4665, loss: 0.2007971554994583\n",
      "batch: 4666, loss: 0.29782986640930176\n",
      "batch: 4667, loss: 0.3057403266429901\n",
      "batch: 4668, loss: 0.12719446420669556\n",
      "batch: 4669, loss: 0.4768756031990051\n",
      "batch: 4670, loss: 0.3451276421546936\n",
      "batch: 4671, loss: 0.16326436400413513\n",
      "batch: 4672, loss: 0.1795370876789093\n",
      "batch: 4673, loss: 0.18800587952136993\n",
      "batch: 4674, loss: 0.11758559942245483\n",
      "batch: 4675, loss: 0.30977505445480347\n",
      "batch: 4676, loss: 0.22141577303409576\n",
      "batch: 4677, loss: 0.13969513773918152\n",
      "batch: 4678, loss: 0.18569841980934143\n",
      "batch: 4679, loss: 0.2667733132839203\n",
      "batch: 4680, loss: 0.30546456575393677\n",
      "batch: 4681, loss: 0.15519767999649048\n",
      "batch: 4682, loss: 0.2921588718891144\n",
      "batch: 4683, loss: 0.14005111157894135\n",
      "batch: 4684, loss: 0.14548778533935547\n",
      "batch: 4685, loss: 0.2345152646303177\n",
      "batch: 4686, loss: 0.21387538313865662\n",
      "batch: 4687, loss: 0.20001885294914246\n",
      "batch: 4688, loss: 0.20983532071113586\n",
      "batch: 4689, loss: 0.21727395057678223\n",
      "batch: 4690, loss: 0.15454699099063873\n",
      "batch: 4691, loss: 0.22078010439872742\n",
      "batch: 4692, loss: 0.1858430802822113\n",
      "batch: 4693, loss: 0.3449426293373108\n",
      "batch: 4694, loss: 0.1441459059715271\n",
      "batch: 4695, loss: 0.09775811433792114\n",
      "batch: 4696, loss: 0.19498369097709656\n",
      "batch: 4697, loss: 0.1593824028968811\n",
      "batch: 4698, loss: 0.3909144997596741\n",
      "batch: 4699, loss: 0.19009071588516235\n",
      "batch: 4700, loss: 0.18669402599334717\n",
      "model saved to ./saving/model.ckpt-47\n",
      "batch: 4701, loss: 0.29291674494743347\n",
      "batch: 4702, loss: 0.15900972485542297\n",
      "batch: 4703, loss: 0.21911993622779846\n",
      "batch: 4704, loss: 0.2689625024795532\n",
      "batch: 4705, loss: 0.24443158507347107\n",
      "batch: 4706, loss: 0.19547885656356812\n",
      "batch: 4707, loss: 0.15705284476280212\n",
      "batch: 4708, loss: 0.21079686284065247\n",
      "batch: 4709, loss: 0.28318721055984497\n",
      "batch: 4710, loss: 0.3092414438724518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 4711, loss: 0.19454607367515564\n",
      "batch: 4712, loss: 0.22180205583572388\n",
      "batch: 4713, loss: 0.13627469539642334\n",
      "batch: 4714, loss: 0.14311948418617249\n",
      "batch: 4715, loss: 0.20560938119888306\n",
      "batch: 4716, loss: 0.2667323648929596\n",
      "batch: 4717, loss: 0.25680869817733765\n",
      "batch: 4718, loss: 0.26607176661491394\n",
      "batch: 4719, loss: 0.3088437616825104\n",
      "batch: 4720, loss: 0.2149488627910614\n",
      "batch: 4721, loss: 0.14480414986610413\n",
      "batch: 4722, loss: 0.14132501184940338\n",
      "batch: 4723, loss: 0.22999721765518188\n",
      "batch: 4724, loss: 0.16267919540405273\n",
      "batch: 4725, loss: 0.08227730542421341\n",
      "batch: 4726, loss: 0.13352462649345398\n",
      "batch: 4727, loss: 0.146141916513443\n",
      "batch: 4728, loss: 0.17402128875255585\n",
      "batch: 4729, loss: 0.2383652627468109\n",
      "batch: 4730, loss: 0.27846306562423706\n",
      "batch: 4731, loss: 0.27372342348098755\n",
      "batch: 4732, loss: 0.12674948573112488\n",
      "batch: 4733, loss: 0.09818839281797409\n",
      "batch: 4734, loss: 0.18605323135852814\n",
      "batch: 4735, loss: 0.2523651719093323\n",
      "batch: 4736, loss: 0.2604556679725647\n",
      "batch: 4737, loss: 0.2559068202972412\n",
      "batch: 4738, loss: 0.2290692925453186\n",
      "batch: 4739, loss: 0.3849055767059326\n",
      "batch: 4740, loss: 0.12874795496463776\n",
      "batch: 4741, loss: 0.24754998087882996\n",
      "batch: 4742, loss: 0.19791167974472046\n",
      "batch: 4743, loss: 0.15444520115852356\n",
      "batch: 4744, loss: 0.11457876861095428\n",
      "batch: 4745, loss: 0.23188626766204834\n",
      "batch: 4746, loss: 0.13369503617286682\n",
      "batch: 4747, loss: 0.16426753997802734\n",
      "batch: 4748, loss: 0.18446463346481323\n",
      "batch: 4749, loss: 0.28471118211746216\n",
      "batch: 4750, loss: 0.22026342153549194\n",
      "batch: 4751, loss: 0.15713423490524292\n",
      "batch: 4752, loss: 0.2593093514442444\n",
      "batch: 4753, loss: 0.11253749579191208\n",
      "batch: 4754, loss: 0.269275963306427\n",
      "batch: 4755, loss: 0.32167893648147583\n",
      "batch: 4756, loss: 0.35053253173828125\n",
      "batch: 4757, loss: 0.20181632041931152\n",
      "batch: 4758, loss: 0.24939888715744019\n",
      "batch: 4759, loss: 0.19398735463619232\n",
      "batch: 4760, loss: 0.18605078756809235\n",
      "batch: 4761, loss: 0.33070772886276245\n",
      "batch: 4762, loss: 0.1906818300485611\n",
      "batch: 4763, loss: 0.13498733937740326\n",
      "batch: 4764, loss: 0.14017178118228912\n",
      "batch: 4765, loss: 0.18907152116298676\n",
      "batch: 4766, loss: 0.21502557396888733\n",
      "batch: 4767, loss: 0.27791357040405273\n",
      "batch: 4768, loss: 0.2023133486509323\n",
      "batch: 4769, loss: 0.19232380390167236\n",
      "batch: 4770, loss: 0.05681811273097992\n",
      "batch: 4771, loss: 0.23322969675064087\n",
      "batch: 4772, loss: 0.3216535151004791\n",
      "batch: 4773, loss: 0.17167231440544128\n",
      "batch: 4774, loss: 0.28286078572273254\n",
      "batch: 4775, loss: 0.16122746467590332\n",
      "batch: 4776, loss: 0.24080303311347961\n",
      "batch: 4777, loss: 0.2383594661951065\n",
      "batch: 4778, loss: 0.13444584608078003\n",
      "batch: 4779, loss: 0.20353029668331146\n",
      "batch: 4780, loss: 0.12501999735832214\n",
      "batch: 4781, loss: 0.14895233511924744\n",
      "batch: 4782, loss: 0.3343927264213562\n",
      "batch: 4783, loss: 0.1422293782234192\n",
      "batch: 4784, loss: 0.2781171202659607\n",
      "batch: 4785, loss: 0.17260339856147766\n",
      "batch: 4786, loss: 0.17059779167175293\n",
      "batch: 4787, loss: 0.13536742329597473\n",
      "batch: 4788, loss: 0.18902990221977234\n",
      "batch: 4789, loss: 0.2706153392791748\n",
      "batch: 4790, loss: 0.2408849596977234\n",
      "batch: 4791, loss: 0.26905807852745056\n",
      "batch: 4792, loss: 0.1348559558391571\n",
      "batch: 4793, loss: 0.23756945133209229\n",
      "batch: 4794, loss: 0.29395270347595215\n",
      "batch: 4795, loss: 0.27708250284194946\n",
      "batch: 4796, loss: 0.19557960331439972\n",
      "batch: 4797, loss: 0.1552170366048813\n",
      "batch: 4798, loss: 0.1667177826166153\n",
      "batch: 4799, loss: 0.2135343700647354\n",
      "batch: 4800, loss: 0.23807057738304138\n",
      "model saved to ./saving/model.ckpt-48\n",
      "batch: 4801, loss: 0.16513729095458984\n",
      "batch: 4802, loss: 0.18136164546012878\n",
      "batch: 4803, loss: 0.27945244312286377\n",
      "batch: 4804, loss: 0.1503542959690094\n",
      "batch: 4805, loss: 0.1517217457294464\n",
      "batch: 4806, loss: 0.1865239441394806\n",
      "batch: 4807, loss: 0.2096327543258667\n",
      "batch: 4808, loss: 0.10789555311203003\n",
      "batch: 4809, loss: 0.12511330842971802\n",
      "batch: 4810, loss: 0.18091082572937012\n",
      "batch: 4811, loss: 0.18504947423934937\n",
      "batch: 4812, loss: 0.30022862553596497\n",
      "batch: 4813, loss: 0.1724342405796051\n",
      "batch: 4814, loss: 0.2379198968410492\n",
      "batch: 4815, loss: 0.32758066058158875\n",
      "batch: 4816, loss: 0.15267080068588257\n",
      "batch: 4817, loss: 0.15750885009765625\n",
      "batch: 4818, loss: 0.19352863729000092\n",
      "batch: 4819, loss: 0.1395450234413147\n",
      "batch: 4820, loss: 0.18766315281391144\n",
      "batch: 4821, loss: 0.28177934885025024\n",
      "batch: 4822, loss: 0.06824515759944916\n",
      "batch: 4823, loss: 0.17566220462322235\n",
      "batch: 4824, loss: 0.3059192895889282\n",
      "batch: 4825, loss: 0.385040819644928\n",
      "batch: 4826, loss: 0.16274113953113556\n",
      "batch: 4827, loss: 0.2754572033882141\n",
      "batch: 4828, loss: 0.07518191635608673\n",
      "batch: 4829, loss: 0.2883026599884033\n",
      "batch: 4830, loss: 0.4343048930168152\n",
      "batch: 4831, loss: 0.19291061162948608\n",
      "batch: 4832, loss: 0.16154745221138\n",
      "batch: 4833, loss: 0.26516878604888916\n",
      "batch: 4834, loss: 0.3180660605430603\n",
      "batch: 4835, loss: 0.12642765045166016\n",
      "batch: 4836, loss: 0.2479991763830185\n",
      "batch: 4837, loss: 0.1725427210330963\n",
      "batch: 4838, loss: 0.10745688527822495\n",
      "batch: 4839, loss: 0.25877055525779724\n",
      "batch: 4840, loss: 0.14273500442504883\n",
      "batch: 4841, loss: 0.22543320059776306\n",
      "batch: 4842, loss: 0.12837110459804535\n",
      "batch: 4843, loss: 0.1760820597410202\n",
      "batch: 4844, loss: 0.23969948291778564\n",
      "batch: 4845, loss: 0.310861200094223\n",
      "batch: 4846, loss: 0.15936484932899475\n",
      "batch: 4847, loss: 0.2302923947572708\n",
      "batch: 4848, loss: 0.21799898147583008\n",
      "batch: 4849, loss: 0.13480356335639954\n",
      "batch: 4850, loss: 0.1696348637342453\n",
      "batch: 4851, loss: 0.22015275061130524\n",
      "batch: 4852, loss: 0.2606319189071655\n",
      "batch: 4853, loss: 0.1498476266860962\n",
      "batch: 4854, loss: 0.3340512812137604\n",
      "batch: 4855, loss: 0.11279673129320145\n",
      "batch: 4856, loss: 0.11539823561906815\n",
      "batch: 4857, loss: 0.08855625241994858\n",
      "batch: 4858, loss: 0.21982616186141968\n",
      "batch: 4859, loss: 0.2245199978351593\n",
      "batch: 4860, loss: 0.21972529590129852\n",
      "batch: 4861, loss: 0.22515030205249786\n",
      "batch: 4862, loss: 0.15642018616199493\n",
      "batch: 4863, loss: 0.20650966465473175\n",
      "batch: 4864, loss: 0.14586493372917175\n",
      "batch: 4865, loss: 0.09978428483009338\n",
      "batch: 4866, loss: 0.11102570593357086\n",
      "batch: 4867, loss: 0.1788373589515686\n",
      "batch: 4868, loss: 0.2027117908000946\n",
      "batch: 4869, loss: 0.23668837547302246\n",
      "batch: 4870, loss: 0.18022599816322327\n",
      "batch: 4871, loss: 0.23995786905288696\n",
      "batch: 4872, loss: 0.11021944135427475\n",
      "batch: 4873, loss: 0.25100332498550415\n",
      "batch: 4874, loss: 0.10725914686918259\n",
      "batch: 4875, loss: 0.16732504963874817\n",
      "batch: 4876, loss: 0.17235621809959412\n",
      "batch: 4877, loss: 0.1360798329114914\n",
      "batch: 4878, loss: 0.21781259775161743\n",
      "batch: 4879, loss: 0.2132914662361145\n",
      "batch: 4880, loss: 0.13971015810966492\n",
      "batch: 4881, loss: 0.2849469780921936\n",
      "batch: 4882, loss: 0.38302379846572876\n",
      "batch: 4883, loss: 0.30888283252716064\n",
      "batch: 4884, loss: 0.20250967144966125\n",
      "batch: 4885, loss: 0.192106232047081\n",
      "batch: 4886, loss: 0.2148163616657257\n",
      "batch: 4887, loss: 0.20605647563934326\n",
      "batch: 4888, loss: 0.26240772008895874\n",
      "batch: 4889, loss: 0.09403230249881744\n",
      "batch: 4890, loss: 0.22649024426937103\n",
      "batch: 4891, loss: 0.2978632152080536\n",
      "batch: 4892, loss: 0.3049837648868561\n",
      "batch: 4893, loss: 0.21600782871246338\n",
      "batch: 4894, loss: 0.1630042940378189\n",
      "batch: 4895, loss: 0.342446506023407\n",
      "batch: 4896, loss: 0.3019634783267975\n",
      "batch: 4897, loss: 0.15051622688770294\n",
      "batch: 4898, loss: 0.2469339668750763\n",
      "batch: 4899, loss: 0.1452597975730896\n",
      "batch: 4900, loss: 0.22100704908370972\n",
      "model saved to ./saving/model.ckpt-49\n",
      "batch: 4901, loss: 0.3098036050796509\n",
      "batch: 4902, loss: 0.3761252462863922\n",
      "batch: 4903, loss: 0.18449215590953827\n",
      "batch: 4904, loss: 0.2601441740989685\n",
      "batch: 4905, loss: 0.1908949315547943\n",
      "batch: 4906, loss: 0.18484710156917572\n",
      "batch: 4907, loss: 0.11586462706327438\n",
      "batch: 4908, loss: 0.46716222167015076\n",
      "batch: 4909, loss: 0.3797339200973511\n",
      "batch: 4910, loss: 0.26246482133865356\n",
      "batch: 4911, loss: 0.11189933866262436\n",
      "batch: 4912, loss: 0.18515506386756897\n",
      "batch: 4913, loss: 0.24436068534851074\n",
      "batch: 4914, loss: 0.33719080686569214\n",
      "batch: 4915, loss: 0.2179027795791626\n",
      "batch: 4916, loss: 0.34009408950805664\n",
      "batch: 4917, loss: 0.1873951256275177\n",
      "batch: 4918, loss: 0.21320588886737823\n",
      "batch: 4919, loss: 0.1559240221977234\n",
      "batch: 4920, loss: 0.1869724541902542\n",
      "batch: 4921, loss: 0.2248060405254364\n",
      "batch: 4922, loss: 0.2997094988822937\n",
      "batch: 4923, loss: 0.2144724428653717\n",
      "batch: 4924, loss: 0.3500933051109314\n",
      "batch: 4925, loss: 0.1430571973323822\n",
      "batch: 4926, loss: 0.1798316091299057\n",
      "batch: 4927, loss: 0.07932303845882416\n",
      "batch: 4928, loss: 0.2152576446533203\n",
      "batch: 4929, loss: 0.25067010521888733\n",
      "batch: 4930, loss: 0.2826315760612488\n",
      "batch: 4931, loss: 0.21133704483509064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 4932, loss: 0.3965827226638794\n",
      "batch: 4933, loss: 0.22075587511062622\n",
      "batch: 4934, loss: 0.1429719626903534\n",
      "batch: 4935, loss: 0.12026046961545944\n",
      "batch: 4936, loss: 0.19699501991271973\n",
      "batch: 4937, loss: 0.13414490222930908\n",
      "batch: 4938, loss: 0.12206166982650757\n",
      "batch: 4939, loss: 0.3200421929359436\n",
      "batch: 4940, loss: 0.1635216772556305\n",
      "batch: 4941, loss: 0.1984633207321167\n",
      "batch: 4942, loss: 0.12061599642038345\n",
      "batch: 4943, loss: 0.2352057844400406\n",
      "batch: 4944, loss: 0.127659872174263\n",
      "batch: 4945, loss: 0.26264339685440063\n",
      "batch: 4946, loss: 0.22568246722221375\n",
      "batch: 4947, loss: 0.19719019532203674\n",
      "batch: 4948, loss: 0.14201852679252625\n",
      "batch: 4949, loss: 0.2502192556858063\n",
      "batch: 4950, loss: 0.17208421230316162\n",
      "batch: 4951, loss: 0.11993438005447388\n",
      "batch: 4952, loss: 0.27618923783302307\n",
      "batch: 4953, loss: 0.1470106989145279\n",
      "batch: 4954, loss: 0.13211891055107117\n",
      "batch: 4955, loss: 0.21062155067920685\n",
      "batch: 4956, loss: 0.08113530278205872\n",
      "batch: 4957, loss: 0.2828114330768585\n",
      "batch: 4958, loss: 0.29769399762153625\n",
      "batch: 4959, loss: 0.28294113278388977\n",
      "batch: 4960, loss: 0.22595800459384918\n",
      "batch: 4961, loss: 0.12109293788671494\n",
      "batch: 4962, loss: 0.2654155194759369\n",
      "batch: 4963, loss: 0.12057583034038544\n",
      "batch: 4964, loss: 0.21291202306747437\n",
      "batch: 4965, loss: 0.16792038083076477\n",
      "batch: 4966, loss: 0.12968966364860535\n",
      "batch: 4967, loss: 0.26707959175109863\n",
      "batch: 4968, loss: 0.09770222008228302\n",
      "batch: 4969, loss: 0.15405124425888062\n",
      "batch: 4970, loss: 0.14200693368911743\n",
      "batch: 4971, loss: 0.13050231337547302\n",
      "batch: 4972, loss: 0.17418351769447327\n",
      "batch: 4973, loss: 0.11633476614952087\n",
      "batch: 4974, loss: 0.18513621389865875\n",
      "batch: 4975, loss: 0.38472652435302734\n",
      "batch: 4976, loss: 0.21301808953285217\n",
      "batch: 4977, loss: 0.121015265583992\n",
      "batch: 4978, loss: 0.1892385184764862\n",
      "batch: 4979, loss: 0.3731693923473358\n",
      "batch: 4980, loss: 0.19038806855678558\n",
      "batch: 4981, loss: 0.20101909339427948\n",
      "batch: 4982, loss: 0.15936461091041565\n",
      "batch: 4983, loss: 0.12516146898269653\n",
      "batch: 4984, loss: 0.3902212381362915\n",
      "batch: 4985, loss: 0.076466865837574\n",
      "batch: 4986, loss: 0.18967020511627197\n",
      "batch: 4987, loss: 0.15146617591381073\n",
      "batch: 4988, loss: 0.17847155034542084\n",
      "batch: 4989, loss: 0.123844675719738\n",
      "batch: 4990, loss: 0.28744974732398987\n",
      "batch: 4991, loss: 0.36931806802749634\n",
      "batch: 4992, loss: 0.17789320647716522\n",
      "batch: 4993, loss: 0.1814977526664734\n",
      "batch: 4994, loss: 0.18328143656253815\n",
      "batch: 4995, loss: 0.25877249240875244\n",
      "batch: 4996, loss: 0.32043352723121643\n",
      "batch: 4997, loss: 0.24372121691703796\n",
      "batch: 4998, loss: 0.22155748307704926\n",
      "batch: 4999, loss: 0.1835516393184662\n",
      "batch: 5000, loss: 0.31777721643447876\n",
      "model saved to ./saving/model.ckpt-50\n",
      "batch: 5001, loss: 0.2273833453655243\n",
      "batch: 5002, loss: 0.27059850096702576\n",
      "batch: 5003, loss: 0.14876893162727356\n",
      "batch: 5004, loss: 0.3098980188369751\n",
      "batch: 5005, loss: 0.2970393896102905\n",
      "batch: 5006, loss: 0.27919381856918335\n",
      "batch: 5007, loss: 0.35308271646499634\n",
      "batch: 5008, loss: 0.12761186063289642\n",
      "batch: 5009, loss: 0.23271644115447998\n",
      "batch: 5010, loss: 0.22650766372680664\n",
      "batch: 5011, loss: 0.34496891498565674\n",
      "batch: 5012, loss: 0.13053128123283386\n",
      "batch: 5013, loss: 0.23158065974712372\n",
      "batch: 5014, loss: 0.08049438148736954\n",
      "batch: 5015, loss: 0.21163597702980042\n",
      "batch: 5016, loss: 0.342640221118927\n",
      "batch: 5017, loss: 0.39258360862731934\n",
      "batch: 5018, loss: 0.17801585793495178\n",
      "batch: 5019, loss: 0.12402355670928955\n",
      "batch: 5020, loss: 0.1984577625989914\n",
      "batch: 5021, loss: 0.13426470756530762\n",
      "batch: 5022, loss: 0.24378907680511475\n",
      "batch: 5023, loss: 0.09952086955308914\n",
      "batch: 5024, loss: 0.20393115282058716\n",
      "batch: 5025, loss: 0.24572668969631195\n",
      "batch: 5026, loss: 0.045866336673498154\n",
      "batch: 5027, loss: 0.16642668843269348\n",
      "batch: 5028, loss: 0.2045431137084961\n",
      "batch: 5029, loss: 0.1073695719242096\n",
      "batch: 5030, loss: 0.27543720602989197\n",
      "batch: 5031, loss: 0.28463277220726013\n",
      "batch: 5032, loss: 0.25284868478775024\n",
      "batch: 5033, loss: 0.22166502475738525\n",
      "batch: 5034, loss: 0.20544926822185516\n",
      "batch: 5035, loss: 0.1587510108947754\n",
      "batch: 5036, loss: 0.15267163515090942\n",
      "batch: 5037, loss: 0.08672723174095154\n",
      "batch: 5038, loss: 0.2699165940284729\n",
      "batch: 5039, loss: 0.25399190187454224\n",
      "batch: 5040, loss: 0.1947590559720993\n",
      "batch: 5041, loss: 0.16785398125648499\n",
      "batch: 5042, loss: 0.3998190760612488\n",
      "batch: 5043, loss: 0.13235145807266235\n",
      "batch: 5044, loss: 0.3061387836933136\n",
      "batch: 5045, loss: 0.16123828291893005\n",
      "batch: 5046, loss: 0.1449158489704132\n",
      "batch: 5047, loss: 0.19750644266605377\n",
      "batch: 5048, loss: 0.18436956405639648\n",
      "batch: 5049, loss: 0.22383961081504822\n",
      "batch: 5050, loss: 0.11993204802274704\n",
      "batch: 5051, loss: 0.3144938349723816\n",
      "batch: 5052, loss: 0.28590744733810425\n",
      "batch: 5053, loss: 0.15658606588840485\n",
      "batch: 5054, loss: 0.41792652010917664\n",
      "batch: 5055, loss: 0.3522733449935913\n",
      "batch: 5056, loss: 0.2930874228477478\n",
      "batch: 5057, loss: 0.15363913774490356\n",
      "batch: 5058, loss: 0.1329612284898758\n",
      "batch: 5059, loss: 0.12753555178642273\n",
      "batch: 5060, loss: 0.3010174334049225\n",
      "batch: 5061, loss: 0.2582331597805023\n",
      "batch: 5062, loss: 0.16307055950164795\n",
      "batch: 5063, loss: 0.21632590889930725\n",
      "batch: 5064, loss: 0.2262284755706787\n",
      "batch: 5065, loss: 0.341451495885849\n",
      "batch: 5066, loss: 0.20862150192260742\n",
      "batch: 5067, loss: 0.26998600363731384\n",
      "batch: 5068, loss: 0.14022225141525269\n",
      "batch: 5069, loss: 0.2656818926334381\n",
      "batch: 5070, loss: 0.232644185423851\n",
      "batch: 5071, loss: 0.36952000856399536\n",
      "batch: 5072, loss: 0.13620850443840027\n",
      "batch: 5073, loss: 0.19455721974372864\n",
      "batch: 5074, loss: 0.18932218849658966\n",
      "batch: 5075, loss: 0.2303539514541626\n",
      "batch: 5076, loss: 0.2623860836029053\n",
      "batch: 5077, loss: 0.09724779427051544\n",
      "batch: 5078, loss: 0.2540704011917114\n",
      "batch: 5079, loss: 0.15205609798431396\n",
      "batch: 5080, loss: 0.20599982142448425\n",
      "batch: 5081, loss: 0.3377634584903717\n",
      "batch: 5082, loss: 0.22177299857139587\n",
      "batch: 5083, loss: 0.09931211918592453\n",
      "batch: 5084, loss: 0.32190924882888794\n",
      "batch: 5085, loss: 0.1828841269016266\n",
      "batch: 5086, loss: 0.19926737248897552\n",
      "batch: 5087, loss: 0.11023098230361938\n",
      "batch: 5088, loss: 0.23315581679344177\n",
      "batch: 5089, loss: 0.22647133469581604\n",
      "batch: 5090, loss: 0.23041731119155884\n",
      "batch: 5091, loss: 0.2123982459306717\n",
      "batch: 5092, loss: 0.11548186838626862\n",
      "batch: 5093, loss: 0.11779555678367615\n",
      "batch: 5094, loss: 0.2400302290916443\n",
      "batch: 5095, loss: 0.36050766706466675\n",
      "batch: 5096, loss: 0.20812469720840454\n",
      "batch: 5097, loss: 0.1543201208114624\n",
      "batch: 5098, loss: 0.1877489537000656\n",
      "batch: 5099, loss: 0.16140657663345337\n",
      "batch: 5100, loss: 0.20042866468429565\n",
      "model saved to ./saving/model.ckpt-51\n",
      "batch: 5101, loss: 0.40459516644477844\n",
      "batch: 5102, loss: 0.2080788016319275\n",
      "batch: 5103, loss: 0.10616548359394073\n",
      "batch: 5104, loss: 0.30698269605636597\n",
      "batch: 5105, loss: 0.17717671394348145\n",
      "batch: 5106, loss: 0.21798470616340637\n",
      "batch: 5107, loss: 0.14347949624061584\n",
      "batch: 5108, loss: 0.1581258475780487\n",
      "batch: 5109, loss: 0.1441582441329956\n",
      "batch: 5110, loss: 0.34056198596954346\n",
      "batch: 5111, loss: 0.5353047251701355\n",
      "batch: 5112, loss: 0.22078916430473328\n",
      "batch: 5113, loss: 0.07402142882347107\n",
      "batch: 5114, loss: 0.2437225878238678\n",
      "batch: 5115, loss: 0.16144610941410065\n",
      "batch: 5116, loss: 0.17239265143871307\n",
      "batch: 5117, loss: 0.10340091586112976\n",
      "batch: 5118, loss: 0.19359931349754333\n",
      "batch: 5119, loss: 0.12527093291282654\n",
      "batch: 5120, loss: 0.18030332028865814\n",
      "batch: 5121, loss: 0.1610555201768875\n",
      "batch: 5122, loss: 0.20698460936546326\n",
      "batch: 5123, loss: 0.3351934552192688\n",
      "batch: 5124, loss: 0.23721742630004883\n",
      "batch: 5125, loss: 0.39072224497795105\n",
      "batch: 5126, loss: 0.1478147804737091\n",
      "batch: 5127, loss: 0.24074798822402954\n",
      "batch: 5128, loss: 0.2526398301124573\n",
      "batch: 5129, loss: 0.25365424156188965\n",
      "batch: 5130, loss: 0.11717551946640015\n",
      "batch: 5131, loss: 0.14232608675956726\n",
      "batch: 5132, loss: 0.18111681938171387\n",
      "batch: 5133, loss: 0.2613297998905182\n",
      "batch: 5134, loss: 0.22321565449237823\n",
      "batch: 5135, loss: 0.19745135307312012\n",
      "batch: 5136, loss: 0.11434566974639893\n",
      "batch: 5137, loss: 0.264495849609375\n",
      "batch: 5138, loss: 0.11167420446872711\n",
      "batch: 5139, loss: 0.18115010857582092\n",
      "batch: 5140, loss: 0.19993740320205688\n",
      "batch: 5141, loss: 0.27770987153053284\n",
      "batch: 5142, loss: 0.17178252339363098\n",
      "batch: 5143, loss: 0.1121661365032196\n",
      "batch: 5144, loss: 0.26989200711250305\n",
      "batch: 5145, loss: 0.1281801015138626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 5146, loss: 0.33443987369537354\n",
      "batch: 5147, loss: 0.23859554529190063\n",
      "batch: 5148, loss: 0.08933544158935547\n",
      "batch: 5149, loss: 0.17846286296844482\n",
      "batch: 5150, loss: 0.2571759819984436\n",
      "batch: 5151, loss: 0.2521689832210541\n",
      "batch: 5152, loss: 0.1437862515449524\n",
      "batch: 5153, loss: 0.1892610788345337\n",
      "batch: 5154, loss: 0.3175576329231262\n",
      "batch: 5155, loss: 0.08848525583744049\n",
      "batch: 5156, loss: 0.20387321710586548\n",
      "batch: 5157, loss: 0.2129935771226883\n",
      "batch: 5158, loss: 0.26289063692092896\n",
      "batch: 5159, loss: 0.22747118771076202\n",
      "batch: 5160, loss: 0.12771832942962646\n",
      "batch: 5161, loss: 0.1894744634628296\n",
      "batch: 5162, loss: 0.17708775401115417\n",
      "batch: 5163, loss: 0.42008793354034424\n",
      "batch: 5164, loss: 0.1476978212594986\n",
      "batch: 5165, loss: 0.12839746475219727\n",
      "batch: 5166, loss: 0.17484363913536072\n",
      "batch: 5167, loss: 0.13718324899673462\n",
      "batch: 5168, loss: 0.18960286676883698\n",
      "batch: 5169, loss: 0.19156241416931152\n",
      "batch: 5170, loss: 0.10009565949440002\n",
      "batch: 5171, loss: 0.1961524337530136\n",
      "batch: 5172, loss: 0.3378221392631531\n",
      "batch: 5173, loss: 0.18179821968078613\n",
      "batch: 5174, loss: 0.08436024188995361\n",
      "batch: 5175, loss: 0.12929436564445496\n",
      "batch: 5176, loss: 0.10950777679681778\n",
      "batch: 5177, loss: 0.19142907857894897\n",
      "batch: 5178, loss: 0.26523542404174805\n",
      "batch: 5179, loss: 0.12873215973377228\n",
      "batch: 5180, loss: 0.2677247226238251\n",
      "batch: 5181, loss: 0.35743582248687744\n",
      "batch: 5182, loss: 0.23775097727775574\n",
      "batch: 5183, loss: 0.28399378061294556\n",
      "batch: 5184, loss: 0.2884812355041504\n",
      "batch: 5185, loss: 0.21504506468772888\n",
      "batch: 5186, loss: 0.30222219228744507\n",
      "batch: 5187, loss: 0.15509618818759918\n",
      "batch: 5188, loss: 0.187822163105011\n",
      "batch: 5189, loss: 0.26684048771858215\n",
      "batch: 5190, loss: 0.17613700032234192\n",
      "batch: 5191, loss: 0.18037697672843933\n",
      "batch: 5192, loss: 0.23892581462860107\n",
      "batch: 5193, loss: 0.3885173797607422\n",
      "batch: 5194, loss: 0.17308084666728973\n",
      "batch: 5195, loss: 0.13010549545288086\n",
      "batch: 5196, loss: 0.17882509529590607\n",
      "batch: 5197, loss: 0.2642839848995209\n",
      "batch: 5198, loss: 0.19635896384716034\n",
      "batch: 5199, loss: 0.22559714317321777\n",
      "batch: 5200, loss: 0.12533225119113922\n",
      "model saved to ./saving/model.ckpt-52\n",
      "batch: 5201, loss: 0.3014785647392273\n",
      "batch: 5202, loss: 0.2795206308364868\n",
      "batch: 5203, loss: 0.3124748766422272\n",
      "batch: 5204, loss: 0.13983206450939178\n",
      "batch: 5205, loss: 0.3480150103569031\n",
      "batch: 5206, loss: 0.16166719794273376\n",
      "batch: 5207, loss: 0.1316707581281662\n",
      "batch: 5208, loss: 0.2680918872356415\n",
      "batch: 5209, loss: 0.09890241920948029\n",
      "batch: 5210, loss: 0.1762523055076599\n",
      "batch: 5211, loss: 0.22650255262851715\n",
      "batch: 5212, loss: 0.14130887389183044\n",
      "batch: 5213, loss: 0.06595143675804138\n",
      "batch: 5214, loss: 0.16616718471050262\n",
      "batch: 5215, loss: 0.2731552720069885\n",
      "batch: 5216, loss: 0.24461349844932556\n",
      "batch: 5217, loss: 0.2802743911743164\n",
      "batch: 5218, loss: 0.3164448142051697\n",
      "batch: 5219, loss: 0.14357902109622955\n",
      "batch: 5220, loss: 0.21978092193603516\n",
      "batch: 5221, loss: 0.2201659381389618\n",
      "batch: 5222, loss: 0.26062270998954773\n",
      "batch: 5223, loss: 0.12153632193803787\n",
      "batch: 5224, loss: 0.18594524264335632\n",
      "batch: 5225, loss: 0.201344832777977\n",
      "batch: 5226, loss: 0.2269611358642578\n",
      "batch: 5227, loss: 0.15452849864959717\n",
      "batch: 5228, loss: 0.285071462392807\n",
      "batch: 5229, loss: 0.23054689168930054\n",
      "batch: 5230, loss: 0.16161476075649261\n",
      "batch: 5231, loss: 0.28283318877220154\n",
      "batch: 5232, loss: 0.0969959944486618\n",
      "batch: 5233, loss: 0.2004210650920868\n",
      "batch: 5234, loss: 0.26255983114242554\n",
      "batch: 5235, loss: 0.2166418731212616\n",
      "batch: 5236, loss: 0.18600419163703918\n",
      "batch: 5237, loss: 0.17000068724155426\n",
      "batch: 5238, loss: 0.1331188678741455\n",
      "batch: 5239, loss: 0.2152278572320938\n",
      "batch: 5240, loss: 0.15285833179950714\n",
      "batch: 5241, loss: 0.10993893444538116\n",
      "batch: 5242, loss: 0.20213817059993744\n",
      "batch: 5243, loss: 0.10306881368160248\n",
      "batch: 5244, loss: 0.45585107803344727\n",
      "batch: 5245, loss: 0.18331849575042725\n",
      "batch: 5246, loss: 0.11865630000829697\n",
      "batch: 5247, loss: 0.3722967207431793\n",
      "batch: 5248, loss: 0.14287112653255463\n",
      "batch: 5249, loss: 0.31708693504333496\n",
      "batch: 5250, loss: 0.22191943228244781\n",
      "batch: 5251, loss: 0.24842426180839539\n",
      "batch: 5252, loss: 0.26087576150894165\n",
      "batch: 5253, loss: 0.2312506139278412\n",
      "batch: 5254, loss: 0.13306596875190735\n",
      "batch: 5255, loss: 0.1910277009010315\n",
      "batch: 5256, loss: 0.3942895233631134\n",
      "batch: 5257, loss: 0.23050054907798767\n",
      "batch: 5258, loss: 0.1922958493232727\n",
      "batch: 5259, loss: 0.13964593410491943\n",
      "batch: 5260, loss: 0.12883055210113525\n",
      "batch: 5261, loss: 0.26522892713546753\n",
      "batch: 5262, loss: 0.2688749134540558\n",
      "batch: 5263, loss: 0.34146177768707275\n",
      "batch: 5264, loss: 0.1331486999988556\n",
      "batch: 5265, loss: 0.11999016255140305\n",
      "batch: 5266, loss: 0.15118762850761414\n",
      "batch: 5267, loss: 0.11851194500923157\n",
      "batch: 5268, loss: 0.1536090075969696\n",
      "batch: 5269, loss: 0.35302698612213135\n",
      "batch: 5270, loss: 0.14602699875831604\n",
      "batch: 5271, loss: 0.26199036836624146\n",
      "batch: 5272, loss: 0.2881457209587097\n",
      "batch: 5273, loss: 0.14028771221637726\n",
      "batch: 5274, loss: 0.21355634927749634\n",
      "batch: 5275, loss: 0.1739691197872162\n",
      "batch: 5276, loss: 0.2741411328315735\n",
      "batch: 5277, loss: 0.15522782504558563\n",
      "batch: 5278, loss: 0.29603737592697144\n",
      "batch: 5279, loss: 0.10272125899791718\n",
      "batch: 5280, loss: 0.23459550738334656\n",
      "batch: 5281, loss: 0.11744310706853867\n",
      "batch: 5282, loss: 0.2455553561449051\n",
      "batch: 5283, loss: 0.3032306134700775\n",
      "batch: 5284, loss: 0.38256460428237915\n",
      "batch: 5285, loss: 0.16252119839191437\n",
      "batch: 5286, loss: 0.32006198167800903\n",
      "batch: 5287, loss: 0.20560994744300842\n",
      "batch: 5288, loss: 0.27118727564811707\n",
      "batch: 5289, loss: 0.23304787278175354\n",
      "batch: 5290, loss: 0.30729684233665466\n",
      "batch: 5291, loss: 0.139877051115036\n",
      "batch: 5292, loss: 0.19788900017738342\n",
      "batch: 5293, loss: 0.3743312954902649\n",
      "batch: 5294, loss: 0.31724703311920166\n",
      "batch: 5295, loss: 0.1624605506658554\n",
      "batch: 5296, loss: 0.22516274452209473\n",
      "batch: 5297, loss: 0.12543101608753204\n",
      "batch: 5298, loss: 0.13698124885559082\n",
      "batch: 5299, loss: 0.24145257472991943\n",
      "batch: 5300, loss: 0.25585293769836426\n",
      "model saved to ./saving/model.ckpt-53\n",
      "batch: 5301, loss: 0.18951544165611267\n",
      "batch: 5302, loss: 0.32454830408096313\n",
      "batch: 5303, loss: 0.1538812220096588\n",
      "batch: 5304, loss: 0.3298491835594177\n",
      "batch: 5305, loss: 0.10318872332572937\n",
      "batch: 5306, loss: 0.20780928432941437\n",
      "batch: 5307, loss: 0.24343183636665344\n",
      "batch: 5308, loss: 0.14093193411827087\n",
      "batch: 5309, loss: 0.4582664370536804\n",
      "batch: 5310, loss: 0.11805056035518646\n",
      "batch: 5311, loss: 0.08821002393960953\n",
      "batch: 5312, loss: 0.22312910854816437\n",
      "batch: 5313, loss: 0.23417027294635773\n",
      "batch: 5314, loss: 0.27376335859298706\n",
      "batch: 5315, loss: 0.20392818748950958\n",
      "batch: 5316, loss: 0.17061743140220642\n",
      "batch: 5317, loss: 0.158128023147583\n",
      "batch: 5318, loss: 0.3278757929801941\n",
      "batch: 5319, loss: 0.09966510534286499\n",
      "batch: 5320, loss: 0.13315406441688538\n",
      "batch: 5321, loss: 0.158858060836792\n",
      "batch: 5322, loss: 0.17893213033676147\n",
      "batch: 5323, loss: 0.16044221818447113\n",
      "batch: 5324, loss: 0.07591081410646439\n",
      "batch: 5325, loss: 0.18872393667697906\n",
      "batch: 5326, loss: 0.2282002568244934\n",
      "batch: 5327, loss: 0.3233420252799988\n",
      "batch: 5328, loss: 0.11628457903862\n",
      "batch: 5329, loss: 0.10046148300170898\n",
      "batch: 5330, loss: 0.1138858050107956\n",
      "batch: 5331, loss: 0.12338559329509735\n",
      "batch: 5332, loss: 0.17004728317260742\n",
      "batch: 5333, loss: 0.17397330701351166\n",
      "batch: 5334, loss: 0.23409438133239746\n",
      "batch: 5335, loss: 0.1752173751592636\n",
      "batch: 5336, loss: 0.2578513026237488\n",
      "batch: 5337, loss: 0.15081395208835602\n",
      "batch: 5338, loss: 0.11740119010210037\n",
      "batch: 5339, loss: 0.22809633612632751\n",
      "batch: 5340, loss: 0.41139692068099976\n",
      "batch: 5341, loss: 0.2092290073633194\n",
      "batch: 5342, loss: 0.10833785682916641\n",
      "batch: 5343, loss: 0.17304372787475586\n",
      "batch: 5344, loss: 0.18642832338809967\n",
      "batch: 5345, loss: 0.27885591983795166\n",
      "batch: 5346, loss: 0.2994474172592163\n",
      "batch: 5347, loss: 0.33020737767219543\n",
      "batch: 5348, loss: 0.13475310802459717\n",
      "batch: 5349, loss: 0.3127511739730835\n",
      "batch: 5350, loss: 0.33556050062179565\n",
      "batch: 5351, loss: 0.29284995794296265\n",
      "batch: 5352, loss: 0.284917950630188\n",
      "batch: 5353, loss: 0.26650527119636536\n",
      "batch: 5354, loss: 0.22898468375205994\n",
      "batch: 5355, loss: 0.1869162917137146\n",
      "batch: 5356, loss: 0.32981371879577637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 5357, loss: 0.12135815620422363\n",
      "batch: 5358, loss: 0.2285470813512802\n",
      "batch: 5359, loss: 0.1588195413351059\n",
      "batch: 5360, loss: 0.30083388090133667\n",
      "batch: 5361, loss: 0.1605762392282486\n",
      "batch: 5362, loss: 0.13297298550605774\n",
      "batch: 5363, loss: 0.19837293028831482\n",
      "batch: 5364, loss: 0.23397979140281677\n",
      "batch: 5365, loss: 0.2072845995426178\n",
      "batch: 5366, loss: 0.13341554999351501\n",
      "batch: 5367, loss: 0.11371397227048874\n",
      "batch: 5368, loss: 0.1076468974351883\n",
      "batch: 5369, loss: 0.1351011097431183\n",
      "batch: 5370, loss: 0.16528694331645966\n",
      "batch: 5371, loss: 0.17030002176761627\n",
      "batch: 5372, loss: 0.13205009698867798\n",
      "batch: 5373, loss: 0.139958918094635\n",
      "batch: 5374, loss: 0.3546587824821472\n",
      "batch: 5375, loss: 0.374626100063324\n",
      "batch: 5376, loss: 0.25966572761535645\n",
      "batch: 5377, loss: 0.1522432267665863\n",
      "batch: 5378, loss: 0.38416987657546997\n",
      "batch: 5379, loss: 0.23355859518051147\n",
      "batch: 5380, loss: 0.1777990460395813\n",
      "batch: 5381, loss: 0.12793825566768646\n",
      "batch: 5382, loss: 0.19049811363220215\n",
      "batch: 5383, loss: 0.2888478934764862\n",
      "batch: 5384, loss: 0.26836729049682617\n",
      "batch: 5385, loss: 0.1351853311061859\n",
      "batch: 5386, loss: 0.16877667605876923\n",
      "batch: 5387, loss: 0.23141410946846008\n",
      "batch: 5388, loss: 0.11225654184818268\n",
      "batch: 5389, loss: 0.2150803804397583\n",
      "batch: 5390, loss: 0.17559859156608582\n",
      "batch: 5391, loss: 0.1517098993062973\n",
      "batch: 5392, loss: 0.28497451543807983\n",
      "batch: 5393, loss: 0.23807522654533386\n",
      "batch: 5394, loss: 0.20978577435016632\n",
      "batch: 5395, loss: 0.1675432026386261\n",
      "batch: 5396, loss: 0.18403548002243042\n",
      "batch: 5397, loss: 0.21084141731262207\n",
      "batch: 5398, loss: 0.14288227260112762\n",
      "batch: 5399, loss: 0.09494251012802124\n",
      "batch: 5400, loss: 0.18060126900672913\n",
      "model saved to ./saving/model.ckpt-54\n",
      "batch: 5401, loss: 0.20099911093711853\n",
      "batch: 5402, loss: 0.1839759349822998\n",
      "batch: 5403, loss: 0.18656396865844727\n",
      "batch: 5404, loss: 0.11856544762849808\n",
      "batch: 5405, loss: 0.1356571614742279\n",
      "batch: 5406, loss: 0.18086758255958557\n",
      "batch: 5407, loss: 0.09715712070465088\n",
      "batch: 5408, loss: 0.13644945621490479\n",
      "batch: 5409, loss: 0.30792224407196045\n",
      "batch: 5410, loss: 0.23597879707813263\n",
      "batch: 5411, loss: 0.2383299469947815\n",
      "batch: 5412, loss: 0.18221955001354218\n",
      "batch: 5413, loss: 0.22380605340003967\n",
      "batch: 5414, loss: 0.25067681074142456\n",
      "batch: 5415, loss: 0.272367000579834\n",
      "batch: 5416, loss: 0.2816591262817383\n",
      "batch: 5417, loss: 0.16186155378818512\n",
      "batch: 5418, loss: 0.12772773206233978\n",
      "batch: 5419, loss: 0.22632017731666565\n",
      "batch: 5420, loss: 0.22223466634750366\n",
      "batch: 5421, loss: 0.1870529055595398\n",
      "batch: 5422, loss: 0.2313854992389679\n",
      "batch: 5423, loss: 0.25014299154281616\n",
      "batch: 5424, loss: 0.3069281578063965\n",
      "batch: 5425, loss: 0.20391365885734558\n",
      "batch: 5426, loss: 0.1618400663137436\n",
      "batch: 5427, loss: 0.12201012670993805\n",
      "batch: 5428, loss: 0.1349780261516571\n",
      "batch: 5429, loss: 0.09756532311439514\n",
      "batch: 5430, loss: 0.18023571372032166\n",
      "batch: 5431, loss: 0.09877294301986694\n",
      "batch: 5432, loss: 0.19288048148155212\n",
      "batch: 5433, loss: 0.1593971997499466\n",
      "batch: 5434, loss: 0.12722788751125336\n",
      "batch: 5435, loss: 0.29631829261779785\n",
      "batch: 5436, loss: 0.1343437135219574\n",
      "batch: 5437, loss: 0.08560474961996078\n",
      "batch: 5438, loss: 0.17869655787944794\n",
      "batch: 5439, loss: 0.18675416707992554\n",
      "batch: 5440, loss: 0.14031866192817688\n",
      "batch: 5441, loss: 0.16432619094848633\n",
      "batch: 5442, loss: 0.10751581937074661\n",
      "batch: 5443, loss: 0.1534186750650406\n",
      "batch: 5444, loss: 0.24076572060585022\n",
      "batch: 5445, loss: 0.21774323284626007\n",
      "batch: 5446, loss: 0.46018463373184204\n",
      "batch: 5447, loss: 0.22706393897533417\n",
      "batch: 5448, loss: 0.20971697568893433\n",
      "batch: 5449, loss: 0.26779112219810486\n",
      "batch: 5450, loss: 0.1979542076587677\n",
      "batch: 5451, loss: 0.32580772042274475\n",
      "batch: 5452, loss: 0.1568688601255417\n",
      "batch: 5453, loss: 0.22222380340099335\n",
      "batch: 5454, loss: 0.26609236001968384\n",
      "batch: 5455, loss: 0.1939070224761963\n",
      "batch: 5456, loss: 0.10563275963068008\n",
      "batch: 5457, loss: 0.1918042004108429\n",
      "batch: 5458, loss: 0.2492823600769043\n",
      "batch: 5459, loss: 0.39763468503952026\n",
      "batch: 5460, loss: 0.22551020979881287\n",
      "batch: 5461, loss: 0.18294218182563782\n",
      "batch: 5462, loss: 0.11238538473844528\n",
      "batch: 5463, loss: 0.13184206187725067\n",
      "batch: 5464, loss: 0.1675090491771698\n",
      "batch: 5465, loss: 0.16520893573760986\n",
      "batch: 5466, loss: 0.40623074769973755\n",
      "batch: 5467, loss: 0.20134690403938293\n",
      "batch: 5468, loss: 0.24029800295829773\n",
      "batch: 5469, loss: 0.3692828416824341\n",
      "batch: 5470, loss: 0.16499191522598267\n",
      "batch: 5471, loss: 0.15941062569618225\n",
      "batch: 5472, loss: 0.16863824427127838\n",
      "batch: 5473, loss: 0.32030510902404785\n",
      "batch: 5474, loss: 0.29009148478507996\n",
      "batch: 5475, loss: 0.08113069087266922\n",
      "batch: 5476, loss: 0.20154042541980743\n",
      "batch: 5477, loss: 0.23366661369800568\n",
      "batch: 5478, loss: 0.13212454319000244\n",
      "batch: 5479, loss: 0.1136581227183342\n",
      "batch: 5480, loss: 0.38152819871902466\n",
      "batch: 5481, loss: 0.11760615557432175\n",
      "batch: 5482, loss: 0.3093559443950653\n",
      "batch: 5483, loss: 0.19614481925964355\n",
      "batch: 5484, loss: 0.15156716108322144\n",
      "batch: 5485, loss: 0.27389591932296753\n",
      "batch: 5486, loss: 0.18133600056171417\n",
      "batch: 5487, loss: 0.1933915764093399\n",
      "batch: 5488, loss: 0.14796815812587738\n",
      "batch: 5489, loss: 0.23242712020874023\n",
      "batch: 5490, loss: 0.2755115032196045\n",
      "batch: 5491, loss: 0.137897789478302\n",
      "batch: 5492, loss: 0.1974298357963562\n",
      "batch: 5493, loss: 0.10001921653747559\n",
      "batch: 5494, loss: 0.2768508791923523\n",
      "batch: 5495, loss: 0.15572115778923035\n",
      "batch: 5496, loss: 0.12592962384223938\n",
      "batch: 5497, loss: 0.1661543995141983\n",
      "batch: 5498, loss: 0.29641371965408325\n",
      "batch: 5499, loss: 0.26366129517555237\n",
      "batch: 5500, loss: 0.24876584112644196\n",
      "model saved to ./saving/model.ckpt-55\n",
      "batch: 5501, loss: 0.2027789056301117\n",
      "batch: 5502, loss: 0.1780933141708374\n",
      "batch: 5503, loss: 0.11642853170633316\n",
      "batch: 5504, loss: 0.11224499344825745\n",
      "batch: 5505, loss: 0.33749908208847046\n",
      "batch: 5506, loss: 0.08810295164585114\n",
      "batch: 5507, loss: 0.22127701342105865\n",
      "batch: 5508, loss: 0.2422848641872406\n",
      "batch: 5509, loss: 0.11270873248577118\n",
      "batch: 5510, loss: 0.07247790694236755\n",
      "batch: 5511, loss: 0.2897326648235321\n",
      "batch: 5512, loss: 0.11925776302814484\n",
      "batch: 5513, loss: 0.1293690949678421\n",
      "batch: 5514, loss: 0.2558511793613434\n",
      "batch: 5515, loss: 0.21315020322799683\n",
      "batch: 5516, loss: 0.13821089267730713\n",
      "batch: 5517, loss: 0.09390845894813538\n",
      "batch: 5518, loss: 0.31632673740386963\n",
      "batch: 5519, loss: 0.21401189267635345\n",
      "batch: 5520, loss: 0.14772836863994598\n",
      "batch: 5521, loss: 0.25496989488601685\n",
      "batch: 5522, loss: 0.18828967213630676\n",
      "batch: 5523, loss: 0.15704286098480225\n",
      "batch: 5524, loss: 0.20358344912528992\n",
      "batch: 5525, loss: 0.2332472950220108\n",
      "batch: 5526, loss: 0.08045263588428497\n",
      "batch: 5527, loss: 0.1143248975276947\n",
      "batch: 5528, loss: 0.3300682306289673\n",
      "batch: 5529, loss: 0.23184385895729065\n",
      "batch: 5530, loss: 0.4475302994251251\n",
      "batch: 5531, loss: 0.15620265901088715\n",
      "batch: 5532, loss: 0.3231605291366577\n",
      "batch: 5533, loss: 0.18837428092956543\n",
      "batch: 5534, loss: 0.3124600648880005\n",
      "batch: 5535, loss: 0.13347025215625763\n",
      "batch: 5536, loss: 0.22047309577465057\n",
      "batch: 5537, loss: 0.19866447150707245\n",
      "batch: 5538, loss: 0.33083197474479675\n",
      "batch: 5539, loss: 0.20006129145622253\n",
      "batch: 5540, loss: 0.31823232769966125\n",
      "batch: 5541, loss: 0.1330403983592987\n",
      "batch: 5542, loss: 0.08560849726200104\n",
      "batch: 5543, loss: 0.20399190485477448\n",
      "batch: 5544, loss: 0.2174624800682068\n",
      "batch: 5545, loss: 0.25810402631759644\n",
      "batch: 5546, loss: 0.13859692215919495\n",
      "batch: 5547, loss: 0.1446271687746048\n",
      "batch: 5548, loss: 0.10831369459629059\n",
      "batch: 5549, loss: 0.18257831037044525\n",
      "batch: 5550, loss: 0.14015409350395203\n",
      "batch: 5551, loss: 0.2621289789676666\n",
      "batch: 5552, loss: 0.11584728956222534\n",
      "batch: 5553, loss: 0.2628076374530792\n",
      "batch: 5554, loss: 0.17161104083061218\n",
      "batch: 5555, loss: 0.1763470470905304\n",
      "batch: 5556, loss: 0.1625458002090454\n",
      "batch: 5557, loss: 0.12182033061981201\n",
      "batch: 5558, loss: 0.18145960569381714\n",
      "batch: 5559, loss: 0.08941783010959625\n",
      "batch: 5560, loss: 0.2488006353378296\n",
      "batch: 5561, loss: 0.2792963683605194\n",
      "batch: 5562, loss: 0.1089305579662323\n",
      "batch: 5563, loss: 0.16330479085445404\n",
      "batch: 5564, loss: 0.18346205353736877\n",
      "batch: 5565, loss: 0.27694228291511536\n",
      "batch: 5566, loss: 0.18242429196834564\n",
      "batch: 5567, loss: 0.14327281713485718\n",
      "batch: 5568, loss: 0.10135695338249207\n",
      "batch: 5569, loss: 0.07491756230592728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 5570, loss: 0.23722302913665771\n",
      "batch: 5571, loss: 0.32248228788375854\n",
      "batch: 5572, loss: 0.24360066652297974\n",
      "batch: 5573, loss: 0.19983366131782532\n",
      "batch: 5574, loss: 0.32344844937324524\n",
      "batch: 5575, loss: 0.17826494574546814\n",
      "batch: 5576, loss: 0.12046967446804047\n",
      "batch: 5577, loss: 0.1814170479774475\n",
      "batch: 5578, loss: 0.26389968395233154\n",
      "batch: 5579, loss: 0.1194327175617218\n",
      "batch: 5580, loss: 0.11822140216827393\n",
      "batch: 5581, loss: 0.21422994136810303\n",
      "batch: 5582, loss: 0.1319817453622818\n",
      "batch: 5583, loss: 0.2767789959907532\n",
      "batch: 5584, loss: 0.2374214082956314\n",
      "batch: 5585, loss: 0.2449503242969513\n",
      "batch: 5586, loss: 0.35770413279533386\n",
      "batch: 5587, loss: 0.35106658935546875\n",
      "batch: 5588, loss: 0.22789786756038666\n",
      "batch: 5589, loss: 0.25475507974624634\n",
      "batch: 5590, loss: 0.11992885172367096\n",
      "batch: 5591, loss: 0.34718552231788635\n",
      "batch: 5592, loss: 0.19815661013126373\n",
      "batch: 5593, loss: 0.2320973128080368\n",
      "batch: 5594, loss: 0.19657349586486816\n",
      "batch: 5595, loss: 0.15820851922035217\n",
      "batch: 5596, loss: 0.181784987449646\n",
      "batch: 5597, loss: 0.09117119759321213\n",
      "batch: 5598, loss: 0.15463989973068237\n",
      "batch: 5599, loss: 0.14888612926006317\n",
      "batch: 5600, loss: 0.23967957496643066\n",
      "model saved to ./saving/model.ckpt-56\n",
      "batch: 5601, loss: 0.19186416268348694\n",
      "batch: 5602, loss: 0.4277852773666382\n",
      "batch: 5603, loss: 0.20186316967010498\n",
      "batch: 5604, loss: 0.12915104627609253\n",
      "batch: 5605, loss: 0.3481120467185974\n",
      "batch: 5606, loss: 0.3021090626716614\n",
      "batch: 5607, loss: 0.2012704610824585\n",
      "batch: 5608, loss: 0.3020760416984558\n",
      "batch: 5609, loss: 0.08888548612594604\n",
      "batch: 5610, loss: 0.13575324416160583\n",
      "batch: 5611, loss: 0.06617897748947144\n",
      "batch: 5612, loss: 0.4041175842285156\n",
      "batch: 5613, loss: 0.20277762413024902\n",
      "batch: 5614, loss: 0.21448510885238647\n",
      "batch: 5615, loss: 0.19550536572933197\n",
      "batch: 5616, loss: 0.1833142638206482\n",
      "batch: 5617, loss: 0.17567436397075653\n",
      "batch: 5618, loss: 0.13725903630256653\n",
      "batch: 5619, loss: 0.5387368202209473\n",
      "batch: 5620, loss: 0.3032829165458679\n",
      "batch: 5621, loss: 0.11548146605491638\n",
      "batch: 5622, loss: 0.07985645532608032\n",
      "batch: 5623, loss: 0.1923898458480835\n",
      "batch: 5624, loss: 0.24861478805541992\n",
      "batch: 5625, loss: 0.3001481294631958\n",
      "batch: 5626, loss: 0.22544054687023163\n",
      "batch: 5627, loss: 0.16937068104743958\n",
      "batch: 5628, loss: 0.22039511799812317\n",
      "batch: 5629, loss: 0.14303304255008698\n",
      "batch: 5630, loss: 0.12713278830051422\n",
      "batch: 5631, loss: 0.14792762696743011\n",
      "batch: 5632, loss: 0.2109379917383194\n",
      "batch: 5633, loss: 0.10292825102806091\n",
      "batch: 5634, loss: 0.18418751657009125\n",
      "batch: 5635, loss: 0.21027898788452148\n",
      "batch: 5636, loss: 0.36042705178260803\n",
      "batch: 5637, loss: 0.21557438373565674\n",
      "batch: 5638, loss: 0.16743819415569305\n",
      "batch: 5639, loss: 0.22085563838481903\n",
      "batch: 5640, loss: 0.16943782567977905\n",
      "batch: 5641, loss: 0.4209364056587219\n",
      "batch: 5642, loss: 0.15916982293128967\n",
      "batch: 5643, loss: 0.18612968921661377\n",
      "batch: 5644, loss: 0.16652542352676392\n",
      "batch: 5645, loss: 0.23945888876914978\n",
      "batch: 5646, loss: 0.18138039112091064\n",
      "batch: 5647, loss: 0.28602662682533264\n",
      "batch: 5648, loss: 0.08348578959703445\n",
      "batch: 5649, loss: 0.1915150284767151\n",
      "batch: 5650, loss: 0.2866857647895813\n",
      "batch: 5651, loss: 0.16725057363510132\n",
      "batch: 5652, loss: 0.21953585743904114\n",
      "batch: 5653, loss: 0.22040709853172302\n",
      "batch: 5654, loss: 0.20474374294281006\n",
      "batch: 5655, loss: 0.12949982285499573\n",
      "batch: 5656, loss: 0.13834552466869354\n",
      "batch: 5657, loss: 0.1921961009502411\n",
      "batch: 5658, loss: 0.47723251581192017\n",
      "batch: 5659, loss: 0.21251380443572998\n",
      "batch: 5660, loss: 0.215216726064682\n",
      "batch: 5661, loss: 0.14283673465251923\n",
      "batch: 5662, loss: 0.2520902752876282\n",
      "batch: 5663, loss: 0.12428853660821915\n",
      "batch: 5664, loss: 0.13499051332473755\n",
      "batch: 5665, loss: 0.13527387380599976\n",
      "batch: 5666, loss: 0.13205324113368988\n",
      "batch: 5667, loss: 0.13880082964897156\n",
      "batch: 5668, loss: 0.06977417320013046\n",
      "batch: 5669, loss: 0.08986601233482361\n",
      "batch: 5670, loss: 0.1225343644618988\n",
      "batch: 5671, loss: 0.36877521872520447\n",
      "batch: 5672, loss: 0.31120726466178894\n",
      "batch: 5673, loss: 0.1965540051460266\n",
      "batch: 5674, loss: 0.22716164588928223\n",
      "batch: 5675, loss: 0.15949876606464386\n",
      "batch: 5676, loss: 0.14005307853221893\n",
      "batch: 5677, loss: 0.12593239545822144\n",
      "batch: 5678, loss: 0.2825913727283478\n",
      "batch: 5679, loss: 0.06697990745306015\n",
      "batch: 5680, loss: 0.20626713335514069\n",
      "batch: 5681, loss: 0.22272324562072754\n",
      "batch: 5682, loss: 0.32103294134140015\n",
      "batch: 5683, loss: 0.18642261624336243\n",
      "batch: 5684, loss: 0.2576785087585449\n",
      "batch: 5685, loss: 0.10646315664052963\n",
      "batch: 5686, loss: 0.2557971477508545\n",
      "batch: 5687, loss: 0.16058439016342163\n",
      "batch: 5688, loss: 0.25314226746559143\n",
      "batch: 5689, loss: 0.18670761585235596\n",
      "batch: 5690, loss: 0.20597176253795624\n",
      "batch: 5691, loss: 0.22029292583465576\n",
      "batch: 5692, loss: 0.24104389548301697\n",
      "batch: 5693, loss: 0.21323415637016296\n",
      "batch: 5694, loss: 0.11681154370307922\n",
      "batch: 5695, loss: 0.3479323983192444\n",
      "batch: 5696, loss: 0.23748016357421875\n",
      "batch: 5697, loss: 0.19923274219036102\n",
      "batch: 5698, loss: 0.1681540161371231\n",
      "batch: 5699, loss: 0.3491208255290985\n",
      "batch: 5700, loss: 0.17578855156898499\n",
      "model saved to ./saving/model.ckpt-57\n",
      "batch: 5701, loss: 0.12192806601524353\n",
      "batch: 5702, loss: 0.19042032957077026\n",
      "batch: 5703, loss: 0.37764912843704224\n",
      "batch: 5704, loss: 0.16063672304153442\n",
      "batch: 5705, loss: 0.4235292375087738\n",
      "batch: 5706, loss: 0.1978958696126938\n",
      "batch: 5707, loss: 0.0972575768828392\n",
      "batch: 5708, loss: 0.18150146305561066\n",
      "batch: 5709, loss: 0.19824782013893127\n",
      "batch: 5710, loss: 0.2985040247440338\n",
      "batch: 5711, loss: 0.19969648122787476\n",
      "batch: 5712, loss: 0.19657623767852783\n",
      "batch: 5713, loss: 0.14688709378242493\n",
      "batch: 5714, loss: 0.23856836557388306\n",
      "batch: 5715, loss: 0.10119947791099548\n",
      "batch: 5716, loss: 0.2841562032699585\n",
      "batch: 5717, loss: 0.16028207540512085\n",
      "batch: 5718, loss: 0.20667125284671783\n",
      "batch: 5719, loss: 0.29564595222473145\n",
      "batch: 5720, loss: 0.18905818462371826\n",
      "batch: 5721, loss: 0.16491369903087616\n",
      "batch: 5722, loss: 0.1980590969324112\n",
      "batch: 5723, loss: 0.10972672700881958\n",
      "batch: 5724, loss: 0.3009209632873535\n",
      "batch: 5725, loss: 0.0987367108464241\n",
      "batch: 5726, loss: 0.3308068513870239\n",
      "batch: 5727, loss: 0.16911421716213226\n",
      "batch: 5728, loss: 0.32038038969039917\n",
      "batch: 5729, loss: 0.0936889797449112\n",
      "batch: 5730, loss: 0.18189388513565063\n",
      "batch: 5731, loss: 0.26261281967163086\n",
      "batch: 5732, loss: 0.18624670803546906\n",
      "batch: 5733, loss: 0.10822170972824097\n",
      "batch: 5734, loss: 0.26578980684280396\n",
      "batch: 5735, loss: 0.11951471865177155\n",
      "batch: 5736, loss: 0.3131214380264282\n",
      "batch: 5737, loss: 0.33044004440307617\n",
      "batch: 5738, loss: 0.1358332633972168\n",
      "batch: 5739, loss: 0.2711635231971741\n",
      "batch: 5740, loss: 0.1229301169514656\n",
      "batch: 5741, loss: 0.1257133185863495\n",
      "batch: 5742, loss: 0.2531728148460388\n",
      "batch: 5743, loss: 0.21720755100250244\n",
      "batch: 5744, loss: 0.18674279749393463\n",
      "batch: 5745, loss: 0.07090075314044952\n",
      "batch: 5746, loss: 0.21807125210762024\n",
      "batch: 5747, loss: 0.16591477394104004\n",
      "batch: 5748, loss: 0.24747420847415924\n",
      "batch: 5749, loss: 0.4558495879173279\n",
      "batch: 5750, loss: 0.34643107652664185\n",
      "batch: 5751, loss: 0.31565821170806885\n",
      "batch: 5752, loss: 0.14924755692481995\n",
      "batch: 5753, loss: 0.1419084072113037\n",
      "batch: 5754, loss: 0.15055334568023682\n",
      "batch: 5755, loss: 0.2190735638141632\n",
      "batch: 5756, loss: 0.13890480995178223\n",
      "batch: 5757, loss: 0.13580606877803802\n",
      "batch: 5758, loss: 0.23421037197113037\n",
      "batch: 5759, loss: 0.1575891077518463\n",
      "batch: 5760, loss: 0.11361189186573029\n",
      "batch: 5761, loss: 0.2906489968299866\n",
      "batch: 5762, loss: 0.41393715143203735\n",
      "batch: 5763, loss: 0.22502055764198303\n",
      "batch: 5764, loss: 0.1267397105693817\n",
      "batch: 5765, loss: 0.13331446051597595\n",
      "batch: 5766, loss: 0.210332989692688\n",
      "batch: 5767, loss: 0.28416281938552856\n",
      "batch: 5768, loss: 0.17021310329437256\n",
      "batch: 5769, loss: 0.2238612174987793\n",
      "batch: 5770, loss: 0.22121426463127136\n",
      "batch: 5771, loss: 0.16252541542053223\n",
      "batch: 5772, loss: 0.1930827647447586\n",
      "batch: 5773, loss: 0.2810314893722534\n",
      "batch: 5774, loss: 0.1538299322128296\n",
      "batch: 5775, loss: 0.37532877922058105\n",
      "batch: 5776, loss: 0.38541853427886963\n",
      "batch: 5777, loss: 0.08127614855766296\n",
      "batch: 5778, loss: 0.09696421027183533\n",
      "batch: 5779, loss: 0.16332098841667175\n",
      "batch: 5780, loss: 0.34381940960884094\n",
      "batch: 5781, loss: 0.07387101650238037\n",
      "batch: 5782, loss: 0.39735162258148193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 5783, loss: 0.15116417407989502\n",
      "batch: 5784, loss: 0.15678957104682922\n",
      "batch: 5785, loss: 0.2848471999168396\n",
      "batch: 5786, loss: 0.29250964522361755\n",
      "batch: 5787, loss: 0.08092573285102844\n",
      "batch: 5788, loss: 0.12522929906845093\n",
      "batch: 5789, loss: 0.1372615098953247\n",
      "batch: 5790, loss: 0.4959462583065033\n",
      "batch: 5791, loss: 0.13935339450836182\n",
      "batch: 5792, loss: 0.14415140450000763\n",
      "batch: 5793, loss: 0.1859539896249771\n",
      "batch: 5794, loss: 0.2077370285987854\n",
      "batch: 5795, loss: 0.15368451178073883\n",
      "batch: 5796, loss: 0.22916941344738007\n",
      "batch: 5797, loss: 0.20161308348178864\n",
      "batch: 5798, loss: 0.1061401218175888\n",
      "batch: 5799, loss: 0.16018939018249512\n",
      "batch: 5800, loss: 0.36308690905570984\n",
      "model saved to ./saving/model.ckpt-58\n",
      "batch: 5801, loss: 0.258384108543396\n",
      "batch: 5802, loss: 0.09653544425964355\n",
      "batch: 5803, loss: 0.07594224065542221\n",
      "batch: 5804, loss: 0.2769502103328705\n",
      "batch: 5805, loss: 0.2089429348707199\n",
      "batch: 5806, loss: 0.14479878544807434\n",
      "batch: 5807, loss: 0.12227045744657516\n",
      "batch: 5808, loss: 0.057597849518060684\n",
      "batch: 5809, loss: 0.18644843995571136\n",
      "batch: 5810, loss: 0.10576623678207397\n",
      "batch: 5811, loss: 0.19474363327026367\n",
      "batch: 5812, loss: 0.1513390839099884\n",
      "batch: 5813, loss: 0.12184137105941772\n",
      "batch: 5814, loss: 0.2461140751838684\n",
      "batch: 5815, loss: 0.2737050950527191\n",
      "batch: 5816, loss: 0.26243698596954346\n",
      "batch: 5817, loss: 0.3459833562374115\n",
      "batch: 5818, loss: 0.13932372629642487\n",
      "batch: 5819, loss: 0.2499556839466095\n",
      "batch: 5820, loss: 0.11117133498191833\n",
      "batch: 5821, loss: 0.1765943169593811\n",
      "batch: 5822, loss: 0.40938031673431396\n",
      "batch: 5823, loss: 0.20449823141098022\n",
      "batch: 5824, loss: 0.2992687523365021\n",
      "batch: 5825, loss: 0.275158554315567\n",
      "batch: 5826, loss: 0.0876094251871109\n",
      "batch: 5827, loss: 0.19903051853179932\n",
      "batch: 5828, loss: 0.23702843487262726\n",
      "batch: 5829, loss: 0.2511988878250122\n",
      "batch: 5830, loss: 0.14719267189502716\n",
      "batch: 5831, loss: 0.17016857862472534\n",
      "batch: 5832, loss: 0.07934332638978958\n",
      "batch: 5833, loss: 0.2917332649230957\n",
      "batch: 5834, loss: 0.13671332597732544\n",
      "batch: 5835, loss: 0.24626952409744263\n",
      "batch: 5836, loss: 0.2635268568992615\n",
      "batch: 5837, loss: 0.14275266230106354\n",
      "batch: 5838, loss: 0.365913450717926\n",
      "batch: 5839, loss: 0.39184480905532837\n",
      "batch: 5840, loss: 0.11073005944490433\n",
      "batch: 5841, loss: 0.16245633363723755\n",
      "batch: 5842, loss: 0.13752305507659912\n",
      "batch: 5843, loss: 0.15263928472995758\n",
      "batch: 5844, loss: 0.30619940161705017\n",
      "batch: 5845, loss: 0.12231019139289856\n",
      "batch: 5846, loss: 0.3493197560310364\n",
      "batch: 5847, loss: 0.3770245611667633\n",
      "batch: 5848, loss: 0.06271933764219284\n",
      "batch: 5849, loss: 0.20802506804466248\n",
      "batch: 5850, loss: 0.12112320959568024\n",
      "batch: 5851, loss: 0.11810227483510971\n",
      "batch: 5852, loss: 0.1463703215122223\n",
      "batch: 5853, loss: 0.1845627874135971\n",
      "batch: 5854, loss: 0.07305222004652023\n",
      "batch: 5855, loss: 0.14408443868160248\n",
      "batch: 5856, loss: 0.23731958866119385\n",
      "batch: 5857, loss: 0.14921851456165314\n",
      "batch: 5858, loss: 0.16605274379253387\n",
      "batch: 5859, loss: 0.12598547339439392\n",
      "batch: 5860, loss: 0.11071671545505524\n",
      "batch: 5861, loss: 0.18774136900901794\n",
      "batch: 5862, loss: 0.1525251269340515\n",
      "batch: 5863, loss: 0.14701420068740845\n",
      "batch: 5864, loss: 0.13084179162979126\n",
      "batch: 5865, loss: 0.17323032021522522\n",
      "batch: 5866, loss: 0.13619445264339447\n",
      "batch: 5867, loss: 0.17068389058113098\n",
      "batch: 5868, loss: 0.20383889973163605\n",
      "batch: 5869, loss: 0.09467774629592896\n",
      "batch: 5870, loss: 0.17949950695037842\n",
      "batch: 5871, loss: 0.15916790068149567\n",
      "batch: 5872, loss: 0.1299169659614563\n",
      "batch: 5873, loss: 0.13828593492507935\n",
      "batch: 5874, loss: 0.1663275957107544\n",
      "batch: 5875, loss: 0.11492027342319489\n",
      "batch: 5876, loss: 0.17952196300029755\n",
      "batch: 5877, loss: 0.13935256004333496\n",
      "batch: 5878, loss: 0.23577392101287842\n",
      "batch: 5879, loss: 0.144149512052536\n",
      "batch: 5880, loss: 0.1418001651763916\n",
      "batch: 5881, loss: 0.09661044925451279\n",
      "batch: 5882, loss: 0.11886569857597351\n",
      "batch: 5883, loss: 0.4263114929199219\n",
      "batch: 5884, loss: 0.1743324100971222\n",
      "batch: 5885, loss: 0.15640121698379517\n",
      "batch: 5886, loss: 0.21227282285690308\n",
      "batch: 5887, loss: 0.1937105655670166\n",
      "batch: 5888, loss: 0.15096160769462585\n",
      "batch: 5889, loss: 0.20417499542236328\n",
      "batch: 5890, loss: 0.12740013003349304\n",
      "batch: 5891, loss: 0.06986714899539948\n",
      "batch: 5892, loss: 0.1621987521648407\n",
      "batch: 5893, loss: 0.12014169991016388\n",
      "batch: 5894, loss: 0.12191387265920639\n",
      "batch: 5895, loss: 0.254209965467453\n",
      "batch: 5896, loss: 0.2425834834575653\n",
      "batch: 5897, loss: 0.26763561367988586\n",
      "batch: 5898, loss: 0.26338016986846924\n",
      "batch: 5899, loss: 0.1688547134399414\n",
      "batch: 5900, loss: 0.12771770358085632\n",
      "model saved to ./saving/model.ckpt-59\n",
      "batch: 5901, loss: 0.32445868849754333\n",
      "batch: 5902, loss: 0.21003982424736023\n",
      "batch: 5903, loss: 0.1482056975364685\n",
      "batch: 5904, loss: 0.14605537056922913\n",
      "batch: 5905, loss: 0.22273072600364685\n",
      "batch: 5906, loss: 0.13189071416854858\n",
      "batch: 5907, loss: 0.149461030960083\n",
      "batch: 5908, loss: 0.23643800616264343\n",
      "batch: 5909, loss: 0.2903471887111664\n",
      "batch: 5910, loss: 0.20244793593883514\n",
      "batch: 5911, loss: 0.14676199853420258\n",
      "batch: 5912, loss: 0.11436870694160461\n",
      "batch: 5913, loss: 0.28875213861465454\n",
      "batch: 5914, loss: 0.22139182686805725\n",
      "batch: 5915, loss: 0.08301031589508057\n",
      "batch: 5916, loss: 0.36504054069519043\n",
      "batch: 5917, loss: 0.19012346863746643\n",
      "batch: 5918, loss: 0.1760372668504715\n",
      "batch: 5919, loss: 0.22685323655605316\n",
      "batch: 5920, loss: 0.20665450394153595\n",
      "batch: 5921, loss: 0.18850338459014893\n",
      "batch: 5922, loss: 0.1828710436820984\n",
      "batch: 5923, loss: 0.12367790192365646\n",
      "batch: 5924, loss: 0.1486695259809494\n",
      "batch: 5925, loss: 0.11911506950855255\n",
      "batch: 5926, loss: 0.2008379101753235\n",
      "batch: 5927, loss: 0.15688690543174744\n",
      "batch: 5928, loss: 0.2760673761367798\n",
      "batch: 5929, loss: 0.09081041067838669\n",
      "batch: 5930, loss: 0.16165277361869812\n",
      "batch: 5931, loss: 0.18968430161476135\n",
      "batch: 5932, loss: 0.1473025381565094\n",
      "batch: 5933, loss: 0.180201917886734\n",
      "batch: 5934, loss: 0.3205193877220154\n",
      "batch: 5935, loss: 0.17376087605953217\n",
      "batch: 5936, loss: 0.14523687958717346\n",
      "batch: 5937, loss: 0.08850914239883423\n",
      "batch: 5938, loss: 0.14996278285980225\n",
      "batch: 5939, loss: 0.19479675590991974\n",
      "batch: 5940, loss: 0.20419159531593323\n",
      "batch: 5941, loss: 0.17110493779182434\n",
      "batch: 5942, loss: 0.11260556429624557\n",
      "batch: 5943, loss: 0.1724368780851364\n",
      "batch: 5944, loss: 0.13909626007080078\n",
      "batch: 5945, loss: 0.1997198462486267\n",
      "batch: 5946, loss: 0.10660625249147415\n",
      "batch: 5947, loss: 0.14839591085910797\n",
      "batch: 5948, loss: 0.45828497409820557\n",
      "batch: 5949, loss: 0.339945524930954\n",
      "batch: 5950, loss: 0.22793260216712952\n",
      "batch: 5951, loss: 0.37059348821640015\n",
      "batch: 5952, loss: 0.16988958418369293\n",
      "batch: 5953, loss: 0.10356368869543076\n",
      "batch: 5954, loss: 0.11755318194627762\n",
      "batch: 5955, loss: 0.2134505808353424\n",
      "batch: 5956, loss: 0.22015227377414703\n",
      "batch: 5957, loss: 0.25869956612586975\n",
      "batch: 5958, loss: 0.1735900640487671\n",
      "batch: 5959, loss: 0.1207021027803421\n",
      "batch: 5960, loss: 0.10860717296600342\n",
      "batch: 5961, loss: 0.21468430757522583\n",
      "batch: 5962, loss: 0.0998530313372612\n",
      "batch: 5963, loss: 0.2989945113658905\n",
      "batch: 5964, loss: 0.3595360815525055\n",
      "batch: 5965, loss: 0.11721745878458023\n",
      "batch: 5966, loss: 0.3874415159225464\n",
      "batch: 5967, loss: 0.3016532063484192\n",
      "batch: 5968, loss: 0.17108184099197388\n",
      "batch: 5969, loss: 0.12321025878190994\n",
      "batch: 5970, loss: 0.25870317220687866\n",
      "batch: 5971, loss: 0.0863509252667427\n",
      "batch: 5972, loss: 0.1276092529296875\n",
      "batch: 5973, loss: 0.27180016040802\n",
      "batch: 5974, loss: 0.19196376204490662\n",
      "batch: 5975, loss: 0.10431511700153351\n",
      "batch: 5976, loss: 0.25534528493881226\n",
      "batch: 5977, loss: 0.11262090504169464\n",
      "batch: 5978, loss: 0.2492455691099167\n",
      "batch: 5979, loss: 0.15326598286628723\n",
      "batch: 5980, loss: 0.13366736471652985\n",
      "batch: 5981, loss: 0.21512702107429504\n",
      "batch: 5982, loss: 0.15574073791503906\n",
      "batch: 5983, loss: 0.3769834041595459\n",
      "batch: 5984, loss: 0.2936630845069885\n",
      "batch: 5985, loss: 0.0960058867931366\n",
      "batch: 5986, loss: 0.18373429775238037\n",
      "batch: 5987, loss: 0.2619553506374359\n",
      "batch: 5988, loss: 0.15007513761520386\n",
      "batch: 5989, loss: 0.15577395260334015\n",
      "batch: 5990, loss: 0.24315878748893738\n",
      "batch: 5991, loss: 0.21787986159324646\n",
      "batch: 5992, loss: 0.08003838360309601\n",
      "batch: 5993, loss: 0.22291183471679688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 5994, loss: 0.28243714570999146\n",
      "batch: 5995, loss: 0.21662339568138123\n",
      "batch: 5996, loss: 0.1325167566537857\n",
      "batch: 5997, loss: 0.2848552465438843\n",
      "batch: 5998, loss: 0.24732419848442078\n",
      "batch: 5999, loss: 0.11805877834558487\n",
      "batch: 6000, loss: 0.11787468194961548\n",
      "model saved to ./saving/model.ckpt-60\n",
      "batch: 6001, loss: 0.20825029909610748\n",
      "batch: 6002, loss: 0.32979902625083923\n",
      "batch: 6003, loss: 0.1415204107761383\n",
      "batch: 6004, loss: 0.17761272192001343\n",
      "batch: 6005, loss: 0.23387950658798218\n",
      "batch: 6006, loss: 0.10954125970602036\n",
      "batch: 6007, loss: 0.21809720993041992\n",
      "batch: 6008, loss: 0.18157167732715607\n",
      "batch: 6009, loss: 0.14141523838043213\n",
      "batch: 6010, loss: 0.13959532976150513\n",
      "batch: 6011, loss: 0.15738365054130554\n",
      "batch: 6012, loss: 0.27731838822364807\n",
      "batch: 6013, loss: 0.08228042721748352\n",
      "batch: 6014, loss: 0.287256121635437\n",
      "batch: 6015, loss: 0.30742770433425903\n",
      "batch: 6016, loss: 0.1036970317363739\n",
      "batch: 6017, loss: 0.15295740962028503\n",
      "batch: 6018, loss: 0.14873649179935455\n",
      "batch: 6019, loss: 0.1770886331796646\n",
      "batch: 6020, loss: 0.2975344955921173\n",
      "batch: 6021, loss: 0.30221736431121826\n",
      "batch: 6022, loss: 0.15318691730499268\n",
      "batch: 6023, loss: 0.1391049325466156\n",
      "batch: 6024, loss: 0.22410878539085388\n",
      "batch: 6025, loss: 0.30264541506767273\n",
      "batch: 6026, loss: 0.051724012941122055\n",
      "batch: 6027, loss: 0.17967256903648376\n",
      "batch: 6028, loss: 0.21335260570049286\n",
      "batch: 6029, loss: 0.21026045083999634\n",
      "batch: 6030, loss: 0.15440307557582855\n",
      "batch: 6031, loss: 0.19338726997375488\n",
      "batch: 6032, loss: 0.22567221522331238\n",
      "batch: 6033, loss: 0.14251969754695892\n",
      "batch: 6034, loss: 0.20972323417663574\n",
      "batch: 6035, loss: 0.19588413834571838\n",
      "batch: 6036, loss: 0.12881682813167572\n",
      "batch: 6037, loss: 0.3272971212863922\n",
      "batch: 6038, loss: 0.15365922451019287\n",
      "batch: 6039, loss: 0.3623456358909607\n",
      "batch: 6040, loss: 0.19572395086288452\n",
      "batch: 6041, loss: 0.11153339594602585\n",
      "batch: 6042, loss: 0.08617141097784042\n",
      "batch: 6043, loss: 0.1735178828239441\n",
      "batch: 6044, loss: 0.1960546374320984\n",
      "batch: 6045, loss: 0.1436755657196045\n",
      "batch: 6046, loss: 0.25363630056381226\n",
      "batch: 6047, loss: 0.19198262691497803\n",
      "batch: 6048, loss: 0.1173844262957573\n",
      "batch: 6049, loss: 0.24987146258354187\n",
      "batch: 6050, loss: 0.16259765625\n",
      "batch: 6051, loss: 0.11818334460258484\n",
      "batch: 6052, loss: 0.2573763132095337\n",
      "batch: 6053, loss: 0.1725037842988968\n",
      "batch: 6054, loss: 0.13100188970565796\n",
      "batch: 6055, loss: 0.16393950581550598\n",
      "batch: 6056, loss: 0.0465642511844635\n",
      "batch: 6057, loss: 0.09649598598480225\n",
      "batch: 6058, loss: 0.18876679241657257\n",
      "batch: 6059, loss: 0.2640112638473511\n",
      "batch: 6060, loss: 0.23795059323310852\n",
      "batch: 6061, loss: 0.32840120792388916\n",
      "batch: 6062, loss: 0.1292305290699005\n",
      "batch: 6063, loss: 0.20836755633354187\n",
      "batch: 6064, loss: 0.10470855236053467\n",
      "batch: 6065, loss: 0.18815675377845764\n",
      "batch: 6066, loss: 0.21999402344226837\n",
      "batch: 6067, loss: 0.17339754104614258\n",
      "batch: 6068, loss: 0.28568291664123535\n",
      "batch: 6069, loss: 0.26495206356048584\n",
      "batch: 6070, loss: 0.28141993284225464\n",
      "batch: 6071, loss: 0.17152298986911774\n",
      "batch: 6072, loss: 0.2049665004014969\n",
      "batch: 6073, loss: 0.20493602752685547\n",
      "batch: 6074, loss: 0.28571176528930664\n",
      "batch: 6075, loss: 0.2019231915473938\n",
      "batch: 6076, loss: 0.21178653836250305\n",
      "batch: 6077, loss: 0.23517724871635437\n",
      "batch: 6078, loss: 0.28693321347236633\n",
      "batch: 6079, loss: 0.25128576159477234\n",
      "batch: 6080, loss: 0.13533172011375427\n",
      "batch: 6081, loss: 0.11466965079307556\n",
      "batch: 6082, loss: 0.2209111452102661\n",
      "batch: 6083, loss: 0.08859997242689133\n",
      "batch: 6084, loss: 0.13121888041496277\n",
      "batch: 6085, loss: 0.12404942512512207\n",
      "batch: 6086, loss: 0.1190531998872757\n",
      "batch: 6087, loss: 0.197413370013237\n",
      "batch: 6088, loss: 0.06459204107522964\n",
      "batch: 6089, loss: 0.0854799672961235\n",
      "batch: 6090, loss: 0.12300251424312592\n",
      "batch: 6091, loss: 0.12004395574331284\n",
      "batch: 6092, loss: 0.1742335855960846\n",
      "batch: 6093, loss: 0.3199271559715271\n",
      "batch: 6094, loss: 0.3020467460155487\n",
      "batch: 6095, loss: 0.09465039521455765\n",
      "batch: 6096, loss: 0.10861609131097794\n",
      "batch: 6097, loss: 0.24897953867912292\n",
      "batch: 6098, loss: 0.48238083720207214\n",
      "batch: 6099, loss: 0.1739656925201416\n",
      "batch: 6100, loss: 0.1980273723602295\n",
      "model saved to ./saving/model.ckpt-61\n",
      "batch: 6101, loss: 0.3209986090660095\n",
      "batch: 6102, loss: 0.1558576226234436\n",
      "batch: 6103, loss: 0.1734141707420349\n",
      "batch: 6104, loss: 0.10915932804346085\n",
      "batch: 6105, loss: 0.18332557380199432\n",
      "batch: 6106, loss: 0.23912937939167023\n",
      "batch: 6107, loss: 0.14980004727840424\n",
      "batch: 6108, loss: 0.17579777538776398\n",
      "batch: 6109, loss: 0.1857016682624817\n",
      "batch: 6110, loss: 0.22098946571350098\n",
      "batch: 6111, loss: 0.1883067935705185\n",
      "batch: 6112, loss: 0.2064918577671051\n",
      "batch: 6113, loss: 0.271915078163147\n",
      "batch: 6114, loss: 0.16174229979515076\n",
      "batch: 6115, loss: 0.1782575249671936\n",
      "batch: 6116, loss: 0.314408540725708\n",
      "batch: 6117, loss: 0.17948386073112488\n",
      "batch: 6118, loss: 0.16559924185276031\n",
      "batch: 6119, loss: 0.18674525618553162\n",
      "batch: 6120, loss: 0.12163183838129044\n",
      "batch: 6121, loss: 0.2455512285232544\n",
      "batch: 6122, loss: 0.37548279762268066\n",
      "batch: 6123, loss: 0.19908742606639862\n",
      "batch: 6124, loss: 0.245937317609787\n",
      "batch: 6125, loss: 0.06871260702610016\n",
      "batch: 6126, loss: 0.2146233320236206\n",
      "batch: 6127, loss: 0.192051500082016\n",
      "batch: 6128, loss: 0.26282835006713867\n",
      "batch: 6129, loss: 0.13777482509613037\n",
      "batch: 6130, loss: 0.3200657367706299\n",
      "batch: 6131, loss: 0.17337623238563538\n",
      "batch: 6132, loss: 0.18521198630332947\n",
      "batch: 6133, loss: 0.09580636024475098\n",
      "batch: 6134, loss: 0.27316272258758545\n",
      "batch: 6135, loss: 0.18592509627342224\n",
      "batch: 6136, loss: 0.19212517142295837\n",
      "batch: 6137, loss: 0.25630074739456177\n",
      "batch: 6138, loss: 0.2777217626571655\n",
      "batch: 6139, loss: 0.08595141023397446\n",
      "batch: 6140, loss: 0.12414105236530304\n",
      "batch: 6141, loss: 0.23809823393821716\n",
      "batch: 6142, loss: 0.2717163562774658\n",
      "batch: 6143, loss: 0.16249804198741913\n",
      "batch: 6144, loss: 0.24500685930252075\n",
      "batch: 6145, loss: 0.38856834173202515\n",
      "batch: 6146, loss: 0.1804785430431366\n",
      "batch: 6147, loss: 0.2406121790409088\n",
      "batch: 6148, loss: 0.12197309732437134\n",
      "batch: 6149, loss: 0.26218152046203613\n",
      "batch: 6150, loss: 0.2687377333641052\n",
      "batch: 6151, loss: 0.10082197189331055\n",
      "batch: 6152, loss: 0.11284904181957245\n",
      "batch: 6153, loss: 0.21218933165073395\n",
      "batch: 6154, loss: 0.13582724332809448\n",
      "batch: 6155, loss: 0.22340761125087738\n",
      "batch: 6156, loss: 0.16304564476013184\n",
      "batch: 6157, loss: 0.23984146118164062\n",
      "batch: 6158, loss: 0.3806111812591553\n",
      "batch: 6159, loss: 0.08594075590372086\n",
      "batch: 6160, loss: 0.113323874771595\n",
      "batch: 6161, loss: 0.19121301174163818\n",
      "batch: 6162, loss: 0.18268078565597534\n",
      "batch: 6163, loss: 0.1951475590467453\n",
      "batch: 6164, loss: 0.39806094765663147\n",
      "batch: 6165, loss: 0.11759361624717712\n",
      "batch: 6166, loss: 0.2233811616897583\n",
      "batch: 6167, loss: 0.30216342210769653\n",
      "batch: 6168, loss: 0.10357978940010071\n",
      "batch: 6169, loss: 0.19810321927070618\n",
      "batch: 6170, loss: 0.10055460035800934\n",
      "batch: 6171, loss: 0.12515318393707275\n",
      "batch: 6172, loss: 0.11853189766407013\n",
      "batch: 6173, loss: 0.34535229206085205\n",
      "batch: 6174, loss: 0.20374706387519836\n",
      "batch: 6175, loss: 0.16552448272705078\n",
      "batch: 6176, loss: 0.16790170967578888\n",
      "batch: 6177, loss: 0.17053592205047607\n",
      "batch: 6178, loss: 0.11522518843412399\n",
      "batch: 6179, loss: 0.1286696493625641\n",
      "batch: 6180, loss: 0.12473936378955841\n",
      "batch: 6181, loss: 0.288375586271286\n",
      "batch: 6182, loss: 0.21791145205497742\n",
      "batch: 6183, loss: 0.10451192408800125\n",
      "batch: 6184, loss: 0.23089665174484253\n",
      "batch: 6185, loss: 0.1792985200881958\n",
      "batch: 6186, loss: 0.27465885877609253\n",
      "batch: 6187, loss: 0.12400315701961517\n",
      "batch: 6188, loss: 0.26576995849609375\n",
      "batch: 6189, loss: 0.2578285038471222\n",
      "batch: 6190, loss: 0.21171577274799347\n",
      "batch: 6191, loss: 0.26475346088409424\n",
      "batch: 6192, loss: 0.27084437012672424\n",
      "batch: 6193, loss: 0.11642955988645554\n",
      "batch: 6194, loss: 0.2834712862968445\n",
      "batch: 6195, loss: 0.15712180733680725\n",
      "batch: 6196, loss: 0.22623544931411743\n",
      "batch: 6197, loss: 0.11002832651138306\n",
      "batch: 6198, loss: 0.22750404477119446\n",
      "batch: 6199, loss: 0.23016008734703064\n",
      "batch: 6200, loss: 0.15854261815547943\n",
      "model saved to ./saving/model.ckpt-62\n",
      "batch: 6201, loss: 0.25156301259994507\n",
      "batch: 6202, loss: 0.2120209038257599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 6203, loss: 0.08106721937656403\n",
      "batch: 6204, loss: 0.3216285705566406\n",
      "batch: 6205, loss: 0.2980450391769409\n",
      "batch: 6206, loss: 0.1825571060180664\n",
      "batch: 6207, loss: 0.22998040914535522\n",
      "batch: 6208, loss: 0.22053201496601105\n",
      "batch: 6209, loss: 0.29486802220344543\n",
      "batch: 6210, loss: 0.2648848295211792\n",
      "batch: 6211, loss: 0.2587527930736542\n",
      "batch: 6212, loss: 0.07107293605804443\n",
      "batch: 6213, loss: 0.3633706569671631\n",
      "batch: 6214, loss: 0.29473862051963806\n",
      "batch: 6215, loss: 0.1181827038526535\n",
      "batch: 6216, loss: 0.23855087161064148\n",
      "batch: 6217, loss: 0.28049764037132263\n",
      "batch: 6218, loss: 0.252063512802124\n",
      "batch: 6219, loss: 0.19609792530536652\n",
      "batch: 6220, loss: 0.1211511641740799\n",
      "batch: 6221, loss: 0.13988298177719116\n",
      "batch: 6222, loss: 0.053489115089178085\n",
      "batch: 6223, loss: 0.28274771571159363\n",
      "batch: 6224, loss: 0.1768580675125122\n",
      "batch: 6225, loss: 0.26329106092453003\n",
      "batch: 6226, loss: 0.26724183559417725\n",
      "batch: 6227, loss: 0.2925008535385132\n",
      "batch: 6228, loss: 0.13549840450286865\n",
      "batch: 6229, loss: 0.19249925017356873\n",
      "batch: 6230, loss: 0.17238174378871918\n",
      "batch: 6231, loss: 0.24599172174930573\n",
      "batch: 6232, loss: 0.26674944162368774\n",
      "batch: 6233, loss: 0.1124417707324028\n",
      "batch: 6234, loss: 0.10481587052345276\n",
      "batch: 6235, loss: 0.08946560323238373\n",
      "batch: 6236, loss: 0.10919180512428284\n",
      "batch: 6237, loss: 0.09108518809080124\n",
      "batch: 6238, loss: 0.13800093531608582\n",
      "batch: 6239, loss: 0.10503630340099335\n",
      "batch: 6240, loss: 0.12884093821048737\n",
      "batch: 6241, loss: 0.09120716154575348\n",
      "batch: 6242, loss: 0.2692643702030182\n",
      "batch: 6243, loss: 0.13956815004348755\n",
      "batch: 6244, loss: 0.08264331519603729\n",
      "batch: 6245, loss: 0.24746933579444885\n",
      "batch: 6246, loss: 0.15486955642700195\n",
      "batch: 6247, loss: 0.11301159858703613\n",
      "batch: 6248, loss: 0.12177468091249466\n",
      "batch: 6249, loss: 0.07185417413711548\n",
      "batch: 6250, loss: 0.16538041830062866\n",
      "batch: 6251, loss: 0.14492572844028473\n",
      "batch: 6252, loss: 0.1344529688358307\n",
      "batch: 6253, loss: 0.15392205119132996\n",
      "batch: 6254, loss: 0.2829737663269043\n",
      "batch: 6255, loss: 0.10915552079677582\n",
      "batch: 6256, loss: 0.22950074076652527\n",
      "batch: 6257, loss: 0.0966326966881752\n",
      "batch: 6258, loss: 0.20172175765037537\n",
      "batch: 6259, loss: 0.29982683062553406\n",
      "batch: 6260, loss: 0.20240312814712524\n",
      "batch: 6261, loss: 0.07173902541399002\n",
      "batch: 6262, loss: 0.09753533452749252\n",
      "batch: 6263, loss: 0.11519964039325714\n",
      "batch: 6264, loss: 0.09447629749774933\n",
      "batch: 6265, loss: 0.174826979637146\n",
      "batch: 6266, loss: 0.3412335216999054\n",
      "batch: 6267, loss: 0.13382746279239655\n",
      "batch: 6268, loss: 0.1772748976945877\n",
      "batch: 6269, loss: 0.22343015670776367\n",
      "batch: 6270, loss: 0.3045338988304138\n",
      "batch: 6271, loss: 0.14214085042476654\n",
      "batch: 6272, loss: 0.09757915139198303\n",
      "batch: 6273, loss: 0.1798885464668274\n",
      "batch: 6274, loss: 0.2219085842370987\n",
      "batch: 6275, loss: 0.15764208137989044\n",
      "batch: 6276, loss: 0.4648742973804474\n",
      "batch: 6277, loss: 0.3440793752670288\n",
      "batch: 6278, loss: 0.2541421055793762\n",
      "batch: 6279, loss: 0.156259685754776\n",
      "batch: 6280, loss: 0.31812983751296997\n",
      "batch: 6281, loss: 0.17426204681396484\n",
      "batch: 6282, loss: 0.20332446694374084\n",
      "batch: 6283, loss: 0.13972604274749756\n",
      "batch: 6284, loss: 0.2796391248703003\n",
      "batch: 6285, loss: 0.15220434963703156\n",
      "batch: 6286, loss: 0.16173341870307922\n",
      "batch: 6287, loss: 0.18154530227184296\n",
      "batch: 6288, loss: 0.24083688855171204\n",
      "batch: 6289, loss: 0.2896525263786316\n",
      "batch: 6290, loss: 0.1693948358297348\n",
      "batch: 6291, loss: 0.14622974395751953\n",
      "batch: 6292, loss: 0.28368693590164185\n",
      "batch: 6293, loss: 0.2345706671476364\n",
      "batch: 6294, loss: 0.2165648341178894\n",
      "batch: 6295, loss: 0.21780911087989807\n",
      "batch: 6296, loss: 0.12473129481077194\n",
      "batch: 6297, loss: 0.14633691310882568\n",
      "batch: 6298, loss: 0.20326754450798035\n",
      "batch: 6299, loss: 0.18945884704589844\n",
      "batch: 6300, loss: 0.26837819814682007\n",
      "model saved to ./saving/model.ckpt-63\n",
      "batch: 6301, loss: 0.14897894859313965\n",
      "batch: 6302, loss: 0.10623498260974884\n",
      "batch: 6303, loss: 0.2835991382598877\n",
      "batch: 6304, loss: 0.25312814116477966\n",
      "batch: 6305, loss: 0.17234380543231964\n",
      "batch: 6306, loss: 0.13413287699222565\n",
      "batch: 6307, loss: 0.18010210990905762\n",
      "batch: 6308, loss: 0.17123346030712128\n",
      "batch: 6309, loss: 0.5137730240821838\n",
      "batch: 6310, loss: 0.218788743019104\n",
      "batch: 6311, loss: 0.13258597254753113\n",
      "batch: 6312, loss: 0.19653432071208954\n",
      "batch: 6313, loss: 0.30004870891571045\n",
      "batch: 6314, loss: 0.10786309838294983\n",
      "batch: 6315, loss: 0.15148738026618958\n",
      "batch: 6316, loss: 0.36521008610725403\n",
      "batch: 6317, loss: 0.20035101473331451\n",
      "batch: 6318, loss: 0.20898154377937317\n",
      "batch: 6319, loss: 0.13834796845912933\n",
      "batch: 6320, loss: 0.28560271859169006\n",
      "batch: 6321, loss: 0.46988844871520996\n",
      "batch: 6322, loss: 0.3324970304965973\n",
      "batch: 6323, loss: 0.2086980789899826\n",
      "batch: 6324, loss: 0.3507014513015747\n",
      "batch: 6325, loss: 0.19112816452980042\n",
      "batch: 6326, loss: 0.1454865038394928\n",
      "batch: 6327, loss: 0.3537098467350006\n",
      "batch: 6328, loss: 0.3425474166870117\n",
      "batch: 6329, loss: 0.16398727893829346\n",
      "batch: 6330, loss: 0.17561393976211548\n",
      "batch: 6331, loss: 0.3214677572250366\n",
      "batch: 6332, loss: 0.3010583519935608\n",
      "batch: 6333, loss: 0.16110670566558838\n",
      "batch: 6334, loss: 0.1052662804722786\n",
      "batch: 6335, loss: 0.06951475143432617\n",
      "batch: 6336, loss: 0.4276527762413025\n",
      "batch: 6337, loss: 0.23519045114517212\n",
      "batch: 6338, loss: 0.1852288544178009\n",
      "batch: 6339, loss: 0.22123607993125916\n",
      "batch: 6340, loss: 0.18298059701919556\n",
      "batch: 6341, loss: 0.2502678334712982\n",
      "batch: 6342, loss: 0.2862570881843567\n",
      "batch: 6343, loss: 0.15692690014839172\n",
      "batch: 6344, loss: 0.1367473304271698\n",
      "batch: 6345, loss: 0.20365726947784424\n",
      "batch: 6346, loss: 0.11532390117645264\n",
      "batch: 6347, loss: 0.23723460733890533\n",
      "batch: 6348, loss: 0.12784211337566376\n",
      "batch: 6349, loss: 0.17044761776924133\n",
      "batch: 6350, loss: 0.2362573742866516\n",
      "batch: 6351, loss: 0.11479436606168747\n",
      "batch: 6352, loss: 0.29210585355758667\n",
      "batch: 6353, loss: 0.3011634945869446\n",
      "batch: 6354, loss: 0.17940664291381836\n",
      "batch: 6355, loss: 0.05618629604578018\n",
      "batch: 6356, loss: 0.2266060709953308\n",
      "batch: 6357, loss: 0.17915946245193481\n",
      "batch: 6358, loss: 0.23987826704978943\n",
      "batch: 6359, loss: 0.44349467754364014\n",
      "batch: 6360, loss: 0.18378287553787231\n",
      "batch: 6361, loss: 0.1549317091703415\n",
      "batch: 6362, loss: 0.1929808408021927\n",
      "batch: 6363, loss: 0.17991343140602112\n",
      "batch: 6364, loss: 0.1329951286315918\n",
      "batch: 6365, loss: 0.1181684136390686\n",
      "batch: 6366, loss: 0.07264363765716553\n",
      "batch: 6367, loss: 0.31447577476501465\n",
      "batch: 6368, loss: 0.14304351806640625\n",
      "batch: 6369, loss: 0.10727572441101074\n",
      "batch: 6370, loss: 0.13118289411067963\n",
      "batch: 6371, loss: 0.11480988562107086\n",
      "batch: 6372, loss: 0.1779763102531433\n",
      "batch: 6373, loss: 0.14608365297317505\n",
      "batch: 6374, loss: 0.34016692638397217\n",
      "batch: 6375, loss: 0.19545018672943115\n",
      "batch: 6376, loss: 0.16043908894062042\n",
      "batch: 6377, loss: 0.15889178216457367\n",
      "batch: 6378, loss: 0.17767801880836487\n",
      "batch: 6379, loss: 0.1620425581932068\n",
      "batch: 6380, loss: 0.22514204680919647\n",
      "batch: 6381, loss: 0.28577864170074463\n",
      "batch: 6382, loss: 0.08850155770778656\n",
      "batch: 6383, loss: 0.1542191356420517\n",
      "batch: 6384, loss: 0.19774693250656128\n",
      "batch: 6385, loss: 0.13685759902000427\n",
      "batch: 6386, loss: 0.1798139214515686\n",
      "batch: 6387, loss: 0.32197314500808716\n",
      "batch: 6388, loss: 0.12177830934524536\n",
      "batch: 6389, loss: 0.13277214765548706\n",
      "batch: 6390, loss: 0.1906280815601349\n",
      "batch: 6391, loss: 0.288178026676178\n",
      "batch: 6392, loss: 0.10625369101762772\n",
      "batch: 6393, loss: 0.2112417221069336\n",
      "batch: 6394, loss: 0.152446910738945\n",
      "batch: 6395, loss: 0.2669420838356018\n",
      "batch: 6396, loss: 0.08552053570747375\n",
      "batch: 6397, loss: 0.12303676456212997\n",
      "batch: 6398, loss: 0.20870691537857056\n",
      "batch: 6399, loss: 0.18182310461997986\n",
      "batch: 6400, loss: 0.1187170073390007\n",
      "model saved to ./saving/model.ckpt-64\n",
      "batch: 6401, loss: 0.06467738002538681\n",
      "batch: 6402, loss: 0.14499159157276154\n",
      "batch: 6403, loss: 0.37284207344055176\n",
      "batch: 6404, loss: 0.13764499127864838\n",
      "batch: 6405, loss: 0.1936718225479126\n",
      "batch: 6406, loss: 0.12314088642597198\n",
      "batch: 6407, loss: 0.26276808977127075\n",
      "batch: 6408, loss: 0.3272700011730194\n",
      "batch: 6409, loss: 0.218203604221344\n",
      "batch: 6410, loss: 0.15879309177398682\n",
      "batch: 6411, loss: 0.18775300681591034\n",
      "batch: 6412, loss: 0.1044217199087143\n",
      "batch: 6413, loss: 0.08153596520423889\n",
      "batch: 6414, loss: 0.20807474851608276\n",
      "batch: 6415, loss: 0.0758897215127945\n",
      "batch: 6416, loss: 0.06226889789104462\n",
      "batch: 6417, loss: 0.19110235571861267\n",
      "batch: 6418, loss: 0.06531617045402527\n",
      "batch: 6419, loss: 0.17832496762275696\n",
      "batch: 6420, loss: 0.19083654880523682\n",
      "batch: 6421, loss: 0.10039304196834564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 6422, loss: 0.13826516270637512\n",
      "batch: 6423, loss: 0.13030005991458893\n",
      "batch: 6424, loss: 0.10081750899553299\n",
      "batch: 6425, loss: 0.29055464267730713\n",
      "batch: 6426, loss: 0.18786728382110596\n",
      "batch: 6427, loss: 0.1980808675289154\n",
      "batch: 6428, loss: 0.23310713469982147\n",
      "batch: 6429, loss: 0.1892169862985611\n",
      "batch: 6430, loss: 0.11317341029644012\n",
      "batch: 6431, loss: 0.14721167087554932\n",
      "batch: 6432, loss: 0.12223591655492783\n",
      "batch: 6433, loss: 0.2024250328540802\n",
      "batch: 6434, loss: 0.36528611183166504\n",
      "batch: 6435, loss: 0.2646387815475464\n",
      "batch: 6436, loss: 0.2928076386451721\n",
      "batch: 6437, loss: 0.16918855905532837\n",
      "batch: 6438, loss: 0.20280717313289642\n",
      "batch: 6439, loss: 0.213253915309906\n",
      "batch: 6440, loss: 0.205828458070755\n",
      "batch: 6441, loss: 0.18859243392944336\n",
      "batch: 6442, loss: 0.144327312707901\n",
      "batch: 6443, loss: 0.07275859266519547\n",
      "batch: 6444, loss: 0.22268308699131012\n",
      "batch: 6445, loss: 0.10323454439640045\n",
      "batch: 6446, loss: 0.14999046921730042\n",
      "batch: 6447, loss: 0.18622525036334991\n",
      "batch: 6448, loss: 0.1659938097000122\n",
      "batch: 6449, loss: 0.26300156116485596\n",
      "batch: 6450, loss: 0.18819180130958557\n",
      "batch: 6451, loss: 0.1285364031791687\n",
      "batch: 6452, loss: 0.18652231991291046\n",
      "batch: 6453, loss: 0.23863022029399872\n",
      "batch: 6454, loss: 0.15008127689361572\n",
      "batch: 6455, loss: 0.1536960005760193\n",
      "batch: 6456, loss: 0.12251114845275879\n",
      "batch: 6457, loss: 0.15453793108463287\n",
      "batch: 6458, loss: 0.1229829490184784\n",
      "batch: 6459, loss: 0.12944620847702026\n",
      "batch: 6460, loss: 0.11630383133888245\n",
      "batch: 6461, loss: 0.24679893255233765\n",
      "batch: 6462, loss: 0.14414545893669128\n",
      "batch: 6463, loss: 0.0870649591088295\n",
      "batch: 6464, loss: 0.16171787679195404\n",
      "batch: 6465, loss: 0.11450271308422089\n",
      "batch: 6466, loss: 0.2042067050933838\n",
      "batch: 6467, loss: 0.1811983436346054\n",
      "batch: 6468, loss: 0.2654552161693573\n",
      "batch: 6469, loss: 0.1753242462873459\n",
      "batch: 6470, loss: 0.19573615491390228\n",
      "batch: 6471, loss: 0.1813291758298874\n",
      "batch: 6472, loss: 0.20601922273635864\n",
      "batch: 6473, loss: 0.1539905071258545\n",
      "batch: 6474, loss: 0.24484001100063324\n",
      "batch: 6475, loss: 0.11436055600643158\n",
      "batch: 6476, loss: 0.17821955680847168\n",
      "batch: 6477, loss: 0.12962377071380615\n",
      "batch: 6478, loss: 0.17278797924518585\n",
      "batch: 6479, loss: 0.21649391949176788\n",
      "batch: 6480, loss: 0.19609498977661133\n",
      "batch: 6481, loss: 0.10344840586185455\n",
      "batch: 6482, loss: 0.1297554075717926\n",
      "batch: 6483, loss: 0.21108032763004303\n",
      "batch: 6484, loss: 0.12331703305244446\n",
      "batch: 6485, loss: 0.2677280902862549\n",
      "batch: 6486, loss: 0.1733608841896057\n",
      "batch: 6487, loss: 0.28985732793807983\n",
      "batch: 6488, loss: 0.223942369222641\n",
      "batch: 6489, loss: 0.16900861263275146\n",
      "batch: 6490, loss: 0.13400641083717346\n",
      "batch: 6491, loss: 0.1971713900566101\n",
      "batch: 6492, loss: 0.13811299204826355\n",
      "batch: 6493, loss: 0.17235830426216125\n",
      "batch: 6494, loss: 0.10881362110376358\n",
      "batch: 6495, loss: 0.11815778911113739\n",
      "batch: 6496, loss: 0.14363688230514526\n",
      "batch: 6497, loss: 0.27120620012283325\n",
      "batch: 6498, loss: 0.09916473925113678\n",
      "batch: 6499, loss: 0.17853373289108276\n",
      "batch: 6500, loss: 0.07096826285123825\n",
      "model saved to ./saving/model.ckpt-65\n",
      "batch: 6501, loss: 0.25065985321998596\n",
      "batch: 6502, loss: 0.12547431886196136\n",
      "batch: 6503, loss: 0.06124728173017502\n",
      "batch: 6504, loss: 0.20418044924736023\n",
      "batch: 6505, loss: 0.1632240116596222\n",
      "batch: 6506, loss: 0.0928964614868164\n",
      "batch: 6507, loss: 0.19865310192108154\n",
      "batch: 6508, loss: 0.08443934470415115\n",
      "batch: 6509, loss: 0.22210848331451416\n",
      "batch: 6510, loss: 0.16109415888786316\n",
      "batch: 6511, loss: 0.1598316729068756\n",
      "batch: 6512, loss: 0.4533463716506958\n",
      "batch: 6513, loss: 0.13911229372024536\n",
      "batch: 6514, loss: 0.20351654291152954\n",
      "batch: 6515, loss: 0.2597338855266571\n",
      "batch: 6516, loss: 0.12413634359836578\n",
      "batch: 6517, loss: 0.1967831254005432\n",
      "batch: 6518, loss: 0.24145038425922394\n",
      "batch: 6519, loss: 0.19517742097377777\n",
      "batch: 6520, loss: 0.0903773307800293\n",
      "batch: 6521, loss: 0.2509410083293915\n",
      "batch: 6522, loss: 0.2296193689107895\n",
      "batch: 6523, loss: 0.20907491445541382\n",
      "batch: 6524, loss: 0.07895262539386749\n",
      "batch: 6525, loss: 0.11469209939241409\n",
      "batch: 6526, loss: 0.16861052811145782\n",
      "batch: 6527, loss: 0.09391544759273529\n",
      "batch: 6528, loss: 0.10170410573482513\n",
      "batch: 6529, loss: 0.14117446541786194\n",
      "batch: 6530, loss: 0.3113020062446594\n",
      "batch: 6531, loss: 0.21523094177246094\n",
      "batch: 6532, loss: 0.25196927785873413\n",
      "batch: 6533, loss: 0.16945019364356995\n",
      "batch: 6534, loss: 0.1338595449924469\n",
      "batch: 6535, loss: 0.26001715660095215\n",
      "batch: 6536, loss: 0.08737879991531372\n",
      "batch: 6537, loss: 0.09216950833797455\n",
      "batch: 6538, loss: 0.40304750204086304\n",
      "batch: 6539, loss: 0.15867270529270172\n",
      "batch: 6540, loss: 0.23614123463630676\n",
      "batch: 6541, loss: 0.2856031656265259\n",
      "batch: 6542, loss: 0.13939756155014038\n",
      "batch: 6543, loss: 0.16714634001255035\n",
      "batch: 6544, loss: 0.11058591306209564\n",
      "batch: 6545, loss: 0.23511797189712524\n",
      "batch: 6546, loss: 0.18096712231636047\n",
      "batch: 6547, loss: 0.177004873752594\n",
      "batch: 6548, loss: 0.41894471645355225\n",
      "batch: 6549, loss: 0.18129000067710876\n",
      "batch: 6550, loss: 0.48084449768066406\n",
      "batch: 6551, loss: 0.2921115458011627\n",
      "batch: 6552, loss: 0.0822923481464386\n",
      "batch: 6553, loss: 0.3142629861831665\n",
      "batch: 6554, loss: 0.23272131383419037\n",
      "batch: 6555, loss: 0.3016408085823059\n",
      "batch: 6556, loss: 0.23910780251026154\n",
      "batch: 6557, loss: 0.12878811359405518\n",
      "batch: 6558, loss: 0.10045112669467926\n",
      "batch: 6559, loss: 0.13608437776565552\n",
      "batch: 6560, loss: 0.23858535289764404\n",
      "batch: 6561, loss: 0.23186972737312317\n",
      "batch: 6562, loss: 0.24132785201072693\n",
      "batch: 6563, loss: 0.18169935047626495\n",
      "batch: 6564, loss: 0.09101956337690353\n",
      "batch: 6565, loss: 0.18216633796691895\n",
      "batch: 6566, loss: 0.17019881308078766\n",
      "batch: 6567, loss: 0.193663090467453\n",
      "batch: 6568, loss: 0.1409892439842224\n",
      "batch: 6569, loss: 0.12681835889816284\n",
      "batch: 6570, loss: 0.1379324197769165\n",
      "batch: 6571, loss: 0.1524827927350998\n",
      "batch: 6572, loss: 0.24049201607704163\n",
      "batch: 6573, loss: 0.21583354473114014\n",
      "batch: 6574, loss: 0.18760620057582855\n",
      "batch: 6575, loss: 0.20745539665222168\n",
      "batch: 6576, loss: 0.15558874607086182\n",
      "batch: 6577, loss: 0.2595536708831787\n",
      "batch: 6578, loss: 0.25025635957717896\n",
      "batch: 6579, loss: 0.13506650924682617\n",
      "batch: 6580, loss: 0.17263825237751007\n",
      "batch: 6581, loss: 0.13770291209220886\n",
      "batch: 6582, loss: 0.21641771495342255\n",
      "batch: 6583, loss: 0.15850037336349487\n",
      "batch: 6584, loss: 0.1290319263935089\n",
      "batch: 6585, loss: 0.19688856601715088\n",
      "batch: 6586, loss: 0.13015460968017578\n",
      "batch: 6587, loss: 0.2091389298439026\n",
      "batch: 6588, loss: 0.1440235674381256\n",
      "batch: 6589, loss: 0.19282525777816772\n",
      "batch: 6590, loss: 0.08876786381006241\n",
      "batch: 6591, loss: 0.2099926620721817\n",
      "batch: 6592, loss: 0.22586478292942047\n",
      "batch: 6593, loss: 0.1966801881790161\n",
      "batch: 6594, loss: 0.08456863462924957\n",
      "batch: 6595, loss: 0.12155241519212723\n",
      "batch: 6596, loss: 0.16697964072227478\n",
      "batch: 6597, loss: 0.21368750929832458\n",
      "batch: 6598, loss: 0.19748684763908386\n",
      "batch: 6599, loss: 0.20354074239730835\n",
      "batch: 6600, loss: 0.07515999674797058\n",
      "model saved to ./saving/model.ckpt-66\n",
      "batch: 6601, loss: 0.2177734673023224\n",
      "batch: 6602, loss: 0.185691237449646\n",
      "batch: 6603, loss: 0.19281962513923645\n",
      "batch: 6604, loss: 0.21293765306472778\n",
      "batch: 6605, loss: 0.4451494812965393\n",
      "batch: 6606, loss: 0.12784124910831451\n",
      "batch: 6607, loss: 0.16880032420158386\n",
      "batch: 6608, loss: 0.2301633208990097\n",
      "batch: 6609, loss: 0.08596587926149368\n",
      "batch: 6610, loss: 0.17893174290657043\n",
      "batch: 6611, loss: 0.21205849945545197\n",
      "batch: 6612, loss: 0.06445551663637161\n",
      "batch: 6613, loss: 0.11720505356788635\n",
      "batch: 6614, loss: 0.2400490790605545\n",
      "batch: 6615, loss: 0.07941144704818726\n",
      "batch: 6616, loss: 0.1302768886089325\n",
      "batch: 6617, loss: 0.14441058039665222\n",
      "batch: 6618, loss: 0.13810807466506958\n",
      "batch: 6619, loss: 0.18106529116630554\n",
      "batch: 6620, loss: 0.18687374889850616\n",
      "batch: 6621, loss: 0.19903182983398438\n",
      "batch: 6622, loss: 0.17807820439338684\n",
      "batch: 6623, loss: 0.3735075294971466\n",
      "batch: 6624, loss: 0.1225421130657196\n",
      "batch: 6625, loss: 0.09355372190475464\n",
      "batch: 6626, loss: 0.2006237506866455\n",
      "batch: 6627, loss: 0.14464402198791504\n",
      "batch: 6628, loss: 0.2021886706352234\n",
      "batch: 6629, loss: 0.11727553606033325\n",
      "batch: 6630, loss: 0.075865738093853\n",
      "batch: 6631, loss: 0.20527756214141846\n",
      "batch: 6632, loss: 0.10243969410657883\n",
      "batch: 6633, loss: 0.12574023008346558\n",
      "batch: 6634, loss: 0.07397250831127167\n",
      "batch: 6635, loss: 0.0935901403427124\n",
      "batch: 6636, loss: 0.10300734639167786\n",
      "batch: 6637, loss: 0.2553492486476898\n",
      "batch: 6638, loss: 0.3759782314300537\n",
      "batch: 6639, loss: 0.061415497213602066\n",
      "batch: 6640, loss: 0.06924645602703094\n",
      "batch: 6641, loss: 0.20741090178489685\n",
      "batch: 6642, loss: 0.23421189188957214\n",
      "batch: 6643, loss: 0.19873139262199402\n",
      "batch: 6644, loss: 0.3030908703804016\n",
      "batch: 6645, loss: 0.20847925543785095\n",
      "batch: 6646, loss: 0.18660953640937805\n",
      "batch: 6647, loss: 0.16198590397834778\n",
      "batch: 6648, loss: 0.11160562932491302\n",
      "batch: 6649, loss: 0.13043877482414246\n",
      "batch: 6650, loss: 0.26677843928337097\n",
      "batch: 6651, loss: 0.08233626931905746\n",
      "batch: 6652, loss: 0.19649261236190796\n",
      "batch: 6653, loss: 0.10017335414886475\n",
      "batch: 6654, loss: 0.1683787703514099\n",
      "batch: 6655, loss: 0.14380332827568054\n",
      "batch: 6656, loss: 0.08846858143806458\n",
      "batch: 6657, loss: 0.13111981749534607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 6658, loss: 0.187040776014328\n",
      "batch: 6659, loss: 0.35278433561325073\n",
      "batch: 6660, loss: 0.12003496289253235\n",
      "batch: 6661, loss: 0.28033167123794556\n",
      "batch: 6662, loss: 0.2093120962381363\n",
      "batch: 6663, loss: 0.10527317225933075\n",
      "batch: 6664, loss: 0.15134306252002716\n",
      "batch: 6665, loss: 0.19586890935897827\n",
      "batch: 6666, loss: 0.2208128422498703\n",
      "batch: 6667, loss: 0.26026827096939087\n",
      "batch: 6668, loss: 0.14599624276161194\n",
      "batch: 6669, loss: 0.2770503759384155\n",
      "batch: 6670, loss: 0.09580853581428528\n",
      "batch: 6671, loss: 0.12709063291549683\n",
      "batch: 6672, loss: 0.157600998878479\n",
      "batch: 6673, loss: 0.15059661865234375\n",
      "batch: 6674, loss: 0.3040929436683655\n",
      "batch: 6675, loss: 0.12569595873355865\n",
      "batch: 6676, loss: 0.06512115150690079\n",
      "batch: 6677, loss: 0.3477945923805237\n",
      "batch: 6678, loss: 0.16974826157093048\n",
      "batch: 6679, loss: 0.19123616814613342\n",
      "batch: 6680, loss: 0.12948650121688843\n",
      "batch: 6681, loss: 0.09583629667758942\n",
      "batch: 6682, loss: 0.07050642371177673\n",
      "batch: 6683, loss: 0.11607897281646729\n",
      "batch: 6684, loss: 0.17791640758514404\n",
      "batch: 6685, loss: 0.09487806260585785\n",
      "batch: 6686, loss: 0.15472309291362762\n",
      "batch: 6687, loss: 0.17558059096336365\n",
      "batch: 6688, loss: 0.1765517145395279\n",
      "batch: 6689, loss: 0.1631859540939331\n",
      "batch: 6690, loss: 0.21478493511676788\n",
      "batch: 6691, loss: 0.1783694475889206\n",
      "batch: 6692, loss: 0.15634068846702576\n",
      "batch: 6693, loss: 0.13048814237117767\n",
      "batch: 6694, loss: 0.08646602928638458\n",
      "batch: 6695, loss: 0.12202511727809906\n",
      "batch: 6696, loss: 0.12981243431568146\n",
      "batch: 6697, loss: 0.16621609032154083\n",
      "batch: 6698, loss: 0.20977118611335754\n",
      "batch: 6699, loss: 0.1396489143371582\n",
      "batch: 6700, loss: 0.30880582332611084\n",
      "model saved to ./saving/model.ckpt-67\n",
      "batch: 6701, loss: 0.2159000039100647\n",
      "batch: 6702, loss: 0.11301246285438538\n",
      "batch: 6703, loss: 0.25330662727355957\n",
      "batch: 6704, loss: 0.10825767368078232\n",
      "batch: 6705, loss: 0.27707958221435547\n",
      "batch: 6706, loss: 0.08011862635612488\n",
      "batch: 6707, loss: 0.15645216405391693\n",
      "batch: 6708, loss: 0.15022927522659302\n",
      "batch: 6709, loss: 0.4224180579185486\n",
      "batch: 6710, loss: 0.1439588963985443\n",
      "batch: 6711, loss: 0.21306681632995605\n",
      "batch: 6712, loss: 0.176859512925148\n",
      "batch: 6713, loss: 0.13963957130908966\n",
      "batch: 6714, loss: 0.11058884859085083\n",
      "batch: 6715, loss: 0.20493528246879578\n",
      "batch: 6716, loss: 0.138965904712677\n",
      "batch: 6717, loss: 0.19461166858673096\n",
      "batch: 6718, loss: 0.12142691016197205\n",
      "batch: 6719, loss: 0.17352722585201263\n",
      "batch: 6720, loss: 0.17570285499095917\n",
      "batch: 6721, loss: 0.21007660031318665\n",
      "batch: 6722, loss: 0.2076396644115448\n",
      "batch: 6723, loss: 0.211607426404953\n",
      "batch: 6724, loss: 0.2839808166027069\n",
      "batch: 6725, loss: 0.25985294580459595\n",
      "batch: 6726, loss: 0.1624142825603485\n",
      "batch: 6727, loss: 0.09569589048624039\n",
      "batch: 6728, loss: 0.12544021010398865\n",
      "batch: 6729, loss: 0.22785788774490356\n",
      "batch: 6730, loss: 0.2606264352798462\n",
      "batch: 6731, loss: 0.195240780711174\n",
      "batch: 6732, loss: 0.22383230924606323\n",
      "batch: 6733, loss: 0.22136026620864868\n",
      "batch: 6734, loss: 0.14560922980308533\n",
      "batch: 6735, loss: 0.20615705847740173\n",
      "batch: 6736, loss: 0.3355552554130554\n",
      "batch: 6737, loss: 0.1639782339334488\n",
      "batch: 6738, loss: 0.13725866377353668\n",
      "batch: 6739, loss: 0.11218893527984619\n",
      "batch: 6740, loss: 0.06391975283622742\n",
      "batch: 6741, loss: 0.4083411991596222\n",
      "batch: 6742, loss: 0.12126468867063522\n",
      "batch: 6743, loss: 0.13515082001686096\n",
      "batch: 6744, loss: 0.24238687753677368\n",
      "batch: 6745, loss: 0.16642680764198303\n",
      "batch: 6746, loss: 0.2792963683605194\n",
      "batch: 6747, loss: 0.20476481318473816\n",
      "batch: 6748, loss: 0.24765871465206146\n",
      "batch: 6749, loss: 0.23687028884887695\n",
      "batch: 6750, loss: 0.23329828679561615\n",
      "batch: 6751, loss: 0.15609920024871826\n",
      "batch: 6752, loss: 0.19045543670654297\n",
      "batch: 6753, loss: 0.19739821553230286\n",
      "batch: 6754, loss: 0.14988388121128082\n",
      "batch: 6755, loss: 0.16425999999046326\n",
      "batch: 6756, loss: 0.22964763641357422\n",
      "batch: 6757, loss: 0.12599948048591614\n",
      "batch: 6758, loss: 0.19890457391738892\n",
      "batch: 6759, loss: 0.07811610400676727\n",
      "batch: 6760, loss: 0.06257884204387665\n",
      "batch: 6761, loss: 0.25574737787246704\n",
      "batch: 6762, loss: 0.1506650149822235\n",
      "batch: 6763, loss: 0.32755979895591736\n",
      "batch: 6764, loss: 0.3001651465892792\n",
      "batch: 6765, loss: 0.12983417510986328\n",
      "batch: 6766, loss: 0.1830560714006424\n",
      "batch: 6767, loss: 0.060408055782318115\n",
      "batch: 6768, loss: 0.32597020268440247\n",
      "batch: 6769, loss: 0.09109590947628021\n",
      "batch: 6770, loss: 0.20278160274028778\n",
      "batch: 6771, loss: 0.10884173214435577\n",
      "batch: 6772, loss: 0.4494597017765045\n",
      "batch: 6773, loss: 0.1656205952167511\n",
      "batch: 6774, loss: 0.13899065554141998\n",
      "batch: 6775, loss: 0.22053071856498718\n",
      "batch: 6776, loss: 0.3668323755264282\n",
      "batch: 6777, loss: 0.234215646982193\n",
      "batch: 6778, loss: 0.2338695228099823\n",
      "batch: 6779, loss: 0.29778847098350525\n",
      "batch: 6780, loss: 0.22255340218544006\n",
      "batch: 6781, loss: 0.1882043182849884\n",
      "batch: 6782, loss: 0.13681712746620178\n",
      "batch: 6783, loss: 0.13546143472194672\n",
      "batch: 6784, loss: 0.09717656672000885\n",
      "batch: 6785, loss: 0.17265862226486206\n",
      "batch: 6786, loss: 0.15839992463588715\n",
      "batch: 6787, loss: 0.07960940897464752\n",
      "batch: 6788, loss: 0.1367177516222\n",
      "batch: 6789, loss: 0.07188227772712708\n",
      "batch: 6790, loss: 0.21563223004341125\n",
      "batch: 6791, loss: 0.12333524972200394\n",
      "batch: 6792, loss: 0.31453055143356323\n",
      "batch: 6793, loss: 0.2199108898639679\n",
      "batch: 6794, loss: 0.17054158449172974\n",
      "batch: 6795, loss: 0.040557220578193665\n",
      "batch: 6796, loss: 0.24574947357177734\n",
      "batch: 6797, loss: 0.20071034133434296\n",
      "batch: 6798, loss: 0.12332703173160553\n",
      "batch: 6799, loss: 0.2517377436161041\n",
      "batch: 6800, loss: 0.12862730026245117\n",
      "model saved to ./saving/model.ckpt-68\n",
      "batch: 6801, loss: 0.1197376623749733\n",
      "batch: 6802, loss: 0.19326353073120117\n",
      "batch: 6803, loss: 0.0663016140460968\n",
      "batch: 6804, loss: 0.1180141419172287\n",
      "batch: 6805, loss: 0.1535462886095047\n",
      "batch: 6806, loss: 0.10130222141742706\n",
      "batch: 6807, loss: 0.2631768584251404\n",
      "batch: 6808, loss: 0.22711902856826782\n",
      "batch: 6809, loss: 0.2165582925081253\n",
      "batch: 6810, loss: 0.18537193536758423\n",
      "batch: 6811, loss: 0.2400280088186264\n",
      "batch: 6812, loss: 0.08503152430057526\n",
      "batch: 6813, loss: 0.13682785630226135\n",
      "batch: 6814, loss: 0.1577424854040146\n",
      "batch: 6815, loss: 0.28946083784103394\n",
      "batch: 6816, loss: 0.1600392460823059\n",
      "batch: 6817, loss: 0.12925323843955994\n",
      "batch: 6818, loss: 0.17843441665172577\n",
      "batch: 6819, loss: 0.12259439378976822\n",
      "batch: 6820, loss: 0.1084480732679367\n",
      "batch: 6821, loss: 0.16528809070587158\n",
      "batch: 6822, loss: 0.17516463994979858\n",
      "batch: 6823, loss: 0.14724108576774597\n",
      "batch: 6824, loss: 0.20771415531635284\n",
      "batch: 6825, loss: 0.11271817237138748\n",
      "batch: 6826, loss: 0.135809987783432\n",
      "batch: 6827, loss: 0.1520536243915558\n",
      "batch: 6828, loss: 0.41579750180244446\n",
      "batch: 6829, loss: 0.24660669267177582\n",
      "batch: 6830, loss: 0.062026701867580414\n",
      "batch: 6831, loss: 0.05940640717744827\n",
      "batch: 6832, loss: 0.23567557334899902\n",
      "batch: 6833, loss: 0.20433232188224792\n",
      "batch: 6834, loss: 0.22751560807228088\n",
      "batch: 6835, loss: 0.1205589771270752\n",
      "batch: 6836, loss: 0.2690795660018921\n",
      "batch: 6837, loss: 0.1041620671749115\n",
      "batch: 6838, loss: 0.352001816034317\n",
      "batch: 6839, loss: 0.18657928705215454\n",
      "batch: 6840, loss: 0.16698631644248962\n",
      "batch: 6841, loss: 0.25048381090164185\n",
      "batch: 6842, loss: 0.22242817282676697\n",
      "batch: 6843, loss: 0.1093483492732048\n",
      "batch: 6844, loss: 0.15734045207500458\n",
      "batch: 6845, loss: 0.11771830171346664\n",
      "batch: 6846, loss: 0.20816726982593536\n",
      "batch: 6847, loss: 0.10872318595647812\n",
      "batch: 6848, loss: 0.1794755607843399\n",
      "batch: 6849, loss: 0.2017950713634491\n",
      "batch: 6850, loss: 0.15342125296592712\n",
      "batch: 6851, loss: 0.38556358218193054\n",
      "batch: 6852, loss: 0.15025193989276886\n",
      "batch: 6853, loss: 0.42498135566711426\n",
      "batch: 6854, loss: 0.10284824669361115\n",
      "batch: 6855, loss: 0.16611415147781372\n",
      "batch: 6856, loss: 0.09672022610902786\n",
      "batch: 6857, loss: 0.32455235719680786\n",
      "batch: 6858, loss: 0.1742369830608368\n",
      "batch: 6859, loss: 0.15788647532463074\n",
      "batch: 6860, loss: 0.1669343113899231\n",
      "batch: 6861, loss: 0.1331542730331421\n",
      "batch: 6862, loss: 0.37482890486717224\n",
      "batch: 6863, loss: 0.23304259777069092\n",
      "batch: 6864, loss: 0.35369473695755005\n",
      "batch: 6865, loss: 0.17740362882614136\n",
      "batch: 6866, loss: 0.1523997187614441\n",
      "batch: 6867, loss: 0.14063863456249237\n",
      "batch: 6868, loss: 0.169307142496109\n",
      "batch: 6869, loss: 0.15036797523498535\n",
      "batch: 6870, loss: 0.3753806948661804\n",
      "batch: 6871, loss: 0.15133020281791687\n",
      "batch: 6872, loss: 0.2741665542125702\n",
      "batch: 6873, loss: 0.16779205203056335\n",
      "batch: 6874, loss: 0.2157108187675476\n",
      "batch: 6875, loss: 0.1709892749786377\n",
      "batch: 6876, loss: 0.11896534264087677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 6877, loss: 0.22147831320762634\n",
      "batch: 6878, loss: 0.13868805766105652\n",
      "batch: 6879, loss: 0.1038835421204567\n",
      "batch: 6880, loss: 0.1146388053894043\n",
      "batch: 6881, loss: 0.18983161449432373\n",
      "batch: 6882, loss: 0.16657210886478424\n",
      "batch: 6883, loss: 0.14192068576812744\n",
      "batch: 6884, loss: 0.1638469696044922\n",
      "batch: 6885, loss: 0.22747766971588135\n",
      "batch: 6886, loss: 0.13750392198562622\n",
      "batch: 6887, loss: 0.1541523039340973\n",
      "batch: 6888, loss: 0.2405555546283722\n",
      "batch: 6889, loss: 0.19484874606132507\n",
      "batch: 6890, loss: 0.06930705904960632\n",
      "batch: 6891, loss: 0.12312868982553482\n",
      "batch: 6892, loss: 0.16365443170070648\n",
      "batch: 6893, loss: 0.07323913276195526\n",
      "batch: 6894, loss: 0.21350273489952087\n",
      "batch: 6895, loss: 0.12115113437175751\n",
      "batch: 6896, loss: 0.199200838804245\n",
      "batch: 6897, loss: 0.06979503482580185\n",
      "batch: 6898, loss: 0.16899926960468292\n",
      "batch: 6899, loss: 0.1469511091709137\n",
      "batch: 6900, loss: 0.24346128106117249\n",
      "model saved to ./saving/model.ckpt-69\n",
      "batch: 6901, loss: 0.1076139360666275\n",
      "batch: 6902, loss: 0.16252651810646057\n",
      "batch: 6903, loss: 0.21650497615337372\n",
      "batch: 6904, loss: 0.2152383029460907\n",
      "batch: 6905, loss: 0.1701204478740692\n",
      "batch: 6906, loss: 0.1555185317993164\n",
      "batch: 6907, loss: 0.11154477298259735\n",
      "batch: 6908, loss: 0.17705270648002625\n",
      "batch: 6909, loss: 0.09265556931495667\n",
      "batch: 6910, loss: 0.16601908206939697\n",
      "batch: 6911, loss: 0.188067227602005\n",
      "batch: 6912, loss: 0.11533092707395554\n",
      "batch: 6913, loss: 0.23172982037067413\n",
      "batch: 6914, loss: 0.26017796993255615\n",
      "batch: 6915, loss: 0.29025542736053467\n",
      "batch: 6916, loss: 0.12190240621566772\n",
      "batch: 6917, loss: 0.30530986189842224\n",
      "batch: 6918, loss: 0.2588416337966919\n",
      "batch: 6919, loss: 0.2900569438934326\n",
      "batch: 6920, loss: 0.15064755082130432\n",
      "batch: 6921, loss: 0.25474947690963745\n",
      "batch: 6922, loss: 0.20911261439323425\n",
      "batch: 6923, loss: 0.21418729424476624\n",
      "batch: 6924, loss: 0.29690712690353394\n",
      "batch: 6925, loss: 0.09586134552955627\n",
      "batch: 6926, loss: 0.22449719905853271\n",
      "batch: 6927, loss: 0.119495689868927\n",
      "batch: 6928, loss: 0.06809421628713608\n",
      "batch: 6929, loss: 0.20377959311008453\n",
      "batch: 6930, loss: 0.1919831931591034\n",
      "batch: 6931, loss: 0.11435356736183167\n",
      "batch: 6932, loss: 0.09457799792289734\n",
      "batch: 6933, loss: 0.1664368063211441\n",
      "batch: 6934, loss: 0.15484794974327087\n",
      "batch: 6935, loss: 0.18664845824241638\n",
      "batch: 6936, loss: 0.13541992008686066\n",
      "batch: 6937, loss: 0.09031252562999725\n",
      "batch: 6938, loss: 0.12008562684059143\n",
      "batch: 6939, loss: 0.2873935401439667\n",
      "batch: 6940, loss: 0.23415595293045044\n",
      "batch: 6941, loss: 0.1344163417816162\n",
      "batch: 6942, loss: 0.15013527870178223\n",
      "batch: 6943, loss: 0.18694442510604858\n",
      "batch: 6944, loss: 0.15793442726135254\n",
      "batch: 6945, loss: 0.0817481055855751\n",
      "batch: 6946, loss: 0.1351293921470642\n",
      "batch: 6947, loss: 0.06300733983516693\n",
      "batch: 6948, loss: 0.2313118875026703\n",
      "batch: 6949, loss: 0.339635968208313\n",
      "batch: 6950, loss: 0.23922225832939148\n",
      "batch: 6951, loss: 0.1381101757287979\n",
      "batch: 6952, loss: 0.14766457676887512\n",
      "batch: 6953, loss: 0.1676836609840393\n",
      "batch: 6954, loss: 0.25790953636169434\n",
      "batch: 6955, loss: 0.19964531064033508\n",
      "batch: 6956, loss: 0.0857933834195137\n",
      "batch: 6957, loss: 0.1749381721019745\n",
      "batch: 6958, loss: 0.11643116921186447\n",
      "batch: 6959, loss: 0.11151836067438126\n",
      "batch: 6960, loss: 0.1609368473291397\n",
      "batch: 6961, loss: 0.13332365453243256\n",
      "batch: 6962, loss: 0.24082769453525543\n",
      "batch: 6963, loss: 0.19795191287994385\n",
      "batch: 6964, loss: 0.1012553721666336\n",
      "batch: 6965, loss: 0.24066174030303955\n",
      "batch: 6966, loss: 0.17812804877758026\n",
      "batch: 6967, loss: 0.3650188148021698\n",
      "batch: 6968, loss: 0.12545597553253174\n",
      "batch: 6969, loss: 0.12980517745018005\n",
      "batch: 6970, loss: 0.1307716965675354\n",
      "batch: 6971, loss: 0.08841690421104431\n",
      "batch: 6972, loss: 0.1832861602306366\n",
      "batch: 6973, loss: 0.1232185885310173\n",
      "batch: 6974, loss: 0.1333095133304596\n",
      "batch: 6975, loss: 0.3519302010536194\n",
      "batch: 6976, loss: 0.11052340269088745\n",
      "batch: 6977, loss: 0.2110215127468109\n",
      "batch: 6978, loss: 0.19081728160381317\n",
      "batch: 6979, loss: 0.08127617090940475\n",
      "batch: 6980, loss: 0.20111536979675293\n",
      "batch: 6981, loss: 0.22508782148361206\n",
      "batch: 6982, loss: 0.1109132468700409\n",
      "batch: 6983, loss: 0.18517577648162842\n",
      "batch: 6984, loss: 0.1752605438232422\n",
      "batch: 6985, loss: 0.12521204352378845\n",
      "batch: 6986, loss: 0.167010098695755\n",
      "batch: 6987, loss: 0.1283184438943863\n",
      "batch: 6988, loss: 0.10966092348098755\n",
      "batch: 6989, loss: 0.13464397192001343\n",
      "batch: 6990, loss: 0.27060258388519287\n",
      "batch: 6991, loss: 0.1630045473575592\n",
      "batch: 6992, loss: 0.1855546534061432\n",
      "batch: 6993, loss: 0.1461789906024933\n",
      "batch: 6994, loss: 0.17489001154899597\n",
      "batch: 6995, loss: 0.17976787686347961\n",
      "batch: 6996, loss: 0.19251884520053864\n",
      "batch: 6997, loss: 0.06696458160877228\n",
      "batch: 6998, loss: 0.22209349274635315\n",
      "batch: 6999, loss: 0.1313548982143402\n",
      "batch: 7000, loss: 0.20434661209583282\n",
      "model saved to ./saving/model.ckpt-70\n",
      "batch: 7001, loss: 0.310561865568161\n",
      "batch: 7002, loss: 0.15735550224781036\n",
      "batch: 7003, loss: 0.2920554578304291\n",
      "batch: 7004, loss: 0.13658510148525238\n",
      "batch: 7005, loss: 0.1175658106803894\n",
      "batch: 7006, loss: 0.1966206133365631\n",
      "batch: 7007, loss: 0.11155450344085693\n",
      "batch: 7008, loss: 0.13319756090641022\n",
      "batch: 7009, loss: 0.16635139286518097\n",
      "batch: 7010, loss: 0.1673891246318817\n",
      "batch: 7011, loss: 0.08297698199748993\n",
      "batch: 7012, loss: 0.020041417330503464\n",
      "batch: 7013, loss: 0.21361035108566284\n",
      "batch: 7014, loss: 0.28141409158706665\n",
      "batch: 7015, loss: 0.15218651294708252\n",
      "batch: 7016, loss: 0.11870963126420975\n",
      "batch: 7017, loss: 0.20242300629615784\n",
      "batch: 7018, loss: 0.2437695562839508\n",
      "batch: 7019, loss: 0.1906174272298813\n",
      "batch: 7020, loss: 0.25597333908081055\n",
      "batch: 7021, loss: 0.16074219346046448\n",
      "batch: 7022, loss: 0.19783195853233337\n",
      "batch: 7023, loss: 0.24235831201076508\n",
      "batch: 7024, loss: 0.19173875451087952\n",
      "batch: 7025, loss: 0.225080206990242\n",
      "batch: 7026, loss: 0.36379149556159973\n",
      "batch: 7027, loss: 0.15440014004707336\n",
      "batch: 7028, loss: 0.1610022783279419\n",
      "batch: 7029, loss: 0.16135555505752563\n",
      "batch: 7030, loss: 0.16232097148895264\n",
      "batch: 7031, loss: 0.08835940062999725\n",
      "batch: 7032, loss: 0.29085880517959595\n",
      "batch: 7033, loss: 0.07880628854036331\n",
      "batch: 7034, loss: 0.22520214319229126\n",
      "batch: 7035, loss: 0.09509657323360443\n",
      "batch: 7036, loss: 0.08182348310947418\n",
      "batch: 7037, loss: 0.08914360404014587\n",
      "batch: 7038, loss: 0.1452372968196869\n",
      "batch: 7039, loss: 0.18603633344173431\n",
      "batch: 7040, loss: 0.12904292345046997\n",
      "batch: 7041, loss: 0.15659530460834503\n",
      "batch: 7042, loss: 0.24017423391342163\n",
      "batch: 7043, loss: 0.18714410066604614\n",
      "batch: 7044, loss: 0.09588375687599182\n",
      "batch: 7045, loss: 0.16682404279708862\n",
      "batch: 7046, loss: 0.1723412424325943\n",
      "batch: 7047, loss: 0.11356142163276672\n",
      "batch: 7048, loss: 0.1381426453590393\n",
      "batch: 7049, loss: 0.21976801753044128\n",
      "batch: 7050, loss: 0.26256534457206726\n",
      "batch: 7051, loss: 0.12745317816734314\n",
      "batch: 7052, loss: 0.11875875294208527\n",
      "batch: 7053, loss: 0.12001247704029083\n",
      "batch: 7054, loss: 0.19715680181980133\n",
      "batch: 7055, loss: 0.1783609539270401\n",
      "batch: 7056, loss: 0.07607704401016235\n",
      "batch: 7057, loss: 0.16292020678520203\n",
      "batch: 7058, loss: 0.2006794512271881\n",
      "batch: 7059, loss: 0.12039360404014587\n",
      "batch: 7060, loss: 0.20203319191932678\n",
      "batch: 7061, loss: 0.13368931412696838\n",
      "batch: 7062, loss: 0.10918746888637543\n",
      "batch: 7063, loss: 0.365276038646698\n",
      "batch: 7064, loss: 0.06981918215751648\n",
      "batch: 7065, loss: 0.16777458786964417\n",
      "batch: 7066, loss: 0.07055442035198212\n",
      "batch: 7067, loss: 0.29193997383117676\n",
      "batch: 7068, loss: 0.1818089783191681\n",
      "batch: 7069, loss: 0.3396991491317749\n",
      "batch: 7070, loss: 0.2481938898563385\n",
      "batch: 7071, loss: 0.14547953009605408\n",
      "batch: 7072, loss: 0.16715209186077118\n",
      "batch: 7073, loss: 0.17952589690685272\n",
      "batch: 7074, loss: 0.1281317174434662\n",
      "batch: 7075, loss: 0.09513881802558899\n",
      "batch: 7076, loss: 0.10006997734308243\n",
      "batch: 7077, loss: 0.4634053707122803\n",
      "batch: 7078, loss: 0.12472952902317047\n",
      "batch: 7079, loss: 0.20221194624900818\n",
      "batch: 7080, loss: 0.2919391691684723\n",
      "batch: 7081, loss: 0.25585973262786865\n",
      "batch: 7082, loss: 0.29471999406814575\n",
      "batch: 7083, loss: 0.1870528906583786\n",
      "batch: 7084, loss: 0.3548569977283478\n",
      "batch: 7085, loss: 0.3135376572608948\n",
      "batch: 7086, loss: 0.15980304777622223\n",
      "batch: 7087, loss: 0.17742669582366943\n",
      "batch: 7088, loss: 0.2079014629125595\n",
      "batch: 7089, loss: 0.23049481213092804\n",
      "batch: 7090, loss: 0.35608434677124023\n",
      "batch: 7091, loss: 0.1393910050392151\n",
      "batch: 7092, loss: 0.07865298539400101\n",
      "batch: 7093, loss: 0.11629684269428253\n",
      "batch: 7094, loss: 0.09688656032085419\n",
      "batch: 7095, loss: 0.28513646125793457\n",
      "batch: 7096, loss: 0.17727577686309814\n",
      "batch: 7097, loss: 0.1826297789812088\n",
      "batch: 7098, loss: 0.28228920698165894\n",
      "batch: 7099, loss: 0.2981824278831482\n",
      "batch: 7100, loss: 0.08339675515890121\n",
      "model saved to ./saving/model.ckpt-71\n",
      "batch: 7101, loss: 0.24267379939556122\n",
      "batch: 7102, loss: 0.22558939456939697\n",
      "batch: 7103, loss: 0.11771142482757568\n",
      "batch: 7104, loss: 0.082984060049057\n",
      "batch: 7105, loss: 0.16505947709083557\n",
      "batch: 7106, loss: 0.14062032103538513\n",
      "batch: 7107, loss: 0.1960555613040924\n",
      "batch: 7108, loss: 0.2497139871120453\n",
      "batch: 7109, loss: 0.10309271514415741\n",
      "batch: 7110, loss: 0.1322450339794159\n",
      "batch: 7111, loss: 0.2302832305431366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 7112, loss: 0.11978805065155029\n",
      "batch: 7113, loss: 0.15624262392520905\n",
      "batch: 7114, loss: 0.0670427605509758\n",
      "batch: 7115, loss: 0.3008160889148712\n",
      "batch: 7116, loss: 0.10305295139551163\n",
      "batch: 7117, loss: 0.21366551518440247\n",
      "batch: 7118, loss: 0.07834164798259735\n",
      "batch: 7119, loss: 0.30315858125686646\n",
      "batch: 7120, loss: 0.09719288349151611\n",
      "batch: 7121, loss: 0.15615616738796234\n",
      "batch: 7122, loss: 0.1044946014881134\n",
      "batch: 7123, loss: 0.2849917411804199\n",
      "batch: 7124, loss: 0.14707458019256592\n",
      "batch: 7125, loss: 0.11517970263957977\n",
      "batch: 7126, loss: 0.1731545627117157\n",
      "batch: 7127, loss: 0.43008896708488464\n",
      "batch: 7128, loss: 0.1716739982366562\n",
      "batch: 7129, loss: 0.09353461116552353\n",
      "batch: 7130, loss: 0.2064715325832367\n",
      "batch: 7131, loss: 0.2808058261871338\n",
      "batch: 7132, loss: 0.08772999048233032\n",
      "batch: 7133, loss: 0.09309344738721848\n",
      "batch: 7134, loss: 0.23488806188106537\n",
      "batch: 7135, loss: 0.11178043484687805\n",
      "batch: 7136, loss: 0.09190673381090164\n",
      "batch: 7137, loss: 0.10796757787466049\n",
      "batch: 7138, loss: 0.3066714107990265\n",
      "batch: 7139, loss: 0.07666239142417908\n",
      "batch: 7140, loss: 0.11939351260662079\n",
      "batch: 7141, loss: 0.17086704075336456\n",
      "batch: 7142, loss: 0.19661369919776917\n",
      "batch: 7143, loss: 0.11873258650302887\n",
      "batch: 7144, loss: 0.19086794555187225\n",
      "batch: 7145, loss: 0.08805614709854126\n",
      "batch: 7146, loss: 0.15099722146987915\n",
      "batch: 7147, loss: 0.1313953697681427\n",
      "batch: 7148, loss: 0.18450261652469635\n",
      "batch: 7149, loss: 0.12842071056365967\n",
      "batch: 7150, loss: 0.2851066291332245\n",
      "batch: 7151, loss: 0.32241615653038025\n",
      "batch: 7152, loss: 0.12712278962135315\n",
      "batch: 7153, loss: 0.23470531404018402\n",
      "batch: 7154, loss: 0.1544439196586609\n",
      "batch: 7155, loss: 0.264568954706192\n",
      "batch: 7156, loss: 0.1772802770137787\n",
      "batch: 7157, loss: 0.22228547930717468\n",
      "batch: 7158, loss: 0.18297038972377777\n",
      "batch: 7159, loss: 0.0998518168926239\n",
      "batch: 7160, loss: 0.21344000101089478\n",
      "batch: 7161, loss: 0.15205687284469604\n",
      "batch: 7162, loss: 0.3533158302307129\n",
      "batch: 7163, loss: 0.046085797250270844\n",
      "batch: 7164, loss: 0.13383568823337555\n",
      "batch: 7165, loss: 0.24527034163475037\n",
      "batch: 7166, loss: 0.12760205566883087\n",
      "batch: 7167, loss: 0.15868157148361206\n",
      "batch: 7168, loss: 0.08453025668859482\n",
      "batch: 7169, loss: 0.3590468168258667\n",
      "batch: 7170, loss: 0.18317043781280518\n",
      "batch: 7171, loss: 0.08537635207176208\n",
      "batch: 7172, loss: 0.2277657687664032\n",
      "batch: 7173, loss: 0.1754700392484665\n",
      "batch: 7174, loss: 0.17584693431854248\n",
      "batch: 7175, loss: 0.3511357307434082\n",
      "batch: 7176, loss: 0.24416561424732208\n",
      "batch: 7177, loss: 0.2730925977230072\n",
      "batch: 7178, loss: 0.3613210916519165\n",
      "batch: 7179, loss: 0.07559508085250854\n",
      "batch: 7180, loss: 0.20222337543964386\n",
      "batch: 7181, loss: 0.12015783786773682\n",
      "batch: 7182, loss: 0.1614762246608734\n",
      "batch: 7183, loss: 0.1690267026424408\n",
      "batch: 7184, loss: 0.14183282852172852\n",
      "batch: 7185, loss: 0.3185422420501709\n",
      "batch: 7186, loss: 0.10754570364952087\n",
      "batch: 7187, loss: 0.17097705602645874\n",
      "batch: 7188, loss: 0.19945119321346283\n",
      "batch: 7189, loss: 0.21471917629241943\n",
      "batch: 7190, loss: 0.18870064616203308\n",
      "batch: 7191, loss: 0.13037943840026855\n",
      "batch: 7192, loss: 0.14648929238319397\n",
      "batch: 7193, loss: 0.16400370001792908\n",
      "batch: 7194, loss: 0.06443208456039429\n",
      "batch: 7195, loss: 0.21248531341552734\n",
      "batch: 7196, loss: 0.20434041321277618\n",
      "batch: 7197, loss: 0.261036217212677\n",
      "batch: 7198, loss: 0.1424989402294159\n",
      "batch: 7199, loss: 0.1061057448387146\n",
      "batch: 7200, loss: 0.24667863547801971\n",
      "model saved to ./saving/model.ckpt-72\n",
      "batch: 7201, loss: 0.1950661838054657\n",
      "batch: 7202, loss: 0.10382022708654404\n",
      "batch: 7203, loss: 0.15425097942352295\n",
      "batch: 7204, loss: 0.11772525310516357\n",
      "batch: 7205, loss: 0.22395749390125275\n",
      "batch: 7206, loss: 0.18495061993598938\n",
      "batch: 7207, loss: 0.2959132790565491\n",
      "batch: 7208, loss: 0.15680980682373047\n",
      "batch: 7209, loss: 0.16115282475948334\n",
      "batch: 7210, loss: 0.10199649631977081\n",
      "batch: 7211, loss: 0.23539474606513977\n",
      "batch: 7212, loss: 0.17772167921066284\n",
      "batch: 7213, loss: 0.24666310846805573\n",
      "batch: 7214, loss: 0.07115225493907928\n",
      "batch: 7215, loss: 0.11962909251451492\n",
      "batch: 7216, loss: 0.11517474055290222\n",
      "batch: 7217, loss: 0.16104444861412048\n",
      "batch: 7218, loss: 0.11834545433521271\n",
      "batch: 7219, loss: 0.2350890040397644\n",
      "batch: 7220, loss: 0.14955760538578033\n",
      "batch: 7221, loss: 0.18051470816135406\n",
      "batch: 7222, loss: 0.09782005846500397\n",
      "batch: 7223, loss: 0.2267129272222519\n",
      "batch: 7224, loss: 0.1964598000049591\n",
      "batch: 7225, loss: 0.20627085864543915\n",
      "batch: 7226, loss: 0.24540379643440247\n",
      "batch: 7227, loss: 0.1411377489566803\n",
      "batch: 7228, loss: 0.2546912133693695\n",
      "batch: 7229, loss: 0.1779002845287323\n",
      "batch: 7230, loss: 0.31505879759788513\n",
      "batch: 7231, loss: 0.2556324899196625\n",
      "batch: 7232, loss: 0.18827351927757263\n",
      "batch: 7233, loss: 0.20958276093006134\n",
      "batch: 7234, loss: 0.3258178234100342\n",
      "batch: 7235, loss: 0.22490689158439636\n",
      "batch: 7236, loss: 0.14598551392555237\n",
      "batch: 7237, loss: 0.1853892207145691\n",
      "batch: 7238, loss: 0.24187861382961273\n",
      "batch: 7239, loss: 0.17632956802845\n",
      "batch: 7240, loss: 0.2331131249666214\n",
      "batch: 7241, loss: 0.279720664024353\n",
      "batch: 7242, loss: 0.09803512692451477\n",
      "batch: 7243, loss: 0.1860102415084839\n",
      "batch: 7244, loss: 0.11553555727005005\n",
      "batch: 7245, loss: 0.30836254358291626\n",
      "batch: 7246, loss: 0.06097620353102684\n",
      "batch: 7247, loss: 0.3637255132198334\n",
      "batch: 7248, loss: 0.21701648831367493\n",
      "batch: 7249, loss: 0.16046787798404694\n",
      "batch: 7250, loss: 0.11974474042654037\n",
      "batch: 7251, loss: 0.13104647397994995\n",
      "batch: 7252, loss: 0.2365148663520813\n",
      "batch: 7253, loss: 0.2636759281158447\n",
      "batch: 7254, loss: 0.2513452172279358\n",
      "batch: 7255, loss: 0.11111785471439362\n",
      "batch: 7256, loss: 0.23869569599628448\n",
      "batch: 7257, loss: 0.17604361474514008\n",
      "batch: 7258, loss: 0.16837333142757416\n",
      "batch: 7259, loss: 0.2075633406639099\n",
      "batch: 7260, loss: 0.12889914214611053\n",
      "batch: 7261, loss: 0.1153135746717453\n",
      "batch: 7262, loss: 0.10287226736545563\n",
      "batch: 7263, loss: 0.21069569885730743\n",
      "batch: 7264, loss: 0.13000881671905518\n",
      "batch: 7265, loss: 0.17110177874565125\n",
      "batch: 7266, loss: 0.1047869473695755\n",
      "batch: 7267, loss: 0.2763744294643402\n",
      "batch: 7268, loss: 0.12517599761486053\n",
      "batch: 7269, loss: 0.14365346729755402\n",
      "batch: 7270, loss: 0.2681540548801422\n",
      "batch: 7271, loss: 0.1985754817724228\n",
      "batch: 7272, loss: 0.21120846271514893\n",
      "batch: 7273, loss: 0.2098258137702942\n",
      "batch: 7274, loss: 0.24001634120941162\n",
      "batch: 7275, loss: 0.29517751932144165\n",
      "batch: 7276, loss: 0.25359418988227844\n",
      "batch: 7277, loss: 0.1247774213552475\n",
      "batch: 7278, loss: 0.16198329627513885\n",
      "batch: 7279, loss: 0.08042725175619125\n",
      "batch: 7280, loss: 0.14321067929267883\n",
      "batch: 7281, loss: 0.13685517013072968\n",
      "batch: 7282, loss: 0.10099391639232635\n",
      "batch: 7283, loss: 0.14807695150375366\n",
      "batch: 7284, loss: 0.20962341129779816\n",
      "batch: 7285, loss: 0.1941811740398407\n",
      "batch: 7286, loss: 0.1325223445892334\n",
      "batch: 7287, loss: 0.14063459634780884\n",
      "batch: 7288, loss: 0.1833241730928421\n",
      "batch: 7289, loss: 0.18944579362869263\n",
      "batch: 7290, loss: 0.2375422716140747\n",
      "batch: 7291, loss: 0.11512885987758636\n",
      "batch: 7292, loss: 0.3116087317466736\n",
      "batch: 7293, loss: 0.12924724817276\n",
      "batch: 7294, loss: 0.3186952471733093\n",
      "batch: 7295, loss: 0.12974238395690918\n",
      "batch: 7296, loss: 0.17701075971126556\n",
      "batch: 7297, loss: 0.14552666246891022\n",
      "batch: 7298, loss: 0.16593097150325775\n",
      "batch: 7299, loss: 0.11795715987682343\n",
      "batch: 7300, loss: 0.1195574551820755\n",
      "model saved to ./saving/model.ckpt-73\n",
      "batch: 7301, loss: 0.08607707172632217\n",
      "batch: 7302, loss: 0.25064361095428467\n",
      "batch: 7303, loss: 0.2057127058506012\n",
      "batch: 7304, loss: 0.09807682782411575\n",
      "batch: 7305, loss: 0.1519644409418106\n",
      "batch: 7306, loss: 0.08393391221761703\n",
      "batch: 7307, loss: 0.10810308158397675\n",
      "batch: 7308, loss: 0.12670812010765076\n",
      "batch: 7309, loss: 0.1691250056028366\n",
      "batch: 7310, loss: 0.29174724221229553\n",
      "batch: 7311, loss: 0.17033705115318298\n",
      "batch: 7312, loss: 0.22318612039089203\n",
      "batch: 7313, loss: 0.15296489000320435\n",
      "batch: 7314, loss: 0.11803658306598663\n",
      "batch: 7315, loss: 0.21482716500759125\n",
      "batch: 7316, loss: 0.0935937836766243\n",
      "batch: 7317, loss: 0.26275983452796936\n",
      "batch: 7318, loss: 0.21829214692115784\n",
      "batch: 7319, loss: 0.27982383966445923\n",
      "batch: 7320, loss: 0.0847184881567955\n",
      "batch: 7321, loss: 0.2576286196708679\n",
      "batch: 7322, loss: 0.20782163739204407\n",
      "batch: 7323, loss: 0.09161699563264847\n",
      "batch: 7324, loss: 0.22183342278003693\n",
      "batch: 7325, loss: 0.24361684918403625\n",
      "batch: 7326, loss: 0.098310187458992\n",
      "batch: 7327, loss: 0.11832204461097717\n",
      "batch: 7328, loss: 0.13857491314411163\n",
      "batch: 7329, loss: 0.16082888841629028\n",
      "batch: 7330, loss: 0.15326997637748718\n",
      "batch: 7331, loss: 0.2745532989501953\n",
      "batch: 7332, loss: 0.21113617718219757\n",
      "batch: 7333, loss: 0.29755112528800964\n",
      "batch: 7334, loss: 0.06465029716491699\n",
      "batch: 7335, loss: 0.18926292657852173\n",
      "batch: 7336, loss: 0.22001010179519653\n",
      "batch: 7337, loss: 0.3161764144897461\n",
      "batch: 7338, loss: 0.13322727382183075\n",
      "batch: 7339, loss: 0.1096639633178711\n",
      "batch: 7340, loss: 0.16733548045158386\n",
      "batch: 7341, loss: 0.12771464884281158\n",
      "batch: 7342, loss: 0.1340171843767166\n",
      "batch: 7343, loss: 0.21665194630622864\n",
      "batch: 7344, loss: 0.10467852652072906\n",
      "batch: 7345, loss: 0.17468875646591187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 7346, loss: 0.2820378839969635\n",
      "batch: 7347, loss: 0.3438502252101898\n",
      "batch: 7348, loss: 0.162034273147583\n",
      "batch: 7349, loss: 0.09435778856277466\n",
      "batch: 7350, loss: 0.09907582402229309\n",
      "batch: 7351, loss: 0.17656895518302917\n",
      "batch: 7352, loss: 0.22028803825378418\n",
      "batch: 7353, loss: 0.18087968230247498\n",
      "batch: 7354, loss: 0.06411148607730865\n",
      "batch: 7355, loss: 0.12513045966625214\n",
      "batch: 7356, loss: 0.19061537086963654\n",
      "batch: 7357, loss: 0.15786150097846985\n",
      "batch: 7358, loss: 0.08167761564254761\n",
      "batch: 7359, loss: 0.16197893023490906\n",
      "batch: 7360, loss: 0.25713759660720825\n",
      "batch: 7361, loss: 0.06577812880277634\n",
      "batch: 7362, loss: 0.17122308909893036\n",
      "batch: 7363, loss: 0.13951048254966736\n",
      "batch: 7364, loss: 0.12948909401893616\n",
      "batch: 7365, loss: 0.1405937820672989\n",
      "batch: 7366, loss: 0.1969958245754242\n",
      "batch: 7367, loss: 0.23495548963546753\n",
      "batch: 7368, loss: 0.1635264754295349\n",
      "batch: 7369, loss: 0.12205087393522263\n",
      "batch: 7370, loss: 0.15178777277469635\n",
      "batch: 7371, loss: 0.11737791448831558\n",
      "batch: 7372, loss: 0.11059673130512238\n",
      "batch: 7373, loss: 0.13997749984264374\n",
      "batch: 7374, loss: 0.1977650225162506\n",
      "batch: 7375, loss: 0.16103707253932953\n",
      "batch: 7376, loss: 0.25013086199760437\n",
      "batch: 7377, loss: 0.26336848735809326\n",
      "batch: 7378, loss: 0.1037430539727211\n",
      "batch: 7379, loss: 0.17959900200366974\n",
      "batch: 7380, loss: 0.19642585515975952\n",
      "batch: 7381, loss: 0.13067562878131866\n",
      "batch: 7382, loss: 0.11950398236513138\n",
      "batch: 7383, loss: 0.07544590532779694\n",
      "batch: 7384, loss: 0.1305077075958252\n",
      "batch: 7385, loss: 0.07867082953453064\n",
      "batch: 7386, loss: 0.18176253139972687\n",
      "batch: 7387, loss: 0.13333535194396973\n",
      "batch: 7388, loss: 0.1604374498128891\n",
      "batch: 7389, loss: 0.11598757654428482\n",
      "batch: 7390, loss: 0.1312670111656189\n",
      "batch: 7391, loss: 0.15038377046585083\n",
      "batch: 7392, loss: 0.10706260055303574\n",
      "batch: 7393, loss: 0.17363406717777252\n",
      "batch: 7394, loss: 0.22333070635795593\n",
      "batch: 7395, loss: 0.12792223691940308\n",
      "batch: 7396, loss: 0.07831671088933945\n",
      "batch: 7397, loss: 0.2136891484260559\n",
      "batch: 7398, loss: 0.09624160826206207\n",
      "batch: 7399, loss: 0.10126453638076782\n",
      "batch: 7400, loss: 0.17799463868141174\n",
      "model saved to ./saving/model.ckpt-74\n",
      "batch: 7401, loss: 0.26889747381210327\n",
      "batch: 7402, loss: 0.05221002176403999\n",
      "batch: 7403, loss: 0.19004321098327637\n",
      "batch: 7404, loss: 0.1476011574268341\n",
      "batch: 7405, loss: 0.04784803092479706\n",
      "batch: 7406, loss: 0.16909267008304596\n",
      "batch: 7407, loss: 0.20316195487976074\n",
      "batch: 7408, loss: 0.07245325297117233\n",
      "batch: 7409, loss: 0.1856275498867035\n",
      "batch: 7410, loss: 0.1012457013130188\n",
      "batch: 7411, loss: 0.20060868561267853\n",
      "batch: 7412, loss: 0.0981898084282875\n",
      "batch: 7413, loss: 0.2569572925567627\n",
      "batch: 7414, loss: 0.2040507048368454\n",
      "batch: 7415, loss: 0.07293900847434998\n",
      "batch: 7416, loss: 0.14363408088684082\n",
      "batch: 7417, loss: 0.30073675513267517\n",
      "batch: 7418, loss: 0.27186983823776245\n",
      "batch: 7419, loss: 0.20137955248355865\n",
      "batch: 7420, loss: 0.23727238178253174\n",
      "batch: 7421, loss: 0.0876162126660347\n",
      "batch: 7422, loss: 0.11856572329998016\n",
      "batch: 7423, loss: 0.23775549232959747\n",
      "batch: 7424, loss: 0.20753002166748047\n",
      "batch: 7425, loss: 0.14572757482528687\n",
      "batch: 7426, loss: 0.2527139484882355\n",
      "batch: 7427, loss: 0.10205879807472229\n",
      "batch: 7428, loss: 0.1786825805902481\n",
      "batch: 7429, loss: 0.28608763217926025\n",
      "batch: 7430, loss: 0.09047241508960724\n",
      "batch: 7431, loss: 0.0323977917432785\n",
      "batch: 7432, loss: 0.14185766875743866\n",
      "batch: 7433, loss: 0.13237959146499634\n",
      "batch: 7434, loss: 0.27221164107322693\n",
      "batch: 7435, loss: 0.11697538197040558\n",
      "batch: 7436, loss: 0.22149130702018738\n",
      "batch: 7437, loss: 0.24650728702545166\n",
      "batch: 7438, loss: 0.2220054715871811\n",
      "batch: 7439, loss: 0.18257102370262146\n",
      "batch: 7440, loss: 0.15415877103805542\n",
      "batch: 7441, loss: 0.2509192228317261\n",
      "batch: 7442, loss: 0.17484097182750702\n",
      "batch: 7443, loss: 0.0999489277601242\n",
      "batch: 7444, loss: 0.2723502814769745\n",
      "batch: 7445, loss: 0.1387850046157837\n",
      "batch: 7446, loss: 0.09739948809146881\n",
      "batch: 7447, loss: 0.19090084731578827\n",
      "batch: 7448, loss: 0.17534977197647095\n",
      "batch: 7449, loss: 0.06559319794178009\n",
      "batch: 7450, loss: 0.20509806275367737\n",
      "batch: 7451, loss: 0.21924524009227753\n",
      "batch: 7452, loss: 0.2498522698879242\n",
      "batch: 7453, loss: 0.14421100914478302\n",
      "batch: 7454, loss: 0.09606520086526871\n",
      "batch: 7455, loss: 0.1219886988401413\n",
      "batch: 7456, loss: 0.11240547895431519\n",
      "batch: 7457, loss: 0.16583457589149475\n",
      "batch: 7458, loss: 0.14686907827854156\n",
      "batch: 7459, loss: 0.1334349364042282\n",
      "batch: 7460, loss: 0.08638033270835876\n",
      "batch: 7461, loss: 0.16020473837852478\n",
      "batch: 7462, loss: 0.16500985622406006\n",
      "batch: 7463, loss: 0.15499460697174072\n",
      "batch: 7464, loss: 0.15163619816303253\n",
      "batch: 7465, loss: 0.29375702142715454\n",
      "batch: 7466, loss: 0.19920334219932556\n",
      "batch: 7467, loss: 0.20100322365760803\n",
      "batch: 7468, loss: 0.11223653703927994\n",
      "batch: 7469, loss: 0.2159395068883896\n",
      "batch: 7470, loss: 0.20944036543369293\n",
      "batch: 7471, loss: 0.2768135070800781\n",
      "batch: 7472, loss: 0.16069352626800537\n",
      "batch: 7473, loss: 0.22855471074581146\n",
      "batch: 7474, loss: 0.2942812144756317\n",
      "batch: 7475, loss: 0.3166377544403076\n",
      "batch: 7476, loss: 0.1620764136314392\n",
      "batch: 7477, loss: 0.064679354429245\n",
      "batch: 7478, loss: 0.23299235105514526\n",
      "batch: 7479, loss: 0.10386422276496887\n",
      "batch: 7480, loss: 0.1573631465435028\n",
      "batch: 7481, loss: 0.07071291655302048\n",
      "batch: 7482, loss: 0.17350955307483673\n",
      "batch: 7483, loss: 0.08487004041671753\n",
      "batch: 7484, loss: 0.09646914899349213\n",
      "batch: 7485, loss: 0.13868746161460876\n",
      "batch: 7486, loss: 0.09501166641712189\n",
      "batch: 7487, loss: 0.12304943799972534\n",
      "batch: 7488, loss: 0.21610108017921448\n",
      "batch: 7489, loss: 0.13753318786621094\n",
      "batch: 7490, loss: 0.195628821849823\n",
      "batch: 7491, loss: 0.126265749335289\n",
      "batch: 7492, loss: 0.1678014099597931\n",
      "batch: 7493, loss: 0.15981251001358032\n",
      "batch: 7494, loss: 0.2468344122171402\n",
      "batch: 7495, loss: 0.10659587383270264\n",
      "batch: 7496, loss: 0.06624305993318558\n",
      "batch: 7497, loss: 0.1401699334383011\n",
      "batch: 7498, loss: 0.10271868109703064\n",
      "batch: 7499, loss: 0.2013915479183197\n",
      "batch: 7500, loss: 0.23723852634429932\n",
      "model saved to ./saving/model.ckpt-75\n",
      "batch: 7501, loss: 0.09117234498262405\n",
      "batch: 7502, loss: 0.21987445652484894\n",
      "batch: 7503, loss: 0.17738765478134155\n",
      "batch: 7504, loss: 0.0836956650018692\n",
      "batch: 7505, loss: 0.128207728266716\n",
      "batch: 7506, loss: 0.13725873827934265\n",
      "batch: 7507, loss: 0.15476764738559723\n",
      "batch: 7508, loss: 0.24259307980537415\n",
      "batch: 7509, loss: 0.10245665162801743\n",
      "batch: 7510, loss: 0.12403669953346252\n",
      "batch: 7511, loss: 0.17533937096595764\n",
      "batch: 7512, loss: 0.16161775588989258\n",
      "batch: 7513, loss: 0.09122069925069809\n",
      "batch: 7514, loss: 0.25165149569511414\n",
      "batch: 7515, loss: 0.21943558752536774\n",
      "batch: 7516, loss: 0.1821502447128296\n",
      "batch: 7517, loss: 0.1394660621881485\n",
      "batch: 7518, loss: 0.3038374185562134\n",
      "batch: 7519, loss: 0.13685716688632965\n",
      "batch: 7520, loss: 0.1796858012676239\n",
      "batch: 7521, loss: 0.19719462096691132\n",
      "batch: 7522, loss: 0.15463203191757202\n",
      "batch: 7523, loss: 0.1922641098499298\n",
      "batch: 7524, loss: 0.24148674309253693\n",
      "batch: 7525, loss: 0.23592624068260193\n",
      "batch: 7526, loss: 0.17429760098457336\n",
      "batch: 7527, loss: 0.10371364653110504\n",
      "batch: 7528, loss: 0.21750688552856445\n",
      "batch: 7529, loss: 0.4163338840007782\n",
      "batch: 7530, loss: 0.14574426412582397\n",
      "batch: 7531, loss: 0.301647812128067\n",
      "batch: 7532, loss: 0.16939616203308105\n",
      "batch: 7533, loss: 0.07981538772583008\n",
      "batch: 7534, loss: 0.12204153835773468\n",
      "batch: 7535, loss: 0.16420017182826996\n",
      "batch: 7536, loss: 0.13923442363739014\n",
      "batch: 7537, loss: 0.18997006118297577\n",
      "batch: 7538, loss: 0.13241979479789734\n",
      "batch: 7539, loss: 0.08308259397745132\n",
      "batch: 7540, loss: 0.05357120558619499\n",
      "batch: 7541, loss: 0.23908549547195435\n",
      "batch: 7542, loss: 0.18322771787643433\n",
      "batch: 7543, loss: 0.060968659818172455\n",
      "batch: 7544, loss: 0.11583120375871658\n",
      "batch: 7545, loss: 0.180521160364151\n",
      "batch: 7546, loss: 0.16975583136081696\n",
      "batch: 7547, loss: 0.1277444064617157\n",
      "batch: 7548, loss: 0.25972044467926025\n",
      "batch: 7549, loss: 0.20463642477989197\n",
      "batch: 7550, loss: 0.20414799451828003\n",
      "batch: 7551, loss: 0.06241694837808609\n",
      "batch: 7552, loss: 0.2377537190914154\n",
      "batch: 7553, loss: 0.1869007796049118\n",
      "batch: 7554, loss: 0.26313328742980957\n",
      "batch: 7555, loss: 0.14901673793792725\n",
      "batch: 7556, loss: 0.21643459796905518\n",
      "batch: 7557, loss: 0.25318822264671326\n",
      "batch: 7558, loss: 0.2155749797821045\n",
      "batch: 7559, loss: 0.18985505402088165\n",
      "batch: 7560, loss: 0.1604633778333664\n",
      "batch: 7561, loss: 0.08135227113962173\n",
      "batch: 7562, loss: 0.0687803328037262\n",
      "batch: 7563, loss: 0.10658767819404602\n",
      "batch: 7564, loss: 0.080203577876091\n",
      "batch: 7565, loss: 0.17352044582366943\n",
      "batch: 7566, loss: 0.09728328883647919\n",
      "batch: 7567, loss: 0.12915478646755219\n",
      "batch: 7568, loss: 0.18844807147979736\n",
      "batch: 7569, loss: 0.2963995337486267\n",
      "batch: 7570, loss: 0.20965737104415894\n",
      "batch: 7571, loss: 0.2233906090259552\n",
      "batch: 7572, loss: 0.11081214249134064\n",
      "batch: 7573, loss: 0.19009733200073242\n",
      "batch: 7574, loss: 0.08851227164268494\n",
      "batch: 7575, loss: 0.11712434887886047\n",
      "batch: 7576, loss: 0.16243307292461395\n",
      "batch: 7577, loss: 0.21767988801002502\n",
      "batch: 7578, loss: 0.24655479192733765\n",
      "batch: 7579, loss: 0.1330733299255371\n",
      "batch: 7580, loss: 0.13549400866031647\n",
      "batch: 7581, loss: 0.21805301308631897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 7582, loss: 0.14135462045669556\n",
      "batch: 7583, loss: 0.15253400802612305\n",
      "batch: 7584, loss: 0.25192081928253174\n",
      "batch: 7585, loss: 0.1654982566833496\n",
      "batch: 7586, loss: 0.10305466502904892\n",
      "batch: 7587, loss: 0.2191028594970703\n",
      "batch: 7588, loss: 0.13997435569763184\n",
      "batch: 7589, loss: 0.1797114908695221\n",
      "batch: 7590, loss: 0.2001102864742279\n",
      "batch: 7591, loss: 0.06849153339862823\n",
      "batch: 7592, loss: 0.1526590883731842\n",
      "batch: 7593, loss: 0.3219645023345947\n",
      "batch: 7594, loss: 0.2910437285900116\n",
      "batch: 7595, loss: 0.11092909425497055\n",
      "batch: 7596, loss: 0.09687433391809464\n",
      "batch: 7597, loss: 0.1503252387046814\n",
      "batch: 7598, loss: 0.27080777287483215\n",
      "batch: 7599, loss: 0.10901561379432678\n",
      "batch: 7600, loss: 0.1448790729045868\n",
      "model saved to ./saving/model.ckpt-76\n",
      "batch: 7601, loss: 0.16366465389728546\n",
      "batch: 7602, loss: 0.20442739129066467\n",
      "batch: 7603, loss: 0.20545798540115356\n",
      "batch: 7604, loss: 0.24499991536140442\n",
      "batch: 7605, loss: 0.1630680412054062\n",
      "batch: 7606, loss: 0.10490156710147858\n",
      "batch: 7607, loss: 0.4519619345664978\n",
      "batch: 7608, loss: 0.1591481864452362\n",
      "batch: 7609, loss: 0.12037937343120575\n",
      "batch: 7610, loss: 0.10990513861179352\n",
      "batch: 7611, loss: 0.16572977602481842\n",
      "batch: 7612, loss: 0.11789293587207794\n",
      "batch: 7613, loss: 0.09113214910030365\n",
      "batch: 7614, loss: 0.24970948696136475\n",
      "batch: 7615, loss: 0.10860538482666016\n",
      "batch: 7616, loss: 0.09963410347700119\n",
      "batch: 7617, loss: 0.1761893928050995\n",
      "batch: 7618, loss: 0.1666395664215088\n",
      "batch: 7619, loss: 0.1947413980960846\n",
      "batch: 7620, loss: 0.1541997492313385\n",
      "batch: 7621, loss: 0.1719876527786255\n",
      "batch: 7622, loss: 0.2731245756149292\n",
      "batch: 7623, loss: 0.14177919924259186\n",
      "batch: 7624, loss: 0.17810463905334473\n",
      "batch: 7625, loss: 0.09607294201850891\n",
      "batch: 7626, loss: 0.13181638717651367\n",
      "batch: 7627, loss: 0.22748814523220062\n",
      "batch: 7628, loss: 0.14155900478363037\n",
      "batch: 7629, loss: 0.15064099431037903\n",
      "batch: 7630, loss: 0.16870072484016418\n",
      "batch: 7631, loss: 0.3357422649860382\n",
      "batch: 7632, loss: 0.15774503350257874\n",
      "batch: 7633, loss: 0.23664522171020508\n",
      "batch: 7634, loss: 0.198115274310112\n",
      "batch: 7635, loss: 0.3568482995033264\n",
      "batch: 7636, loss: 0.1745637059211731\n",
      "batch: 7637, loss: 0.14482516050338745\n",
      "batch: 7638, loss: 0.1745201051235199\n",
      "batch: 7639, loss: 0.19249169528484344\n",
      "batch: 7640, loss: 0.11215798556804657\n",
      "batch: 7641, loss: 0.14346718788146973\n",
      "batch: 7642, loss: 0.11662913858890533\n",
      "batch: 7643, loss: 0.25509586930274963\n",
      "batch: 7644, loss: 0.08202128857374191\n",
      "batch: 7645, loss: 0.19689355790615082\n",
      "batch: 7646, loss: 0.1546027958393097\n",
      "batch: 7647, loss: 0.1872154027223587\n",
      "batch: 7648, loss: 0.12662257254123688\n",
      "batch: 7649, loss: 0.11621992290019989\n",
      "batch: 7650, loss: 0.14851424098014832\n",
      "batch: 7651, loss: 0.1587570458650589\n",
      "batch: 7652, loss: 0.13351424038410187\n",
      "batch: 7653, loss: 0.12568432092666626\n",
      "batch: 7654, loss: 0.06901095062494278\n",
      "batch: 7655, loss: 0.2856048345565796\n",
      "batch: 7656, loss: 0.15300703048706055\n",
      "batch: 7657, loss: 0.25527822971343994\n",
      "batch: 7658, loss: 0.13246753811836243\n",
      "batch: 7659, loss: 0.12225596606731415\n",
      "batch: 7660, loss: 0.1619260162115097\n",
      "batch: 7661, loss: 0.09665083885192871\n",
      "batch: 7662, loss: 0.132878378033638\n",
      "batch: 7663, loss: 0.2194017767906189\n",
      "batch: 7664, loss: 0.14609889686107635\n",
      "batch: 7665, loss: 0.1210177019238472\n",
      "batch: 7666, loss: 0.11162251234054565\n",
      "batch: 7667, loss: 0.14490990340709686\n",
      "batch: 7668, loss: 0.17544400691986084\n",
      "batch: 7669, loss: 0.18624505400657654\n",
      "batch: 7670, loss: 0.12453220784664154\n",
      "batch: 7671, loss: 0.28397688269615173\n",
      "batch: 7672, loss: 0.14971493184566498\n",
      "batch: 7673, loss: 0.12929269671440125\n",
      "batch: 7674, loss: 0.19961848855018616\n",
      "batch: 7675, loss: 0.2198001891374588\n",
      "batch: 7676, loss: 0.0853654220700264\n",
      "batch: 7677, loss: 0.20559288561344147\n",
      "batch: 7678, loss: 0.31777408719062805\n",
      "batch: 7679, loss: 0.09399513900279999\n",
      "batch: 7680, loss: 0.17953914403915405\n",
      "batch: 7681, loss: 0.12167100608348846\n",
      "batch: 7682, loss: 0.24212825298309326\n",
      "batch: 7683, loss: 0.1265004724264145\n",
      "batch: 7684, loss: 0.24170026183128357\n",
      "batch: 7685, loss: 0.1432691067457199\n",
      "batch: 7686, loss: 0.08979557454586029\n",
      "batch: 7687, loss: 0.12423532456159592\n",
      "batch: 7688, loss: 0.053604431450366974\n",
      "batch: 7689, loss: 0.11233040690422058\n",
      "batch: 7690, loss: 0.3127334713935852\n",
      "batch: 7691, loss: 0.10955995321273804\n",
      "batch: 7692, loss: 0.1010042056441307\n",
      "batch: 7693, loss: 0.3039696216583252\n",
      "batch: 7694, loss: 0.06827576458454132\n",
      "batch: 7695, loss: 0.17430977523326874\n",
      "batch: 7696, loss: 0.2174030840396881\n",
      "batch: 7697, loss: 0.12812072038650513\n",
      "batch: 7698, loss: 0.24206852912902832\n",
      "batch: 7699, loss: 0.0972694531083107\n",
      "batch: 7700, loss: 0.1702149659395218\n",
      "model saved to ./saving/model.ckpt-77\n",
      "batch: 7701, loss: 0.20242808759212494\n",
      "batch: 7702, loss: 0.19074499607086182\n",
      "batch: 7703, loss: 0.29203033447265625\n",
      "batch: 7704, loss: 0.07545158267021179\n",
      "batch: 7705, loss: 0.1382821947336197\n",
      "batch: 7706, loss: 0.20844468474388123\n",
      "batch: 7707, loss: 0.22497451305389404\n",
      "batch: 7708, loss: 0.2334173321723938\n",
      "batch: 7709, loss: 0.08708379417657852\n",
      "batch: 7710, loss: 0.15430864691734314\n",
      "batch: 7711, loss: 0.2807357609272003\n",
      "batch: 7712, loss: 0.08388927578926086\n",
      "batch: 7713, loss: 0.20118944346904755\n",
      "batch: 7714, loss: 0.08565719425678253\n",
      "batch: 7715, loss: 0.33987361192703247\n",
      "batch: 7716, loss: 0.20709079504013062\n",
      "batch: 7717, loss: 0.20481690764427185\n",
      "batch: 7718, loss: 0.09596257656812668\n",
      "batch: 7719, loss: 0.1379857361316681\n",
      "batch: 7720, loss: 0.0813811644911766\n",
      "batch: 7721, loss: 0.3004232347011566\n",
      "batch: 7722, loss: 0.12908342480659485\n",
      "batch: 7723, loss: 0.0481802374124527\n",
      "batch: 7724, loss: 0.1844041645526886\n",
      "batch: 7725, loss: 0.08987405896186829\n",
      "batch: 7726, loss: 0.1272943913936615\n",
      "batch: 7727, loss: 0.07577294111251831\n",
      "batch: 7728, loss: 0.1440427154302597\n",
      "batch: 7729, loss: 0.2278943508863449\n",
      "batch: 7730, loss: 0.14647530019283295\n",
      "batch: 7731, loss: 0.09162487834692001\n",
      "batch: 7732, loss: 0.18533700704574585\n",
      "batch: 7733, loss: 0.22865869104862213\n",
      "batch: 7734, loss: 0.17545345425605774\n",
      "batch: 7735, loss: 0.13910117745399475\n",
      "batch: 7736, loss: 0.20396599173545837\n",
      "batch: 7737, loss: 0.19821472465991974\n",
      "batch: 7738, loss: 0.14813631772994995\n",
      "batch: 7739, loss: 0.15808439254760742\n",
      "batch: 7740, loss: 0.07941010594367981\n",
      "batch: 7741, loss: 0.13675692677497864\n",
      "batch: 7742, loss: 0.1748386025428772\n",
      "batch: 7743, loss: 0.2668161690235138\n",
      "batch: 7744, loss: 0.3082917332649231\n",
      "batch: 7745, loss: 0.15197819471359253\n",
      "batch: 7746, loss: 0.07147284597158432\n",
      "batch: 7747, loss: 0.08395743370056152\n",
      "batch: 7748, loss: 0.05984215438365936\n",
      "batch: 7749, loss: 0.13507458567619324\n",
      "batch: 7750, loss: 0.16347664594650269\n",
      "batch: 7751, loss: 0.21357381343841553\n",
      "batch: 7752, loss: 0.3067566156387329\n",
      "batch: 7753, loss: 0.0826982632279396\n",
      "batch: 7754, loss: 0.3244484066963196\n",
      "batch: 7755, loss: 0.07955671846866608\n",
      "batch: 7756, loss: 0.06551269441843033\n",
      "batch: 7757, loss: 0.21686774492263794\n",
      "batch: 7758, loss: 0.16506151854991913\n",
      "batch: 7759, loss: 0.12907272577285767\n",
      "batch: 7760, loss: 0.27415183186531067\n",
      "batch: 7761, loss: 0.11770325899124146\n",
      "batch: 7762, loss: 0.1518486887216568\n",
      "batch: 7763, loss: 0.24037718772888184\n",
      "batch: 7764, loss: 0.20684517920017242\n",
      "batch: 7765, loss: 0.2368883490562439\n",
      "batch: 7766, loss: 0.07594023644924164\n",
      "batch: 7767, loss: 0.20897512137889862\n",
      "batch: 7768, loss: 0.16982673108577728\n",
      "batch: 7769, loss: 0.045106273144483566\n",
      "batch: 7770, loss: 0.10204479098320007\n",
      "batch: 7771, loss: 0.16414614021778107\n",
      "batch: 7772, loss: 0.07330179214477539\n",
      "batch: 7773, loss: 0.1365615725517273\n",
      "batch: 7774, loss: 0.20284882187843323\n",
      "batch: 7775, loss: 0.2832321524620056\n",
      "batch: 7776, loss: 0.3184153437614441\n",
      "batch: 7777, loss: 0.19841955602169037\n",
      "batch: 7778, loss: 0.10053247958421707\n",
      "batch: 7779, loss: 0.16310948133468628\n",
      "batch: 7780, loss: 0.2261190116405487\n",
      "batch: 7781, loss: 0.33146995306015015\n",
      "batch: 7782, loss: 0.1308048516511917\n",
      "batch: 7783, loss: 0.27732744812965393\n",
      "batch: 7784, loss: 0.11494621634483337\n",
      "batch: 7785, loss: 0.14281417429447174\n",
      "batch: 7786, loss: 0.20221148431301117\n",
      "batch: 7787, loss: 0.30049964785575867\n",
      "batch: 7788, loss: 0.08470021188259125\n",
      "batch: 7789, loss: 0.12112496793270111\n",
      "batch: 7790, loss: 0.19009648263454437\n",
      "batch: 7791, loss: 0.35507887601852417\n",
      "batch: 7792, loss: 0.10579298436641693\n",
      "batch: 7793, loss: 0.17563287913799286\n",
      "batch: 7794, loss: 0.15790335834026337\n",
      "batch: 7795, loss: 0.21285495162010193\n",
      "batch: 7796, loss: 0.19349637627601624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 7797, loss: 0.2544929087162018\n",
      "batch: 7798, loss: 0.38104578852653503\n",
      "batch: 7799, loss: 0.1400572955608368\n",
      "batch: 7800, loss: 0.13388589024543762\n",
      "model saved to ./saving/model.ckpt-78\n",
      "batch: 7801, loss: 0.14005516469478607\n",
      "batch: 7802, loss: 0.19124233722686768\n",
      "batch: 7803, loss: 0.0918908640742302\n",
      "batch: 7804, loss: 0.2117777168750763\n",
      "batch: 7805, loss: 0.1856028139591217\n",
      "batch: 7806, loss: 0.179956316947937\n",
      "batch: 7807, loss: 0.12762677669525146\n",
      "batch: 7808, loss: 0.1244019865989685\n",
      "batch: 7809, loss: 0.11546383798122406\n",
      "batch: 7810, loss: 0.27541112899780273\n",
      "batch: 7811, loss: 0.12973985075950623\n",
      "batch: 7812, loss: 0.17730437219142914\n",
      "batch: 7813, loss: 0.17775295674800873\n",
      "batch: 7814, loss: 0.1278270035982132\n",
      "batch: 7815, loss: 0.15668077766895294\n",
      "batch: 7816, loss: 0.11664668470621109\n",
      "batch: 7817, loss: 0.13230450451374054\n",
      "batch: 7818, loss: 0.11457087099552155\n",
      "batch: 7819, loss: 0.12578704953193665\n",
      "batch: 7820, loss: 0.07046668231487274\n",
      "batch: 7821, loss: 0.0968991070985794\n",
      "batch: 7822, loss: 0.13524046540260315\n",
      "batch: 7823, loss: 0.08127805590629578\n",
      "batch: 7824, loss: 0.13650694489479065\n",
      "batch: 7825, loss: 0.27909839153289795\n",
      "batch: 7826, loss: 0.20010387897491455\n",
      "batch: 7827, loss: 0.21370473504066467\n",
      "batch: 7828, loss: 0.21408739686012268\n",
      "batch: 7829, loss: 0.08159242570400238\n",
      "batch: 7830, loss: 0.10278642177581787\n",
      "batch: 7831, loss: 0.12107354402542114\n",
      "batch: 7832, loss: 0.12481933832168579\n",
      "batch: 7833, loss: 0.10970144718885422\n",
      "batch: 7834, loss: 0.21816140413284302\n",
      "batch: 7835, loss: 0.13525494933128357\n",
      "batch: 7836, loss: 0.12471136450767517\n",
      "batch: 7837, loss: 0.10980928689241409\n",
      "batch: 7838, loss: 0.09681492298841476\n",
      "batch: 7839, loss: 0.1079913079738617\n",
      "batch: 7840, loss: 0.13748914003372192\n",
      "batch: 7841, loss: 0.2110155075788498\n",
      "batch: 7842, loss: 0.15329857170581818\n",
      "batch: 7843, loss: 0.09501665830612183\n",
      "batch: 7844, loss: 0.18686631321907043\n",
      "batch: 7845, loss: 0.09207842499017715\n",
      "batch: 7846, loss: 0.2230520248413086\n",
      "batch: 7847, loss: 0.09975755214691162\n",
      "batch: 7848, loss: 0.2292773574590683\n",
      "batch: 7849, loss: 0.12099272757768631\n",
      "batch: 7850, loss: 0.17981836199760437\n",
      "batch: 7851, loss: 0.0694865733385086\n",
      "batch: 7852, loss: 0.31267231702804565\n",
      "batch: 7853, loss: 0.08470495045185089\n",
      "batch: 7854, loss: 0.12691065669059753\n",
      "batch: 7855, loss: 0.16187456250190735\n",
      "batch: 7856, loss: 0.05802684649825096\n",
      "batch: 7857, loss: 0.3082859516143799\n",
      "batch: 7858, loss: 0.2185143232345581\n",
      "batch: 7859, loss: 0.15187476575374603\n",
      "batch: 7860, loss: 0.09559676051139832\n",
      "batch: 7861, loss: 0.15855713188648224\n",
      "batch: 7862, loss: 0.15017390251159668\n",
      "batch: 7863, loss: 0.13619479537010193\n",
      "batch: 7864, loss: 0.19274330139160156\n",
      "batch: 7865, loss: 0.1332697719335556\n",
      "batch: 7866, loss: 0.1696469485759735\n",
      "batch: 7867, loss: 0.1215895563364029\n",
      "batch: 7868, loss: 0.08488704264163971\n",
      "batch: 7869, loss: 0.16476446390151978\n",
      "batch: 7870, loss: 0.11620014160871506\n",
      "batch: 7871, loss: 0.17804816365242004\n",
      "batch: 7872, loss: 0.1245485320687294\n",
      "batch: 7873, loss: 0.0720783919095993\n",
      "batch: 7874, loss: 0.12128721922636032\n",
      "batch: 7875, loss: 0.11831891536712646\n",
      "batch: 7876, loss: 0.2352253794670105\n",
      "batch: 7877, loss: 0.25644004344940186\n",
      "batch: 7878, loss: 0.28292274475097656\n",
      "batch: 7879, loss: 0.20897486805915833\n",
      "batch: 7880, loss: 0.22348526120185852\n",
      "batch: 7881, loss: 0.3038334846496582\n",
      "batch: 7882, loss: 0.08182470500469208\n",
      "batch: 7883, loss: 0.06403224170207977\n",
      "batch: 7884, loss: 0.060985028743743896\n",
      "batch: 7885, loss: 0.1564822793006897\n",
      "batch: 7886, loss: 0.12472821772098541\n",
      "batch: 7887, loss: 0.272026002407074\n",
      "batch: 7888, loss: 0.1386878788471222\n",
      "batch: 7889, loss: 0.31780675053596497\n",
      "batch: 7890, loss: 0.27169540524482727\n",
      "batch: 7891, loss: 0.13115116953849792\n",
      "batch: 7892, loss: 0.05138354003429413\n",
      "batch: 7893, loss: 0.08201846480369568\n",
      "batch: 7894, loss: 0.14301447570323944\n",
      "batch: 7895, loss: 0.06863300502300262\n",
      "batch: 7896, loss: 0.1780799925327301\n",
      "batch: 7897, loss: 0.08255648612976074\n",
      "batch: 7898, loss: 0.13161048293113708\n",
      "batch: 7899, loss: 0.2373715341091156\n",
      "batch: 7900, loss: 0.09630783647298813\n",
      "model saved to ./saving/model.ckpt-79\n",
      "batch: 7901, loss: 0.1052316352725029\n",
      "batch: 7902, loss: 0.09570980072021484\n",
      "batch: 7903, loss: 0.15482553839683533\n",
      "batch: 7904, loss: 0.1796330213546753\n",
      "batch: 7905, loss: 0.1642344892024994\n",
      "batch: 7906, loss: 0.18567007780075073\n",
      "batch: 7907, loss: 0.08883624523878098\n",
      "batch: 7908, loss: 0.128648042678833\n",
      "batch: 7909, loss: 0.2116585075855255\n",
      "batch: 7910, loss: 0.2531783878803253\n",
      "batch: 7911, loss: 0.11859530210494995\n",
      "batch: 7912, loss: 0.18659982085227966\n",
      "batch: 7913, loss: 0.2017921805381775\n",
      "batch: 7914, loss: 0.13321168720722198\n",
      "batch: 7915, loss: 0.13683874905109406\n",
      "batch: 7916, loss: 0.1822531521320343\n",
      "batch: 7917, loss: 0.16186925768852234\n",
      "batch: 7918, loss: 0.13130192458629608\n",
      "batch: 7919, loss: 0.21929052472114563\n",
      "batch: 7920, loss: 0.15556561946868896\n",
      "batch: 7921, loss: 0.15917402505874634\n",
      "batch: 7922, loss: 0.16648191213607788\n",
      "batch: 7923, loss: 0.2243184745311737\n",
      "batch: 7924, loss: 0.2313496470451355\n",
      "batch: 7925, loss: 0.16057683527469635\n",
      "batch: 7926, loss: 0.267275869846344\n",
      "batch: 7927, loss: 0.12287686765193939\n",
      "batch: 7928, loss: 0.16321571171283722\n",
      "batch: 7929, loss: 0.2840633988380432\n",
      "batch: 7930, loss: 0.17229896783828735\n",
      "batch: 7931, loss: 0.1534976214170456\n",
      "batch: 7932, loss: 0.09909041225910187\n",
      "batch: 7933, loss: 0.08558471500873566\n",
      "batch: 7934, loss: 0.20171216130256653\n",
      "batch: 7935, loss: 0.2557535767555237\n",
      "batch: 7936, loss: 0.08695515990257263\n",
      "batch: 7937, loss: 0.20419754087924957\n",
      "batch: 7938, loss: 0.08839743584394455\n",
      "batch: 7939, loss: 0.17996102571487427\n",
      "batch: 7940, loss: 0.05080994963645935\n",
      "batch: 7941, loss: 0.11691227555274963\n",
      "batch: 7942, loss: 0.1479998677968979\n",
      "batch: 7943, loss: 0.1567697376012802\n",
      "batch: 7944, loss: 0.06020770221948624\n",
      "batch: 7945, loss: 0.13355210423469543\n",
      "batch: 7946, loss: 0.10500577092170715\n",
      "batch: 7947, loss: 0.13579148054122925\n",
      "batch: 7948, loss: 0.21558164060115814\n",
      "batch: 7949, loss: 0.08265794813632965\n",
      "batch: 7950, loss: 0.19320222735404968\n",
      "batch: 7951, loss: 0.09841291606426239\n",
      "batch: 7952, loss: 0.12580963969230652\n",
      "batch: 7953, loss: 0.1689041256904602\n",
      "batch: 7954, loss: 0.3469827473163605\n",
      "batch: 7955, loss: 0.13548310101032257\n",
      "batch: 7956, loss: 0.246529221534729\n",
      "batch: 7957, loss: 0.1602521538734436\n",
      "batch: 7958, loss: 0.20996876060962677\n",
      "batch: 7959, loss: 0.06807538866996765\n",
      "batch: 7960, loss: 0.14502866566181183\n",
      "batch: 7961, loss: 0.15270252525806427\n",
      "batch: 7962, loss: 0.14530843496322632\n",
      "batch: 7963, loss: 0.19407834112644196\n",
      "batch: 7964, loss: 0.19799286127090454\n",
      "batch: 7965, loss: 0.06269793212413788\n",
      "batch: 7966, loss: 0.10369911789894104\n",
      "batch: 7967, loss: 0.1622174084186554\n",
      "batch: 7968, loss: 0.08092831075191498\n",
      "batch: 7969, loss: 0.0909065455198288\n",
      "batch: 7970, loss: 0.08667556941509247\n",
      "batch: 7971, loss: 0.15973693132400513\n",
      "batch: 7972, loss: 0.3412783145904541\n",
      "batch: 7973, loss: 0.13804465532302856\n",
      "batch: 7974, loss: 0.21192152798175812\n",
      "batch: 7975, loss: 0.0831259936094284\n",
      "batch: 7976, loss: 0.15693233907222748\n",
      "batch: 7977, loss: 0.32383859157562256\n",
      "batch: 7978, loss: 0.1400589942932129\n",
      "batch: 7979, loss: 0.20975880324840546\n",
      "batch: 7980, loss: 0.28553494811058044\n",
      "batch: 7981, loss: 0.22274598479270935\n",
      "batch: 7982, loss: 0.1829967200756073\n",
      "batch: 7983, loss: 0.2512524724006653\n",
      "batch: 7984, loss: 0.2528684139251709\n",
      "batch: 7985, loss: 0.04746907949447632\n",
      "batch: 7986, loss: 0.049966905266046524\n",
      "batch: 7987, loss: 0.28231486678123474\n",
      "batch: 7988, loss: 0.09466072916984558\n",
      "batch: 7989, loss: 0.20388469099998474\n",
      "batch: 7990, loss: 0.17916147410869598\n",
      "batch: 7991, loss: 0.15471692383289337\n",
      "batch: 7992, loss: 0.1813662350177765\n",
      "batch: 7993, loss: 0.12554897367954254\n",
      "batch: 7994, loss: 0.23321571946144104\n",
      "batch: 7995, loss: 0.24525298178195953\n",
      "batch: 7996, loss: 0.20836171507835388\n",
      "batch: 7997, loss: 0.21908287703990936\n",
      "batch: 7998, loss: 0.15136724710464478\n",
      "batch: 7999, loss: 0.06153210252523422\n",
      "batch: 8000, loss: 0.10844477266073227\n",
      "model saved to ./saving/model.ckpt-80\n",
      "batch: 8001, loss: 0.24552810192108154\n",
      "batch: 8002, loss: 0.19216057658195496\n",
      "batch: 8003, loss: 0.17106696963310242\n",
      "batch: 8004, loss: 0.1663035750389099\n",
      "batch: 8005, loss: 0.16214631497859955\n",
      "batch: 8006, loss: 0.22173964977264404\n",
      "batch: 8007, loss: 0.09525016695261002\n",
      "batch: 8008, loss: 0.16014987230300903\n",
      "batch: 8009, loss: 0.12923294305801392\n",
      "batch: 8010, loss: 0.32649683952331543\n",
      "batch: 8011, loss: 0.18843239545822144\n",
      "batch: 8012, loss: 0.11073935776948929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 8013, loss: 0.17968356609344482\n",
      "batch: 8014, loss: 0.09226559102535248\n",
      "batch: 8015, loss: 0.09062808007001877\n",
      "batch: 8016, loss: 0.20145940780639648\n",
      "batch: 8017, loss: 0.06535783410072327\n",
      "batch: 8018, loss: 0.1410754919052124\n",
      "batch: 8019, loss: 0.1524069905281067\n",
      "batch: 8020, loss: 0.09913863986730576\n",
      "batch: 8021, loss: 0.1266564428806305\n",
      "batch: 8022, loss: 0.10636435449123383\n",
      "batch: 8023, loss: 0.16894873976707458\n",
      "batch: 8024, loss: 0.0688726007938385\n",
      "batch: 8025, loss: 0.1908448487520218\n",
      "batch: 8026, loss: 0.1305173933506012\n",
      "batch: 8027, loss: 0.060437384992837906\n",
      "batch: 8028, loss: 0.06226065009832382\n",
      "batch: 8029, loss: 0.15573051571846008\n",
      "batch: 8030, loss: 0.18167538940906525\n",
      "batch: 8031, loss: 0.14812937378883362\n",
      "batch: 8032, loss: 0.21696045994758606\n",
      "batch: 8033, loss: 0.12238936871290207\n",
      "batch: 8034, loss: 0.22749152779579163\n",
      "batch: 8035, loss: 0.18916822969913483\n",
      "batch: 8036, loss: 0.09666424989700317\n",
      "batch: 8037, loss: 0.2568100094795227\n",
      "batch: 8038, loss: 0.10012826323509216\n",
      "batch: 8039, loss: 0.07807677984237671\n",
      "batch: 8040, loss: 0.1720372438430786\n",
      "batch: 8041, loss: 0.17537662386894226\n",
      "batch: 8042, loss: 0.27532094717025757\n",
      "batch: 8043, loss: 0.23547163605690002\n",
      "batch: 8044, loss: 0.11837814003229141\n",
      "batch: 8045, loss: 0.08572369813919067\n",
      "batch: 8046, loss: 0.22106313705444336\n",
      "batch: 8047, loss: 0.19868376851081848\n",
      "batch: 8048, loss: 0.13877327740192413\n",
      "batch: 8049, loss: 0.21710911393165588\n",
      "batch: 8050, loss: 0.23173293471336365\n",
      "batch: 8051, loss: 0.23575752973556519\n",
      "batch: 8052, loss: 0.28631600737571716\n",
      "batch: 8053, loss: 0.09074309468269348\n",
      "batch: 8054, loss: 0.07481147348880768\n",
      "batch: 8055, loss: 0.18416666984558105\n",
      "batch: 8056, loss: 0.1798066943883896\n",
      "batch: 8057, loss: 0.18811453878879547\n",
      "batch: 8058, loss: 0.13630571961402893\n",
      "batch: 8059, loss: 0.1924930214881897\n",
      "batch: 8060, loss: 0.32468464970588684\n",
      "batch: 8061, loss: 0.1495939940214157\n",
      "batch: 8062, loss: 0.11971975862979889\n",
      "batch: 8063, loss: 0.09422634541988373\n",
      "batch: 8064, loss: 0.1541496068239212\n",
      "batch: 8065, loss: 0.20723509788513184\n",
      "batch: 8066, loss: 0.06891536712646484\n",
      "batch: 8067, loss: 0.12798447906970978\n",
      "batch: 8068, loss: 0.05694620683789253\n",
      "batch: 8069, loss: 0.15679657459259033\n",
      "batch: 8070, loss: 0.0844758152961731\n",
      "batch: 8071, loss: 0.06679297238588333\n",
      "batch: 8072, loss: 0.1351061761379242\n",
      "batch: 8073, loss: 0.1259010136127472\n",
      "batch: 8074, loss: 0.11686916649341583\n",
      "batch: 8075, loss: 0.14961227774620056\n",
      "batch: 8076, loss: 0.0691082775592804\n",
      "batch: 8077, loss: 0.1361202597618103\n",
      "batch: 8078, loss: 0.15372128784656525\n",
      "batch: 8079, loss: 0.2462272047996521\n",
      "batch: 8080, loss: 0.31630006432533264\n",
      "batch: 8081, loss: 0.35333842039108276\n",
      "batch: 8082, loss: 0.145412415266037\n",
      "batch: 8083, loss: 0.1597222089767456\n",
      "batch: 8084, loss: 0.18848013877868652\n",
      "batch: 8085, loss: 0.11446283012628555\n",
      "batch: 8086, loss: 0.11680939793586731\n",
      "batch: 8087, loss: 0.08721883594989777\n",
      "batch: 8088, loss: 0.15119215846061707\n",
      "batch: 8089, loss: 0.11523759365081787\n",
      "batch: 8090, loss: 0.18659977614879608\n",
      "batch: 8091, loss: 0.10378316789865494\n",
      "batch: 8092, loss: 0.20827387273311615\n",
      "batch: 8093, loss: 0.19736093282699585\n",
      "batch: 8094, loss: 0.10800155997276306\n",
      "batch: 8095, loss: 0.12843941152095795\n",
      "batch: 8096, loss: 0.20193710923194885\n",
      "batch: 8097, loss: 0.28479310870170593\n",
      "batch: 8098, loss: 0.06337563693523407\n",
      "batch: 8099, loss: 0.161404088139534\n",
      "batch: 8100, loss: 0.22032898664474487\n",
      "model saved to ./saving/model.ckpt-81\n",
      "batch: 8101, loss: 0.08019662648439407\n",
      "batch: 8102, loss: 0.1081540435552597\n",
      "batch: 8103, loss: 0.31771034002304077\n",
      "batch: 8104, loss: 0.11352095007896423\n",
      "batch: 8105, loss: 0.09344188868999481\n",
      "batch: 8106, loss: 0.16988272964954376\n",
      "batch: 8107, loss: 0.09629126638174057\n",
      "batch: 8108, loss: 0.14033222198486328\n",
      "batch: 8109, loss: 0.15078140795230865\n",
      "batch: 8110, loss: 0.40587347745895386\n",
      "batch: 8111, loss: 0.21586984395980835\n",
      "batch: 8112, loss: 0.08363603800535202\n",
      "batch: 8113, loss: 0.26934871077537537\n",
      "batch: 8114, loss: 0.19488409161567688\n",
      "batch: 8115, loss: 0.32515978813171387\n",
      "batch: 8116, loss: 0.25335466861724854\n",
      "batch: 8117, loss: 0.24793410301208496\n",
      "batch: 8118, loss: 0.1939997375011444\n",
      "batch: 8119, loss: 0.06907963007688522\n",
      "batch: 8120, loss: 0.13060177862644196\n",
      "batch: 8121, loss: 0.26665228605270386\n",
      "batch: 8122, loss: 0.1509135216474533\n",
      "batch: 8123, loss: 0.10883902758359909\n",
      "batch: 8124, loss: 0.1844930648803711\n",
      "batch: 8125, loss: 0.2275211364030838\n",
      "batch: 8126, loss: 0.09744585305452347\n",
      "batch: 8127, loss: 0.07336287945508957\n",
      "batch: 8128, loss: 0.08901680260896683\n",
      "batch: 8129, loss: 0.20973652601242065\n",
      "batch: 8130, loss: 0.13973687589168549\n",
      "batch: 8131, loss: 0.09658254683017731\n",
      "batch: 8132, loss: 0.12017690390348434\n",
      "batch: 8133, loss: 0.11907830089330673\n",
      "batch: 8134, loss: 0.12386542558670044\n",
      "batch: 8135, loss: 0.1293942928314209\n",
      "batch: 8136, loss: 0.20337042212486267\n",
      "batch: 8137, loss: 0.17648792266845703\n",
      "batch: 8138, loss: 0.25163111090660095\n",
      "batch: 8139, loss: 0.17636051774024963\n",
      "batch: 8140, loss: 0.14524394273757935\n",
      "batch: 8141, loss: 0.15834519267082214\n",
      "batch: 8142, loss: 0.15777963399887085\n",
      "batch: 8143, loss: 0.13968795537948608\n",
      "batch: 8144, loss: 0.1417737454175949\n",
      "batch: 8145, loss: 0.2163233458995819\n",
      "batch: 8146, loss: 0.14508411288261414\n",
      "batch: 8147, loss: 0.15325242280960083\n",
      "batch: 8148, loss: 0.20197239518165588\n",
      "batch: 8149, loss: 0.1065228134393692\n",
      "batch: 8150, loss: 0.10245458781719208\n",
      "batch: 8151, loss: 0.2093012034893036\n",
      "batch: 8152, loss: 0.1550643891096115\n",
      "batch: 8153, loss: 0.3288314938545227\n",
      "batch: 8154, loss: 0.12871679663658142\n",
      "batch: 8155, loss: 0.15560945868492126\n",
      "batch: 8156, loss: 0.22244760394096375\n",
      "batch: 8157, loss: 0.11752801388502121\n",
      "batch: 8158, loss: 0.10142107307910919\n",
      "batch: 8159, loss: 0.11438879370689392\n",
      "batch: 8160, loss: 0.0775521993637085\n",
      "batch: 8161, loss: 0.12767909467220306\n",
      "batch: 8162, loss: 0.4419327974319458\n",
      "batch: 8163, loss: 0.28679388761520386\n",
      "batch: 8164, loss: 0.06152317300438881\n",
      "batch: 8165, loss: 0.28092294931411743\n",
      "batch: 8166, loss: 0.21321314573287964\n",
      "batch: 8167, loss: 0.10783785581588745\n",
      "batch: 8168, loss: 0.2195274531841278\n",
      "batch: 8169, loss: 0.3269706666469574\n",
      "batch: 8170, loss: 0.10454673320055008\n",
      "batch: 8171, loss: 0.10384777933359146\n",
      "batch: 8172, loss: 0.21870630979537964\n",
      "batch: 8173, loss: 0.21091192960739136\n",
      "batch: 8174, loss: 0.18731769919395447\n",
      "batch: 8175, loss: 0.12547896802425385\n",
      "batch: 8176, loss: 0.23440682888031006\n",
      "batch: 8177, loss: 0.08943527936935425\n",
      "batch: 8178, loss: 0.11715209484100342\n",
      "batch: 8179, loss: 0.06389990448951721\n",
      "batch: 8180, loss: 0.11361030489206314\n",
      "batch: 8181, loss: 0.10024018585681915\n",
      "batch: 8182, loss: 0.19699019193649292\n",
      "batch: 8183, loss: 0.12196170538663864\n",
      "batch: 8184, loss: 0.13173708319664001\n",
      "batch: 8185, loss: 0.22277963161468506\n",
      "batch: 8186, loss: 0.1579556167125702\n",
      "batch: 8187, loss: 0.10180921852588654\n",
      "batch: 8188, loss: 0.16122028231620789\n",
      "batch: 8189, loss: 0.14294449985027313\n",
      "batch: 8190, loss: 0.33123159408569336\n",
      "batch: 8191, loss: 0.2538738250732422\n",
      "batch: 8192, loss: 0.2011321485042572\n",
      "batch: 8193, loss: 0.10185538232326508\n",
      "batch: 8194, loss: 0.064785897731781\n",
      "batch: 8195, loss: 0.08633463829755783\n",
      "batch: 8196, loss: 0.21460755169391632\n",
      "batch: 8197, loss: 0.24333319067955017\n",
      "batch: 8198, loss: 0.17379499971866608\n",
      "batch: 8199, loss: 0.16593098640441895\n",
      "batch: 8200, loss: 0.11692675203084946\n",
      "model saved to ./saving/model.ckpt-82\n",
      "batch: 8201, loss: 0.1819494664669037\n",
      "batch: 8202, loss: 0.16950735449790955\n",
      "batch: 8203, loss: 0.11680088937282562\n",
      "batch: 8204, loss: 0.4177713692188263\n",
      "batch: 8205, loss: 0.11800999194383621\n",
      "batch: 8206, loss: 0.2080405056476593\n",
      "batch: 8207, loss: 0.1286575198173523\n",
      "batch: 8208, loss: 0.19564256072044373\n",
      "batch: 8209, loss: 0.13674384355545044\n",
      "batch: 8210, loss: 0.12331794947385788\n",
      "batch: 8211, loss: 0.21181634068489075\n",
      "batch: 8212, loss: 0.17134824395179749\n",
      "batch: 8213, loss: 0.0946858823299408\n",
      "batch: 8214, loss: 0.19208338856697083\n",
      "batch: 8215, loss: 0.2205113172531128\n",
      "batch: 8216, loss: 0.13162527978420258\n",
      "batch: 8217, loss: 0.20297348499298096\n",
      "batch: 8218, loss: 0.15400592982769012\n",
      "batch: 8219, loss: 0.14785261452198029\n",
      "batch: 8220, loss: 0.1729990690946579\n",
      "batch: 8221, loss: 0.13320991396903992\n",
      "batch: 8222, loss: 0.287280797958374\n",
      "batch: 8223, loss: 0.22754883766174316\n",
      "batch: 8224, loss: 0.09650880098342896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 8225, loss: 0.13006862998008728\n",
      "batch: 8226, loss: 0.15784280002117157\n",
      "batch: 8227, loss: 0.16946718096733093\n",
      "batch: 8228, loss: 0.2743857800960541\n",
      "batch: 8229, loss: 0.11244621127843857\n",
      "batch: 8230, loss: 0.04661647230386734\n",
      "batch: 8231, loss: 0.2522248327732086\n",
      "batch: 8232, loss: 0.12114036083221436\n",
      "batch: 8233, loss: 0.24150098860263824\n",
      "batch: 8234, loss: 0.04582880437374115\n",
      "batch: 8235, loss: 0.12904830276966095\n",
      "batch: 8236, loss: 0.13385240733623505\n",
      "batch: 8237, loss: 0.1328534632921219\n",
      "batch: 8238, loss: 0.14500585198402405\n",
      "batch: 8239, loss: 0.3914714753627777\n",
      "batch: 8240, loss: 0.1455947905778885\n",
      "batch: 8241, loss: 0.2267061471939087\n",
      "batch: 8242, loss: 0.10025334358215332\n",
      "batch: 8243, loss: 0.14228418469429016\n",
      "batch: 8244, loss: 0.2949067950248718\n",
      "batch: 8245, loss: 0.17620667815208435\n",
      "batch: 8246, loss: 0.3571171760559082\n",
      "batch: 8247, loss: 0.2294270396232605\n",
      "batch: 8248, loss: 0.16374941170215607\n",
      "batch: 8249, loss: 0.05926046147942543\n",
      "batch: 8250, loss: 0.05389918014407158\n",
      "batch: 8251, loss: 0.13360723853111267\n",
      "batch: 8252, loss: 0.1559811532497406\n",
      "batch: 8253, loss: 0.20089870691299438\n",
      "batch: 8254, loss: 0.3171117305755615\n",
      "batch: 8255, loss: 0.1883440464735031\n",
      "batch: 8256, loss: 0.09437856823205948\n",
      "batch: 8257, loss: 0.1317279189825058\n",
      "batch: 8258, loss: 0.09634242206811905\n",
      "batch: 8259, loss: 0.2258036732673645\n",
      "batch: 8260, loss: 0.10553808510303497\n",
      "batch: 8261, loss: 0.21176806092262268\n",
      "batch: 8262, loss: 0.12253223359584808\n",
      "batch: 8263, loss: 0.14044757187366486\n",
      "batch: 8264, loss: 0.11521288007497787\n",
      "batch: 8265, loss: 0.23922286927700043\n",
      "batch: 8266, loss: 0.12747815251350403\n",
      "batch: 8267, loss: 0.2214178740978241\n",
      "batch: 8268, loss: 0.4495241940021515\n",
      "batch: 8269, loss: 0.16002795100212097\n",
      "batch: 8270, loss: 0.242327481508255\n",
      "batch: 8271, loss: 0.11254757642745972\n",
      "batch: 8272, loss: 0.0810340940952301\n",
      "batch: 8273, loss: 0.07540367543697357\n",
      "batch: 8274, loss: 0.12252889573574066\n",
      "batch: 8275, loss: 0.17154423892498016\n",
      "batch: 8276, loss: 0.07525110244750977\n",
      "batch: 8277, loss: 0.07610587775707245\n",
      "batch: 8278, loss: 0.16471844911575317\n",
      "batch: 8279, loss: 0.0942993238568306\n",
      "batch: 8280, loss: 0.25524765253067017\n",
      "batch: 8281, loss: 0.09066609293222427\n",
      "batch: 8282, loss: 0.04932495579123497\n",
      "batch: 8283, loss: 0.1354181170463562\n",
      "batch: 8284, loss: 0.18169502913951874\n",
      "batch: 8285, loss: 0.2002291977405548\n",
      "batch: 8286, loss: 0.2535926103591919\n",
      "batch: 8287, loss: 0.07145379483699799\n",
      "batch: 8288, loss: 0.19015157222747803\n",
      "batch: 8289, loss: 0.14257128536701202\n",
      "batch: 8290, loss: 0.06870010495185852\n",
      "batch: 8291, loss: 0.08266375213861465\n",
      "batch: 8292, loss: 0.10278340429067612\n",
      "batch: 8293, loss: 0.27128177881240845\n",
      "batch: 8294, loss: 0.08493003994226456\n",
      "batch: 8295, loss: 0.1975264549255371\n",
      "batch: 8296, loss: 0.09593071043491364\n",
      "batch: 8297, loss: 0.11966808885335922\n",
      "batch: 8298, loss: 0.20464107394218445\n",
      "batch: 8299, loss: 0.19603115320205688\n",
      "batch: 8300, loss: 0.07497487962245941\n",
      "model saved to ./saving/model.ckpt-83\n",
      "batch: 8301, loss: 0.1739489734172821\n",
      "batch: 8302, loss: 0.17185305058956146\n",
      "batch: 8303, loss: 0.10966050624847412\n",
      "batch: 8304, loss: 0.08815993368625641\n",
      "batch: 8305, loss: 0.07965236902236938\n",
      "batch: 8306, loss: 0.09195049852132797\n",
      "batch: 8307, loss: 0.1449953317642212\n",
      "batch: 8308, loss: 0.14964798092842102\n",
      "batch: 8309, loss: 0.19903472065925598\n",
      "batch: 8310, loss: 0.1392894983291626\n",
      "batch: 8311, loss: 0.2266068458557129\n",
      "batch: 8312, loss: 0.14093048870563507\n",
      "batch: 8313, loss: 0.1792324185371399\n",
      "batch: 8314, loss: 0.18650873005390167\n",
      "batch: 8315, loss: 0.20717093348503113\n",
      "batch: 8316, loss: 0.22740088403224945\n",
      "batch: 8317, loss: 0.07540784776210785\n",
      "batch: 8318, loss: 0.13120391964912415\n",
      "batch: 8319, loss: 0.1861027181148529\n",
      "batch: 8320, loss: 0.13184309005737305\n",
      "batch: 8321, loss: 0.15852093696594238\n",
      "batch: 8322, loss: 0.2899777293205261\n",
      "batch: 8323, loss: 0.11724727600812912\n",
      "batch: 8324, loss: 0.1510193943977356\n",
      "batch: 8325, loss: 0.1405068039894104\n",
      "batch: 8326, loss: 0.22717733681201935\n",
      "batch: 8327, loss: 0.10088546574115753\n",
      "batch: 8328, loss: 0.10387931764125824\n",
      "batch: 8329, loss: 0.1129814088344574\n",
      "batch: 8330, loss: 0.15331265330314636\n",
      "batch: 8331, loss: 0.17558231949806213\n",
      "batch: 8332, loss: 0.3409830331802368\n",
      "batch: 8333, loss: 0.1343754678964615\n",
      "batch: 8334, loss: 0.17660847306251526\n",
      "batch: 8335, loss: 0.22223760187625885\n",
      "batch: 8336, loss: 0.15108859539031982\n",
      "batch: 8337, loss: 0.15348970890045166\n",
      "batch: 8338, loss: 0.1054053083062172\n",
      "batch: 8339, loss: 0.36053401231765747\n",
      "batch: 8340, loss: 0.10438314080238342\n",
      "batch: 8341, loss: 0.23621338605880737\n",
      "batch: 8342, loss: 0.15717381238937378\n",
      "batch: 8343, loss: 0.34724414348602295\n",
      "batch: 8344, loss: 0.1110760048031807\n",
      "batch: 8345, loss: 0.14742740988731384\n",
      "batch: 8346, loss: 0.2048073709011078\n",
      "batch: 8347, loss: 0.08485852181911469\n",
      "batch: 8348, loss: 0.27323469519615173\n",
      "batch: 8349, loss: 0.15885385870933533\n",
      "batch: 8350, loss: 0.2046271562576294\n",
      "batch: 8351, loss: 0.18412286043167114\n",
      "batch: 8352, loss: 0.05606213957071304\n",
      "batch: 8353, loss: 0.22757597267627716\n",
      "batch: 8354, loss: 0.1632571518421173\n",
      "batch: 8355, loss: 0.17813439667224884\n",
      "batch: 8356, loss: 0.07035467028617859\n",
      "batch: 8357, loss: 0.16609898209571838\n",
      "batch: 8358, loss: 0.12905706465244293\n",
      "batch: 8359, loss: 0.18937429785728455\n",
      "batch: 8360, loss: 0.2300189584493637\n",
      "batch: 8361, loss: 0.06410256028175354\n",
      "batch: 8362, loss: 0.20713332295417786\n",
      "batch: 8363, loss: 0.1420159786939621\n",
      "batch: 8364, loss: 0.1521136462688446\n",
      "batch: 8365, loss: 0.1376776397228241\n",
      "batch: 8366, loss: 0.059318363666534424\n",
      "batch: 8367, loss: 0.07126694917678833\n",
      "batch: 8368, loss: 0.2226102352142334\n",
      "batch: 8369, loss: 0.12858393788337708\n",
      "batch: 8370, loss: 0.15974760055541992\n",
      "batch: 8371, loss: 0.07811430096626282\n",
      "batch: 8372, loss: 0.08092378079891205\n",
      "batch: 8373, loss: 0.14342722296714783\n",
      "batch: 8374, loss: 0.052922844886779785\n",
      "batch: 8375, loss: 0.18705016374588013\n",
      "batch: 8376, loss: 0.17336080968379974\n",
      "batch: 8377, loss: 0.16998308897018433\n",
      "batch: 8378, loss: 0.07318561524152756\n",
      "batch: 8379, loss: 0.17384962737560272\n",
      "batch: 8380, loss: 0.10242550075054169\n",
      "batch: 8381, loss: 0.21180443465709686\n",
      "batch: 8382, loss: 0.25900551676750183\n",
      "batch: 8383, loss: 0.08752845227718353\n",
      "batch: 8384, loss: 0.18177783489227295\n",
      "batch: 8385, loss: 0.14517879486083984\n",
      "batch: 8386, loss: 0.1786475032567978\n",
      "batch: 8387, loss: 0.1985381692647934\n",
      "batch: 8388, loss: 0.24413001537322998\n",
      "batch: 8389, loss: 0.08868010342121124\n",
      "batch: 8390, loss: 0.13524997234344482\n",
      "batch: 8391, loss: 0.20753106474876404\n",
      "batch: 8392, loss: 0.08730264008045197\n",
      "batch: 8393, loss: 0.10462561249732971\n",
      "batch: 8394, loss: 0.05561725050210953\n",
      "batch: 8395, loss: 0.2065027952194214\n",
      "batch: 8396, loss: 0.18880897760391235\n",
      "batch: 8397, loss: 0.09340838342905045\n",
      "batch: 8398, loss: 0.06943607330322266\n",
      "batch: 8399, loss: 0.10187197476625443\n",
      "batch: 8400, loss: 0.14425250887870789\n",
      "model saved to ./saving/model.ckpt-84\n",
      "batch: 8401, loss: 0.09607286006212234\n",
      "batch: 8402, loss: 0.1198558509349823\n",
      "batch: 8403, loss: 0.24660593271255493\n",
      "batch: 8404, loss: 0.20265614986419678\n",
      "batch: 8405, loss: 0.1898280680179596\n",
      "batch: 8406, loss: 0.32575491070747375\n",
      "batch: 8407, loss: 0.15550540387630463\n",
      "batch: 8408, loss: 0.13696123659610748\n",
      "batch: 8409, loss: 0.19620443880558014\n",
      "batch: 8410, loss: 0.12048927694559097\n",
      "batch: 8411, loss: 0.10385582596063614\n",
      "batch: 8412, loss: 0.09801195561885834\n",
      "batch: 8413, loss: 0.1606835275888443\n",
      "batch: 8414, loss: 0.2761342525482178\n",
      "batch: 8415, loss: 0.3210110366344452\n",
      "batch: 8416, loss: 0.053525183349847794\n",
      "batch: 8417, loss: 0.13691282272338867\n",
      "batch: 8418, loss: 0.12351656705141068\n",
      "batch: 8419, loss: 0.19796094298362732\n",
      "batch: 8420, loss: 0.15969246625900269\n",
      "batch: 8421, loss: 0.08466789871454239\n",
      "batch: 8422, loss: 0.14929725229740143\n",
      "batch: 8423, loss: 0.1443089246749878\n",
      "batch: 8424, loss: 0.10377412289381027\n",
      "batch: 8425, loss: 0.22383397817611694\n",
      "batch: 8426, loss: 0.1403178870677948\n",
      "batch: 8427, loss: 0.16969040036201477\n",
      "batch: 8428, loss: 0.21908625960350037\n",
      "batch: 8429, loss: 0.18523746728897095\n",
      "batch: 8430, loss: 0.08108172565698624\n",
      "batch: 8431, loss: 0.25961822271347046\n",
      "batch: 8432, loss: 0.14092549681663513\n",
      "batch: 8433, loss: 0.18930265307426453\n",
      "batch: 8434, loss: 0.131802499294281\n",
      "batch: 8435, loss: 0.16537341475486755\n",
      "batch: 8436, loss: 0.23587964475154877\n",
      "batch: 8437, loss: 0.2325165867805481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 8438, loss: 0.14744797348976135\n",
      "batch: 8439, loss: 0.32892465591430664\n",
      "batch: 8440, loss: 0.1546328067779541\n",
      "batch: 8441, loss: 0.3102376163005829\n",
      "batch: 8442, loss: 0.10758879035711288\n",
      "batch: 8443, loss: 0.17863863706588745\n",
      "batch: 8444, loss: 0.15581084787845612\n",
      "batch: 8445, loss: 0.15955734252929688\n",
      "batch: 8446, loss: 0.19457845389842987\n",
      "batch: 8447, loss: 0.16886211931705475\n",
      "batch: 8448, loss: 0.1618928611278534\n",
      "batch: 8449, loss: 0.1475866287946701\n",
      "batch: 8450, loss: 0.14745400846004486\n",
      "batch: 8451, loss: 0.07828231155872345\n",
      "batch: 8452, loss: 0.07424159348011017\n",
      "batch: 8453, loss: 0.08804704248905182\n",
      "batch: 8454, loss: 0.1394403576850891\n",
      "batch: 8455, loss: 0.14454692602157593\n",
      "batch: 8456, loss: 0.17353515326976776\n",
      "batch: 8457, loss: 0.17349891364574432\n",
      "batch: 8458, loss: 0.16988609731197357\n",
      "batch: 8459, loss: 0.14796005189418793\n",
      "batch: 8460, loss: 0.17425915598869324\n",
      "batch: 8461, loss: 0.15664246678352356\n",
      "batch: 8462, loss: 0.11321540176868439\n",
      "batch: 8463, loss: 0.2327137291431427\n",
      "batch: 8464, loss: 0.3786027133464813\n",
      "batch: 8465, loss: 0.1385217010974884\n",
      "batch: 8466, loss: 0.10870932042598724\n",
      "batch: 8467, loss: 0.23108413815498352\n",
      "batch: 8468, loss: 0.08814587444067001\n",
      "batch: 8469, loss: 0.07979147136211395\n",
      "batch: 8470, loss: 0.1768331527709961\n",
      "batch: 8471, loss: 0.04222128912806511\n",
      "batch: 8472, loss: 0.1136121153831482\n",
      "batch: 8473, loss: 0.21222613751888275\n",
      "batch: 8474, loss: 0.1933373063802719\n",
      "batch: 8475, loss: 0.0892971083521843\n",
      "batch: 8476, loss: 0.05886954069137573\n",
      "batch: 8477, loss: 0.16279509663581848\n",
      "batch: 8478, loss: 0.2157309204339981\n",
      "batch: 8479, loss: 0.13598360121250153\n",
      "batch: 8480, loss: 0.1309288591146469\n",
      "batch: 8481, loss: 0.3060433566570282\n",
      "batch: 8482, loss: 0.06117601320147514\n",
      "batch: 8483, loss: 0.12254531681537628\n",
      "batch: 8484, loss: 0.12555670738220215\n",
      "batch: 8485, loss: 0.041261959820985794\n",
      "batch: 8486, loss: 0.15898793935775757\n",
      "batch: 8487, loss: 0.29149186611175537\n",
      "batch: 8488, loss: 0.08740965276956558\n",
      "batch: 8489, loss: 0.04565461724996567\n",
      "batch: 8490, loss: 0.11554990708827972\n",
      "batch: 8491, loss: 0.13141192495822906\n",
      "batch: 8492, loss: 0.12041063606739044\n",
      "batch: 8493, loss: 0.09369456768035889\n",
      "batch: 8494, loss: 0.1322629302740097\n",
      "batch: 8495, loss: 0.13620993494987488\n",
      "batch: 8496, loss: 0.13638606667518616\n",
      "batch: 8497, loss: 0.22547584772109985\n",
      "batch: 8498, loss: 0.23566600680351257\n",
      "batch: 8499, loss: 0.17926235496997833\n",
      "batch: 8500, loss: 0.22526806592941284\n",
      "model saved to ./saving/model.ckpt-85\n",
      "batch: 8501, loss: 0.1391856074333191\n",
      "batch: 8502, loss: 0.04640823230147362\n",
      "batch: 8503, loss: 0.14551383256912231\n",
      "batch: 8504, loss: 0.15816788375377655\n",
      "batch: 8505, loss: 0.1849936544895172\n",
      "batch: 8506, loss: 0.18837469816207886\n",
      "batch: 8507, loss: 0.08214274048805237\n",
      "batch: 8508, loss: 0.10286399722099304\n",
      "batch: 8509, loss: 0.19399632513523102\n",
      "batch: 8510, loss: 0.1757657527923584\n",
      "batch: 8511, loss: 0.13290418684482574\n",
      "batch: 8512, loss: 0.18840087950229645\n",
      "batch: 8513, loss: 0.22352199256420135\n",
      "batch: 8514, loss: 0.11636027693748474\n",
      "batch: 8515, loss: 0.2480911910533905\n",
      "batch: 8516, loss: 0.0428590290248394\n",
      "batch: 8517, loss: 0.20004123449325562\n",
      "batch: 8518, loss: 0.10588906705379486\n",
      "batch: 8519, loss: 0.13234984874725342\n",
      "batch: 8520, loss: 0.13247910141944885\n",
      "batch: 8521, loss: 0.1599973738193512\n",
      "batch: 8522, loss: 0.23344528675079346\n",
      "batch: 8523, loss: 0.12863750755786896\n",
      "batch: 8524, loss: 0.1337987780570984\n",
      "batch: 8525, loss: 0.13109275698661804\n",
      "batch: 8526, loss: 0.15158027410507202\n",
      "batch: 8527, loss: 0.21062907576560974\n",
      "batch: 8528, loss: 0.19206956028938293\n",
      "batch: 8529, loss: 0.2724760174751282\n",
      "batch: 8530, loss: 0.1752837896347046\n",
      "batch: 8531, loss: 0.16769695281982422\n",
      "batch: 8532, loss: 0.13654585182666779\n",
      "batch: 8533, loss: 0.13080289959907532\n",
      "batch: 8534, loss: 0.13975155353546143\n",
      "batch: 8535, loss: 0.21103914082050323\n",
      "batch: 8536, loss: 0.1449664682149887\n",
      "batch: 8537, loss: 0.052066728472709656\n",
      "batch: 8538, loss: 0.2911406457424164\n",
      "batch: 8539, loss: 0.06394127011299133\n",
      "batch: 8540, loss: 0.15102511644363403\n",
      "batch: 8541, loss: 0.375892698764801\n",
      "batch: 8542, loss: 0.23443710803985596\n",
      "batch: 8543, loss: 0.23422230780124664\n",
      "batch: 8544, loss: 0.1382184773683548\n",
      "batch: 8545, loss: 0.16080866754055023\n",
      "batch: 8546, loss: 0.07796558737754822\n",
      "batch: 8547, loss: 0.13145679235458374\n",
      "batch: 8548, loss: 0.1054987981915474\n",
      "batch: 8549, loss: 0.10925745964050293\n",
      "batch: 8550, loss: 0.11687904596328735\n",
      "batch: 8551, loss: 0.18922783434391022\n",
      "batch: 8552, loss: 0.12152325361967087\n",
      "batch: 8553, loss: 0.1980665773153305\n",
      "batch: 8554, loss: 0.16440296173095703\n",
      "batch: 8555, loss: 0.20486830174922943\n",
      "batch: 8556, loss: 0.1240026205778122\n",
      "batch: 8557, loss: 0.1185140460729599\n",
      "batch: 8558, loss: 0.28711286187171936\n",
      "batch: 8559, loss: 0.11766417324542999\n",
      "batch: 8560, loss: 0.10003788769245148\n",
      "batch: 8561, loss: 0.10671479254961014\n",
      "batch: 8562, loss: 0.06467223167419434\n",
      "batch: 8563, loss: 0.17634113132953644\n",
      "batch: 8564, loss: 0.17871281504631042\n",
      "batch: 8565, loss: 0.1659606695175171\n",
      "batch: 8566, loss: 0.10639636218547821\n",
      "batch: 8567, loss: 0.17516575753688812\n",
      "batch: 8568, loss: 0.061571381986141205\n",
      "batch: 8569, loss: 0.13654685020446777\n",
      "batch: 8570, loss: 0.15461264550685883\n",
      "batch: 8571, loss: 0.21706047654151917\n",
      "batch: 8572, loss: 0.1143040880560875\n",
      "batch: 8573, loss: 0.3581465780735016\n",
      "batch: 8574, loss: 0.07628949731588364\n",
      "batch: 8575, loss: 0.13609929382801056\n",
      "batch: 8576, loss: 0.025866813957691193\n",
      "batch: 8577, loss: 0.16509294509887695\n",
      "batch: 8578, loss: 0.18661212921142578\n",
      "batch: 8579, loss: 0.11456884443759918\n",
      "batch: 8580, loss: 0.06766226887702942\n",
      "batch: 8581, loss: 0.18206687271595\n",
      "batch: 8582, loss: 0.2322232723236084\n",
      "batch: 8583, loss: 0.17066910862922668\n",
      "batch: 8584, loss: 0.1627645343542099\n",
      "batch: 8585, loss: 0.19090408086776733\n",
      "batch: 8586, loss: 0.11408093571662903\n",
      "batch: 8587, loss: 0.08998198807239532\n",
      "batch: 8588, loss: 0.11666935682296753\n",
      "batch: 8589, loss: 0.22281168401241302\n",
      "batch: 8590, loss: 0.14482614398002625\n",
      "batch: 8591, loss: 0.13948386907577515\n",
      "batch: 8592, loss: 0.1199415922164917\n",
      "batch: 8593, loss: 0.08156749606132507\n",
      "batch: 8594, loss: 0.18636824190616608\n",
      "batch: 8595, loss: 0.3872602581977844\n",
      "batch: 8596, loss: 0.057754915207624435\n",
      "batch: 8597, loss: 0.13138628005981445\n",
      "batch: 8598, loss: 0.12973244488239288\n",
      "batch: 8599, loss: 0.1009053885936737\n",
      "batch: 8600, loss: 0.14618757367134094\n",
      "model saved to ./saving/model.ckpt-86\n",
      "batch: 8601, loss: 0.19964377582073212\n",
      "batch: 8602, loss: 0.1383519321680069\n",
      "batch: 8603, loss: 0.24564501643180847\n",
      "batch: 8604, loss: 0.1418209969997406\n",
      "batch: 8605, loss: 0.11078996956348419\n",
      "batch: 8606, loss: 0.0857449620962143\n",
      "batch: 8607, loss: 0.13630008697509766\n",
      "batch: 8608, loss: 0.07639270275831223\n",
      "batch: 8609, loss: 0.11905179917812347\n",
      "batch: 8610, loss: 0.2423921823501587\n",
      "batch: 8611, loss: 0.31478309631347656\n",
      "batch: 8612, loss: 0.23123525083065033\n",
      "batch: 8613, loss: 0.08489202708005905\n",
      "batch: 8614, loss: 0.16997523605823517\n",
      "batch: 8615, loss: 0.2609696388244629\n",
      "batch: 8616, loss: 0.19874966144561768\n",
      "batch: 8617, loss: 0.07197554409503937\n",
      "batch: 8618, loss: 0.18887439370155334\n",
      "batch: 8619, loss: 0.08390483260154724\n",
      "batch: 8620, loss: 0.2838073968887329\n",
      "batch: 8621, loss: 0.18624043464660645\n",
      "batch: 8622, loss: 0.11951613426208496\n",
      "batch: 8623, loss: 0.21036607027053833\n",
      "batch: 8624, loss: 0.1433110237121582\n",
      "batch: 8625, loss: 0.2120586633682251\n",
      "batch: 8626, loss: 0.04660409688949585\n",
      "batch: 8627, loss: 0.19127440452575684\n",
      "batch: 8628, loss: 0.1629599928855896\n",
      "batch: 8629, loss: 0.16313999891281128\n",
      "batch: 8630, loss: 0.20639431476593018\n",
      "batch: 8631, loss: 0.15243235230445862\n",
      "batch: 8632, loss: 0.07864031940698624\n",
      "batch: 8633, loss: 0.12646833062171936\n",
      "batch: 8634, loss: 0.13545385003089905\n",
      "batch: 8635, loss: 0.07885372638702393\n",
      "batch: 8636, loss: 0.24100570380687714\n",
      "batch: 8637, loss: 0.17904719710350037\n",
      "batch: 8638, loss: 0.1837942898273468\n",
      "batch: 8639, loss: 0.21244193613529205\n",
      "batch: 8640, loss: 0.15607956051826477\n",
      "batch: 8641, loss: 0.07593269646167755\n",
      "batch: 8642, loss: 0.18521445989608765\n",
      "batch: 8643, loss: 0.0712946206331253\n",
      "batch: 8644, loss: 0.23346905410289764\n",
      "batch: 8645, loss: 0.18570750951766968\n",
      "batch: 8646, loss: 0.06775081157684326\n",
      "batch: 8647, loss: 0.09419510513544083\n",
      "batch: 8648, loss: 0.173899844288826\n",
      "batch: 8649, loss: 0.22647987306118011\n",
      "batch: 8650, loss: 0.1755027323961258\n",
      "batch: 8651, loss: 0.15706586837768555\n",
      "batch: 8652, loss: 0.06016156077384949\n",
      "batch: 8653, loss: 0.1982201486825943\n",
      "batch: 8654, loss: 0.09162984043359756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 8655, loss: 0.1283341944217682\n",
      "batch: 8656, loss: 0.1605094075202942\n",
      "batch: 8657, loss: 0.16262118518352509\n",
      "batch: 8658, loss: 0.18108174204826355\n",
      "batch: 8659, loss: 0.13453799486160278\n",
      "batch: 8660, loss: 0.1818234920501709\n",
      "batch: 8661, loss: 0.20424459874629974\n",
      "batch: 8662, loss: 0.14845487475395203\n",
      "batch: 8663, loss: 0.1305709034204483\n",
      "batch: 8664, loss: 0.044089268893003464\n",
      "batch: 8665, loss: 0.12176531553268433\n",
      "batch: 8666, loss: 0.12606972455978394\n",
      "batch: 8667, loss: 0.09140731394290924\n",
      "batch: 8668, loss: 0.14674443006515503\n",
      "batch: 8669, loss: 0.11719805747270584\n",
      "batch: 8670, loss: 0.11812619864940643\n",
      "batch: 8671, loss: 0.12596458196640015\n",
      "batch: 8672, loss: 0.10468433797359467\n",
      "batch: 8673, loss: 0.09851939976215363\n",
      "batch: 8674, loss: 0.07797467708587646\n",
      "batch: 8675, loss: 0.08714481443166733\n",
      "batch: 8676, loss: 0.21558630466461182\n",
      "batch: 8677, loss: 0.09359321743249893\n",
      "batch: 8678, loss: 0.0650409460067749\n",
      "batch: 8679, loss: 0.15910452604293823\n",
      "batch: 8680, loss: 0.12881924211978912\n",
      "batch: 8681, loss: 0.160772904753685\n",
      "batch: 8682, loss: 0.10017873346805573\n",
      "batch: 8683, loss: 0.2129688560962677\n",
      "batch: 8684, loss: 0.09885324537754059\n",
      "batch: 8685, loss: 0.12917868793010712\n",
      "batch: 8686, loss: 0.17004364728927612\n",
      "batch: 8687, loss: 0.15550868213176727\n",
      "batch: 8688, loss: 0.12122926115989685\n",
      "batch: 8689, loss: 0.12893123924732208\n",
      "batch: 8690, loss: 0.12971855700016022\n",
      "batch: 8691, loss: 0.07289470732212067\n",
      "batch: 8692, loss: 0.13262027502059937\n",
      "batch: 8693, loss: 0.092408686876297\n",
      "batch: 8694, loss: 0.04640994593501091\n",
      "batch: 8695, loss: 0.10038285702466965\n",
      "batch: 8696, loss: 0.22510042786598206\n",
      "batch: 8697, loss: 0.11169308423995972\n",
      "batch: 8698, loss: 0.3156461715698242\n",
      "batch: 8699, loss: 0.12588703632354736\n",
      "batch: 8700, loss: 0.1096939891576767\n",
      "model saved to ./saving/model.ckpt-87\n",
      "batch: 8701, loss: 0.12225522845983505\n",
      "batch: 8702, loss: 0.13267387449741364\n",
      "batch: 8703, loss: 0.21477657556533813\n",
      "batch: 8704, loss: 0.21355251967906952\n",
      "batch: 8705, loss: 0.17594146728515625\n",
      "batch: 8706, loss: 0.0693339928984642\n",
      "batch: 8707, loss: 0.0762249231338501\n",
      "batch: 8708, loss: 0.08758140355348587\n",
      "batch: 8709, loss: 0.10090354830026627\n",
      "batch: 8710, loss: 0.14670196175575256\n",
      "batch: 8711, loss: 0.16084134578704834\n",
      "batch: 8712, loss: 0.07275255024433136\n",
      "batch: 8713, loss: 0.1535365879535675\n",
      "batch: 8714, loss: 0.0732763484120369\n",
      "batch: 8715, loss: 0.16530761122703552\n",
      "batch: 8716, loss: 0.11772365868091583\n",
      "batch: 8717, loss: 0.2842872142791748\n",
      "batch: 8718, loss: 0.058291610330343246\n",
      "batch: 8719, loss: 0.2917553782463074\n",
      "batch: 8720, loss: 0.1827526092529297\n",
      "batch: 8721, loss: 0.12359537184238434\n",
      "batch: 8722, loss: 0.13980668783187866\n",
      "batch: 8723, loss: 0.25877928733825684\n",
      "batch: 8724, loss: 0.2731003165245056\n",
      "batch: 8725, loss: 0.08966494351625443\n",
      "batch: 8726, loss: 0.23203209042549133\n",
      "batch: 8727, loss: 0.11220832169055939\n",
      "batch: 8728, loss: 0.22818633913993835\n",
      "batch: 8729, loss: 0.24051213264465332\n",
      "batch: 8730, loss: 0.18960517644882202\n",
      "batch: 8731, loss: 0.13163791596889496\n",
      "batch: 8732, loss: 0.163033127784729\n",
      "batch: 8733, loss: 0.16674527525901794\n",
      "batch: 8734, loss: 0.08432234823703766\n",
      "batch: 8735, loss: 0.10991393774747849\n",
      "batch: 8736, loss: 0.11859727650880814\n",
      "batch: 8737, loss: 0.18583402037620544\n",
      "batch: 8738, loss: 0.14090970158576965\n",
      "batch: 8739, loss: 0.1226443499326706\n",
      "batch: 8740, loss: 0.14330413937568665\n",
      "batch: 8741, loss: 0.15715321898460388\n",
      "batch: 8742, loss: 0.15424232184886932\n",
      "batch: 8743, loss: 0.15043725073337555\n",
      "batch: 8744, loss: 0.126023069024086\n",
      "batch: 8745, loss: 0.24245768785476685\n",
      "batch: 8746, loss: 0.14115282893180847\n",
      "batch: 8747, loss: 0.16728532314300537\n",
      "batch: 8748, loss: 0.13681533932685852\n",
      "batch: 8749, loss: 0.2347317636013031\n",
      "batch: 8750, loss: 0.09420566260814667\n",
      "batch: 8751, loss: 0.21386264264583588\n",
      "batch: 8752, loss: 0.12019968777894974\n",
      "batch: 8753, loss: 0.0767696350812912\n",
      "batch: 8754, loss: 0.11173051595687866\n",
      "batch: 8755, loss: 0.04537104815244675\n",
      "batch: 8756, loss: 0.20186659693717957\n",
      "batch: 8757, loss: 0.12254718691110611\n",
      "batch: 8758, loss: 0.09302306175231934\n",
      "batch: 8759, loss: 0.3098478317260742\n",
      "batch: 8760, loss: 0.1824343353509903\n",
      "batch: 8761, loss: 0.18584036827087402\n",
      "batch: 8762, loss: 0.2568185031414032\n",
      "batch: 8763, loss: 0.08593648672103882\n",
      "batch: 8764, loss: 0.06420952081680298\n",
      "batch: 8765, loss: 0.16201359033584595\n",
      "batch: 8766, loss: 0.04876717925071716\n",
      "batch: 8767, loss: 0.1706027239561081\n",
      "batch: 8768, loss: 0.11677442491054535\n",
      "batch: 8769, loss: 0.1346154808998108\n",
      "batch: 8770, loss: 0.2055543065071106\n",
      "batch: 8771, loss: 0.08928483724594116\n",
      "batch: 8772, loss: 0.07393838465213776\n",
      "batch: 8773, loss: 0.10303061455488205\n",
      "batch: 8774, loss: 0.09999014437198639\n",
      "batch: 8775, loss: 0.0858539268374443\n",
      "batch: 8776, loss: 0.2593340575695038\n",
      "batch: 8777, loss: 0.1248755156993866\n",
      "batch: 8778, loss: 0.12100405246019363\n",
      "batch: 8779, loss: 0.12686188519001007\n",
      "batch: 8780, loss: 0.23285137116909027\n",
      "batch: 8781, loss: 0.07769890129566193\n",
      "batch: 8782, loss: 0.06282050907611847\n",
      "batch: 8783, loss: 0.21219545602798462\n",
      "batch: 8784, loss: 0.14993205666542053\n",
      "batch: 8785, loss: 0.13362842798233032\n",
      "batch: 8786, loss: 0.23538538813591003\n",
      "batch: 8787, loss: 0.31201300024986267\n",
      "batch: 8788, loss: 0.10663576424121857\n",
      "batch: 8789, loss: 0.21229055523872375\n",
      "batch: 8790, loss: 0.08278835564851761\n",
      "batch: 8791, loss: 0.35182175040245056\n",
      "batch: 8792, loss: 0.1784842312335968\n",
      "batch: 8793, loss: 0.31800398230552673\n",
      "batch: 8794, loss: 0.1365680992603302\n",
      "batch: 8795, loss: 0.12881195545196533\n",
      "batch: 8796, loss: 0.07729306817054749\n",
      "batch: 8797, loss: 0.049713775515556335\n",
      "batch: 8798, loss: 0.17859536409378052\n",
      "batch: 8799, loss: 0.21173232793807983\n",
      "batch: 8800, loss: 0.06807497143745422\n",
      "model saved to ./saving/model.ckpt-88\n",
      "batch: 8801, loss: 0.25697624683380127\n",
      "batch: 8802, loss: 0.09198745340108871\n",
      "batch: 8803, loss: 0.33744263648986816\n",
      "batch: 8804, loss: 0.21983055770397186\n",
      "batch: 8805, loss: 0.14766383171081543\n",
      "batch: 8806, loss: 0.053741246461868286\n",
      "batch: 8807, loss: 0.12420307844877243\n",
      "batch: 8808, loss: 0.12111403793096542\n",
      "batch: 8809, loss: 0.2359035611152649\n",
      "batch: 8810, loss: 0.09861979633569717\n",
      "batch: 8811, loss: 0.15287615358829498\n",
      "batch: 8812, loss: 0.12462005764245987\n",
      "batch: 8813, loss: 0.20670872926712036\n",
      "batch: 8814, loss: 0.32402583956718445\n",
      "batch: 8815, loss: 0.18479987978935242\n",
      "batch: 8816, loss: 0.12332015484571457\n",
      "batch: 8817, loss: 0.07605717331171036\n",
      "batch: 8818, loss: 0.3074270784854889\n",
      "batch: 8819, loss: 0.06901872903108597\n",
      "batch: 8820, loss: 0.2095670998096466\n",
      "batch: 8821, loss: 0.15769174695014954\n",
      "batch: 8822, loss: 0.180947408080101\n",
      "batch: 8823, loss: 0.1247592493891716\n",
      "batch: 8824, loss: 0.15232838690280914\n",
      "batch: 8825, loss: 0.19388416409492493\n",
      "batch: 8826, loss: 0.12563219666481018\n",
      "batch: 8827, loss: 0.1382119059562683\n",
      "batch: 8828, loss: 0.17610055208206177\n",
      "batch: 8829, loss: 0.14284572005271912\n",
      "batch: 8830, loss: 0.10355280339717865\n",
      "batch: 8831, loss: 0.2096826136112213\n",
      "batch: 8832, loss: 0.30126312375068665\n",
      "batch: 8833, loss: 0.21402767300605774\n",
      "batch: 8834, loss: 0.11905492097139359\n",
      "batch: 8835, loss: 0.19012589752674103\n",
      "batch: 8836, loss: 0.04888521879911423\n",
      "batch: 8837, loss: 0.11355936527252197\n",
      "batch: 8838, loss: 0.11833471059799194\n",
      "batch: 8839, loss: 0.15413478016853333\n",
      "batch: 8840, loss: 0.12150458246469498\n",
      "batch: 8841, loss: 0.0783376470208168\n",
      "batch: 8842, loss: 0.1032589003443718\n",
      "batch: 8843, loss: 0.16541007161140442\n",
      "batch: 8844, loss: 0.07570743560791016\n",
      "batch: 8845, loss: 0.06276104599237442\n",
      "batch: 8846, loss: 0.15614241361618042\n",
      "batch: 8847, loss: 0.12260664999485016\n",
      "batch: 8848, loss: 0.1629486083984375\n",
      "batch: 8849, loss: 0.13091683387756348\n",
      "batch: 8850, loss: 0.18666833639144897\n",
      "batch: 8851, loss: 0.17149466276168823\n",
      "batch: 8852, loss: 0.10943830013275146\n",
      "batch: 8853, loss: 0.0744624137878418\n",
      "batch: 8854, loss: 0.06832362711429596\n",
      "batch: 8855, loss: 0.11552394181489944\n",
      "batch: 8856, loss: 0.14119070768356323\n",
      "batch: 8857, loss: 0.10859864950180054\n",
      "batch: 8858, loss: 0.29074227809906006\n",
      "batch: 8859, loss: 0.16773587465286255\n",
      "batch: 8860, loss: 0.20083796977996826\n",
      "batch: 8861, loss: 0.15898872911930084\n",
      "batch: 8862, loss: 0.10000702738761902\n",
      "batch: 8863, loss: 0.21807929873466492\n",
      "batch: 8864, loss: 0.07868510484695435\n",
      "batch: 8865, loss: 0.08009514957666397\n",
      "batch: 8866, loss: 0.15218234062194824\n",
      "batch: 8867, loss: 0.041649624705314636\n",
      "batch: 8868, loss: 0.1064312532544136\n",
      "batch: 8869, loss: 0.18393629789352417\n",
      "batch: 8870, loss: 0.07220251113176346\n",
      "batch: 8871, loss: 0.2510620653629303\n",
      "batch: 8872, loss: 0.15234027802944183\n",
      "batch: 8873, loss: 0.12765955924987793\n",
      "batch: 8874, loss: 0.1675785928964615\n",
      "batch: 8875, loss: 0.18052248656749725\n",
      "batch: 8876, loss: 0.20142684876918793\n",
      "batch: 8877, loss: 0.07147186994552612\n",
      "batch: 8878, loss: 0.13804909586906433\n",
      "batch: 8879, loss: 0.1143454909324646\n",
      "batch: 8880, loss: 0.176174134016037\n",
      "batch: 8881, loss: 0.1743241399526596\n",
      "batch: 8882, loss: 0.1048949733376503\n",
      "batch: 8883, loss: 0.17311450839042664\n",
      "batch: 8884, loss: 0.10472916066646576\n",
      "batch: 8885, loss: 0.09168574213981628\n",
      "batch: 8886, loss: 0.14398466050624847\n",
      "batch: 8887, loss: 0.05646863207221031\n",
      "batch: 8888, loss: 0.11285930871963501\n",
      "batch: 8889, loss: 0.1443771868944168\n",
      "batch: 8890, loss: 0.11921520531177521\n",
      "batch: 8891, loss: 0.10184796154499054\n",
      "batch: 8892, loss: 0.1686936616897583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 8893, loss: 0.0945325642824173\n",
      "batch: 8894, loss: 0.10048924386501312\n",
      "batch: 8895, loss: 0.10068122297525406\n",
      "batch: 8896, loss: 0.12177018076181412\n",
      "batch: 8897, loss: 0.1020960807800293\n",
      "batch: 8898, loss: 0.22390879690647125\n",
      "batch: 8899, loss: 0.12926730513572693\n",
      "batch: 8900, loss: 0.2148577868938446\n",
      "model saved to ./saving/model.ckpt-89\n",
      "batch: 8901, loss: 0.25498539209365845\n",
      "batch: 8902, loss: 0.15141640603542328\n",
      "batch: 8903, loss: 0.1577070653438568\n",
      "batch: 8904, loss: 0.052788592875003815\n",
      "batch: 8905, loss: 0.13757476210594177\n",
      "batch: 8906, loss: 0.09061741828918457\n",
      "batch: 8907, loss: 0.12158313393592834\n",
      "batch: 8908, loss: 0.19134782254695892\n",
      "batch: 8909, loss: 0.13141624629497528\n",
      "batch: 8910, loss: 0.07375677675008774\n",
      "batch: 8911, loss: 0.06579761952161789\n",
      "batch: 8912, loss: 0.1816541850566864\n",
      "batch: 8913, loss: 0.19267606735229492\n",
      "batch: 8914, loss: 0.09615752100944519\n",
      "batch: 8915, loss: 0.22789566218852997\n",
      "batch: 8916, loss: 0.13422861695289612\n",
      "batch: 8917, loss: 0.08230621367692947\n",
      "batch: 8918, loss: 0.12198696285486221\n",
      "batch: 8919, loss: 0.16026118397712708\n",
      "batch: 8920, loss: 0.09215328097343445\n",
      "batch: 8921, loss: 0.24496853351593018\n",
      "batch: 8922, loss: 0.05952367186546326\n",
      "batch: 8923, loss: 0.13487008213996887\n",
      "batch: 8924, loss: 0.10070303082466125\n",
      "batch: 8925, loss: 0.09759663790464401\n",
      "batch: 8926, loss: 0.27435776591300964\n",
      "batch: 8927, loss: 0.11552072316408157\n",
      "batch: 8928, loss: 0.14643755555152893\n",
      "batch: 8929, loss: 0.05219428986310959\n",
      "batch: 8930, loss: 0.18844498693943024\n",
      "batch: 8931, loss: 0.10750995576381683\n",
      "batch: 8932, loss: 0.05453591048717499\n",
      "batch: 8933, loss: 0.1802568882703781\n",
      "batch: 8934, loss: 0.18943962454795837\n",
      "batch: 8935, loss: 0.14717623591423035\n",
      "batch: 8936, loss: 0.1494089812040329\n",
      "batch: 8937, loss: 0.1788807362318039\n",
      "batch: 8938, loss: 0.048419732600450516\n",
      "batch: 8939, loss: 0.11134582757949829\n",
      "batch: 8940, loss: 0.08585428446531296\n",
      "batch: 8941, loss: 0.16290095448493958\n",
      "batch: 8942, loss: 0.11904315650463104\n",
      "batch: 8943, loss: 0.18793067336082458\n",
      "batch: 8944, loss: 0.20001906156539917\n",
      "batch: 8945, loss: 0.2127106487751007\n",
      "batch: 8946, loss: 0.15749379992485046\n",
      "batch: 8947, loss: 0.2539971172809601\n",
      "batch: 8948, loss: 0.13771536946296692\n",
      "batch: 8949, loss: 0.11498371511697769\n",
      "batch: 8950, loss: 0.2950897216796875\n",
      "batch: 8951, loss: 0.1458660364151001\n",
      "batch: 8952, loss: 0.15531809628009796\n",
      "batch: 8953, loss: 0.17200027406215668\n",
      "batch: 8954, loss: 0.27958738803863525\n",
      "batch: 8955, loss: 0.255793035030365\n",
      "batch: 8956, loss: 0.204444020986557\n",
      "batch: 8957, loss: 0.1691851019859314\n",
      "batch: 8958, loss: 0.3106608986854553\n",
      "batch: 8959, loss: 0.22327183187007904\n",
      "batch: 8960, loss: 0.1688721626996994\n",
      "batch: 8961, loss: 0.12326033413410187\n",
      "batch: 8962, loss: 0.10049561411142349\n",
      "batch: 8963, loss: 0.16745904088020325\n",
      "batch: 8964, loss: 0.2946149706840515\n",
      "batch: 8965, loss: 0.0998481959104538\n",
      "batch: 8966, loss: 0.089602530002594\n",
      "batch: 8967, loss: 0.06705793738365173\n",
      "batch: 8968, loss: 0.2436833381652832\n",
      "batch: 8969, loss: 0.15714293718338013\n",
      "batch: 8970, loss: 0.07563797384500504\n",
      "batch: 8971, loss: 0.13432861864566803\n",
      "batch: 8972, loss: 0.15510530769824982\n",
      "batch: 8973, loss: 0.08180026710033417\n",
      "batch: 8974, loss: 0.10285842418670654\n",
      "batch: 8975, loss: 0.17829108238220215\n",
      "batch: 8976, loss: 0.1215440034866333\n",
      "batch: 8977, loss: 0.11305612325668335\n",
      "batch: 8978, loss: 0.18631939589977264\n",
      "batch: 8979, loss: 0.11432532221078873\n",
      "batch: 8980, loss: 0.15878799557685852\n",
      "batch: 8981, loss: 0.1346549540758133\n",
      "batch: 8982, loss: 0.19076178967952728\n",
      "batch: 8983, loss: 0.13342046737670898\n",
      "batch: 8984, loss: 0.08625100553035736\n",
      "batch: 8985, loss: 0.19251595437526703\n",
      "batch: 8986, loss: 0.33716487884521484\n",
      "batch: 8987, loss: 0.09256395697593689\n",
      "batch: 8988, loss: 0.13031698763370514\n",
      "batch: 8989, loss: 0.1273857057094574\n",
      "batch: 8990, loss: 0.11369787156581879\n",
      "batch: 8991, loss: 0.15194843709468842\n",
      "batch: 8992, loss: 0.07207144051790237\n",
      "batch: 8993, loss: 0.12496635317802429\n",
      "batch: 8994, loss: 0.17529615759849548\n",
      "batch: 8995, loss: 0.13707274198532104\n",
      "batch: 8996, loss: 0.1583334505558014\n",
      "batch: 8997, loss: 0.15144555270671844\n",
      "batch: 8998, loss: 0.10541297495365143\n",
      "batch: 8999, loss: 0.15418732166290283\n",
      "batch: 9000, loss: 0.151607945561409\n",
      "model saved to ./saving/model.ckpt-90\n",
      "batch: 9001, loss: 0.15309001505374908\n",
      "batch: 9002, loss: 0.14484694600105286\n",
      "batch: 9003, loss: 0.14172083139419556\n",
      "batch: 9004, loss: 0.15394936501979828\n",
      "batch: 9005, loss: 0.14217281341552734\n",
      "batch: 9006, loss: 0.10920538753271103\n",
      "batch: 9007, loss: 0.17645367980003357\n",
      "batch: 9008, loss: 0.14640238881111145\n",
      "batch: 9009, loss: 0.18652527034282684\n",
      "batch: 9010, loss: 0.13423572480678558\n",
      "batch: 9011, loss: 0.15116050839424133\n",
      "batch: 9012, loss: 0.17814210057258606\n",
      "batch: 9013, loss: 0.14301958680152893\n",
      "batch: 9014, loss: 0.16580358147621155\n",
      "batch: 9015, loss: 0.16037225723266602\n",
      "batch: 9016, loss: 0.17274007201194763\n",
      "batch: 9017, loss: 0.175248920917511\n",
      "batch: 9018, loss: 0.17642147839069366\n",
      "batch: 9019, loss: 0.049743033945560455\n",
      "batch: 9020, loss: 0.13818752765655518\n",
      "batch: 9021, loss: 0.19074147939682007\n",
      "batch: 9022, loss: 0.11613845080137253\n",
      "batch: 9023, loss: 0.16485625505447388\n",
      "batch: 9024, loss: 0.039628226310014725\n",
      "batch: 9025, loss: 0.19568413496017456\n",
      "batch: 9026, loss: 0.2151590883731842\n",
      "batch: 9027, loss: 0.13726642727851868\n",
      "batch: 9028, loss: 0.19227156043052673\n",
      "batch: 9029, loss: 0.18508853018283844\n",
      "batch: 9030, loss: 0.2019854635000229\n",
      "batch: 9031, loss: 0.1733190417289734\n",
      "batch: 9032, loss: 0.21815018355846405\n",
      "batch: 9033, loss: 0.1475302278995514\n",
      "batch: 9034, loss: 0.10349418222904205\n",
      "batch: 9035, loss: 0.17619962990283966\n",
      "batch: 9036, loss: 0.08959493041038513\n",
      "batch: 9037, loss: 0.14916843175888062\n",
      "batch: 9038, loss: 0.15937316417694092\n",
      "batch: 9039, loss: 0.32656386494636536\n",
      "batch: 9040, loss: 0.1321725845336914\n",
      "batch: 9041, loss: 0.16233666241168976\n",
      "batch: 9042, loss: 0.09854817390441895\n",
      "batch: 9043, loss: 0.13229429721832275\n",
      "batch: 9044, loss: 0.14996479451656342\n",
      "batch: 9045, loss: 0.1793784499168396\n",
      "batch: 9046, loss: 0.06801890581846237\n",
      "batch: 9047, loss: 0.04225461930036545\n",
      "batch: 9048, loss: 0.24435481429100037\n",
      "batch: 9049, loss: 0.1174924373626709\n",
      "batch: 9050, loss: 0.07315845042467117\n",
      "batch: 9051, loss: 0.09874030947685242\n",
      "batch: 9052, loss: 0.3268175721168518\n",
      "batch: 9053, loss: 0.06818664073944092\n",
      "batch: 9054, loss: 0.2985655665397644\n",
      "batch: 9055, loss: 0.06583249568939209\n",
      "batch: 9056, loss: 0.14399468898773193\n",
      "batch: 9057, loss: 0.16724489629268646\n",
      "batch: 9058, loss: 0.10529929399490356\n",
      "batch: 9059, loss: 0.21718421578407288\n",
      "batch: 9060, loss: 0.2364690601825714\n",
      "batch: 9061, loss: 0.19537325203418732\n",
      "batch: 9062, loss: 0.11242082715034485\n",
      "batch: 9063, loss: 0.1399191915988922\n",
      "batch: 9064, loss: 0.08613643050193787\n",
      "batch: 9065, loss: 0.07461684197187424\n",
      "batch: 9066, loss: 0.08559912443161011\n",
      "batch: 9067, loss: 0.14137974381446838\n",
      "batch: 9068, loss: 0.16807082295417786\n",
      "batch: 9069, loss: 0.1530800759792328\n",
      "batch: 9070, loss: 0.14335045218467712\n",
      "batch: 9071, loss: 0.23324577510356903\n",
      "batch: 9072, loss: 0.185210719704628\n",
      "batch: 9073, loss: 0.12819904088974\n",
      "batch: 9074, loss: 0.1473628282546997\n",
      "batch: 9075, loss: 0.17029598355293274\n",
      "batch: 9076, loss: 0.1711227297782898\n",
      "batch: 9077, loss: 0.189195454120636\n",
      "batch: 9078, loss: 0.10770256817340851\n",
      "batch: 9079, loss: 0.07138243317604065\n",
      "batch: 9080, loss: 0.2434346079826355\n",
      "batch: 9081, loss: 0.303409218788147\n",
      "batch: 9082, loss: 0.1136549860239029\n",
      "batch: 9083, loss: 0.12216739356517792\n",
      "batch: 9084, loss: 0.12259924411773682\n",
      "batch: 9085, loss: 0.09103858470916748\n",
      "batch: 9086, loss: 0.1895749270915985\n",
      "batch: 9087, loss: 0.08759666979312897\n",
      "batch: 9088, loss: 0.32675257325172424\n",
      "batch: 9089, loss: 0.12900888919830322\n",
      "batch: 9090, loss: 0.20207691192626953\n",
      "batch: 9091, loss: 0.12638749182224274\n",
      "batch: 9092, loss: 0.25931644439697266\n",
      "batch: 9093, loss: 0.1665036976337433\n",
      "batch: 9094, loss: 0.12206962704658508\n",
      "batch: 9095, loss: 0.1704663336277008\n",
      "batch: 9096, loss: 0.06733076274394989\n",
      "batch: 9097, loss: 0.06743398308753967\n",
      "batch: 9098, loss: 0.262715607881546\n",
      "batch: 9099, loss: 0.11788180470466614\n",
      "batch: 9100, loss: 0.15135110914707184\n",
      "model saved to ./saving/model.ckpt-91\n",
      "batch: 9101, loss: 0.1890033334493637\n",
      "batch: 9102, loss: 0.18443959951400757\n",
      "batch: 9103, loss: 0.14418044686317444\n",
      "batch: 9104, loss: 0.11794733256101608\n",
      "batch: 9105, loss: 0.16724377870559692\n",
      "batch: 9106, loss: 0.16515925526618958\n",
      "batch: 9107, loss: 0.062438003718853\n",
      "batch: 9108, loss: 0.04964429885149002\n",
      "batch: 9109, loss: 0.31557726860046387\n",
      "batch: 9110, loss: 0.10358408838510513\n",
      "batch: 9111, loss: 0.17385894060134888\n",
      "batch: 9112, loss: 0.1301848292350769\n",
      "batch: 9113, loss: 0.1279313564300537\n",
      "batch: 9114, loss: 0.1292746365070343\n",
      "batch: 9115, loss: 0.2708069682121277\n",
      "batch: 9116, loss: 0.2235991209745407\n",
      "batch: 9117, loss: 0.1973952353000641\n",
      "batch: 9118, loss: 0.07247873395681381\n",
      "batch: 9119, loss: 0.15813903510570526\n",
      "batch: 9120, loss: 0.07622477412223816\n",
      "batch: 9121, loss: 0.14781492948532104\n",
      "batch: 9122, loss: 0.0636109933257103\n",
      "batch: 9123, loss: 0.1094730794429779\n",
      "batch: 9124, loss: 0.048813339322805405\n",
      "batch: 9125, loss: 0.13962726294994354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 9126, loss: 0.16058994829654694\n",
      "batch: 9127, loss: 0.20201480388641357\n",
      "batch: 9128, loss: 0.15544483065605164\n",
      "batch: 9129, loss: 0.3230113089084625\n",
      "batch: 9130, loss: 0.08718085289001465\n",
      "batch: 9131, loss: 0.254670113325119\n",
      "batch: 9132, loss: 0.08411428332328796\n",
      "batch: 9133, loss: 0.2365010678768158\n",
      "batch: 9134, loss: 0.10349135845899582\n",
      "batch: 9135, loss: 0.08160117268562317\n",
      "batch: 9136, loss: 0.12976935505867004\n",
      "batch: 9137, loss: 0.11535784602165222\n",
      "batch: 9138, loss: 0.09379411488771439\n",
      "batch: 9139, loss: 0.25349685549736023\n",
      "batch: 9140, loss: 0.12031954526901245\n",
      "batch: 9141, loss: 0.11890123784542084\n",
      "batch: 9142, loss: 0.17912590503692627\n",
      "batch: 9143, loss: 0.13560837507247925\n",
      "batch: 9144, loss: 0.13836988806724548\n",
      "batch: 9145, loss: 0.14534074068069458\n",
      "batch: 9146, loss: 0.11186602711677551\n",
      "batch: 9147, loss: 0.2708739638328552\n",
      "batch: 9148, loss: 0.16293179988861084\n",
      "batch: 9149, loss: 0.12934483587741852\n",
      "batch: 9150, loss: 0.1823008954524994\n",
      "batch: 9151, loss: 0.09201692789793015\n",
      "batch: 9152, loss: 0.06990706920623779\n",
      "batch: 9153, loss: 0.17814068496227264\n",
      "batch: 9154, loss: 0.18173542618751526\n",
      "batch: 9155, loss: 0.11317145824432373\n",
      "batch: 9156, loss: 0.17787253856658936\n",
      "batch: 9157, loss: 0.19819805026054382\n",
      "batch: 9158, loss: 0.08547469973564148\n",
      "batch: 9159, loss: 0.12603183090686798\n",
      "batch: 9160, loss: 0.10045179724693298\n",
      "batch: 9161, loss: 0.12268935143947601\n",
      "batch: 9162, loss: 0.10883336514234543\n",
      "batch: 9163, loss: 0.16741254925727844\n",
      "batch: 9164, loss: 0.12074797600507736\n",
      "batch: 9165, loss: 0.12107893824577332\n",
      "batch: 9166, loss: 0.06261925399303436\n",
      "batch: 9167, loss: 0.5780807137489319\n",
      "batch: 9168, loss: 0.19983062148094177\n",
      "batch: 9169, loss: 0.09212519228458405\n",
      "batch: 9170, loss: 0.12793010473251343\n",
      "batch: 9171, loss: 0.13555467128753662\n",
      "batch: 9172, loss: 0.23879346251487732\n",
      "batch: 9173, loss: 0.08276557177305222\n",
      "batch: 9174, loss: 0.26931965351104736\n",
      "batch: 9175, loss: 0.0898284837603569\n",
      "batch: 9176, loss: 0.12121916562318802\n",
      "batch: 9177, loss: 0.05663323402404785\n",
      "batch: 9178, loss: 0.25992822647094727\n",
      "batch: 9179, loss: 0.1067541167140007\n",
      "batch: 9180, loss: 0.13684222102165222\n",
      "batch: 9181, loss: 0.1605902761220932\n",
      "batch: 9182, loss: 0.1322716325521469\n",
      "batch: 9183, loss: 0.14988771080970764\n",
      "batch: 9184, loss: 0.0642680898308754\n",
      "batch: 9185, loss: 0.07586197555065155\n",
      "batch: 9186, loss: 0.152977854013443\n",
      "batch: 9187, loss: 0.09196986258029938\n",
      "batch: 9188, loss: 0.15168574452400208\n",
      "batch: 9189, loss: 0.24666669964790344\n",
      "batch: 9190, loss: 0.1267259418964386\n",
      "batch: 9191, loss: 0.2114376723766327\n",
      "batch: 9192, loss: 0.1426829695701599\n",
      "batch: 9193, loss: 0.09961225092411041\n",
      "batch: 9194, loss: 0.22528432309627533\n",
      "batch: 9195, loss: 0.13828736543655396\n",
      "batch: 9196, loss: 0.1260165572166443\n",
      "batch: 9197, loss: 0.07967551052570343\n",
      "batch: 9198, loss: 0.2606540620326996\n",
      "batch: 9199, loss: 0.04597466439008713\n",
      "batch: 9200, loss: 0.16273997724056244\n",
      "model saved to ./saving/model.ckpt-92\n",
      "batch: 9201, loss: 0.28076064586639404\n",
      "batch: 9202, loss: 0.07042890787124634\n",
      "batch: 9203, loss: 0.23285675048828125\n",
      "batch: 9204, loss: 0.34613674879074097\n",
      "batch: 9205, loss: 0.15130308270454407\n",
      "batch: 9206, loss: 0.10170330107212067\n",
      "batch: 9207, loss: 0.19268977642059326\n",
      "batch: 9208, loss: 0.1289602518081665\n",
      "batch: 9209, loss: 0.14274513721466064\n",
      "batch: 9210, loss: 0.1248069554567337\n",
      "batch: 9211, loss: 0.12371490895748138\n",
      "batch: 9212, loss: 0.07677255570888519\n",
      "batch: 9213, loss: 0.12239141762256622\n",
      "batch: 9214, loss: 0.09327754378318787\n",
      "batch: 9215, loss: 0.12749168276786804\n",
      "batch: 9216, loss: 0.07507334649562836\n",
      "batch: 9217, loss: 0.31776219606399536\n",
      "batch: 9218, loss: 0.12873558700084686\n",
      "batch: 9219, loss: 0.17408055067062378\n",
      "batch: 9220, loss: 0.15960869193077087\n",
      "batch: 9221, loss: 0.2703481614589691\n",
      "batch: 9222, loss: 0.0835767388343811\n",
      "batch: 9223, loss: 0.10677797347307205\n",
      "batch: 9224, loss: 0.059347301721572876\n",
      "batch: 9225, loss: 0.29286685585975647\n",
      "batch: 9226, loss: 0.277127742767334\n",
      "batch: 9227, loss: 0.2017696350812912\n",
      "batch: 9228, loss: 0.11611522734165192\n",
      "batch: 9229, loss: 0.16190585494041443\n",
      "batch: 9230, loss: 0.12488757818937302\n",
      "batch: 9231, loss: 0.21022763848304749\n",
      "batch: 9232, loss: 0.1904345154762268\n",
      "batch: 9233, loss: 0.1289450228214264\n",
      "batch: 9234, loss: 0.09651331603527069\n",
      "batch: 9235, loss: 0.1804661750793457\n",
      "batch: 9236, loss: 0.12873920798301697\n",
      "batch: 9237, loss: 0.14066292345523834\n",
      "batch: 9238, loss: 0.09959321469068527\n",
      "batch: 9239, loss: 0.29869863390922546\n",
      "batch: 9240, loss: 0.20619383454322815\n",
      "batch: 9241, loss: 0.23447130620479584\n",
      "batch: 9242, loss: 0.06727168709039688\n",
      "batch: 9243, loss: 0.11625489592552185\n",
      "batch: 9244, loss: 0.12657923996448517\n",
      "batch: 9245, loss: 0.2877747118473053\n",
      "batch: 9246, loss: 0.05162930116057396\n",
      "batch: 9247, loss: 0.1073637381196022\n",
      "batch: 9248, loss: 0.25689423084259033\n",
      "batch: 9249, loss: 0.1584661453962326\n",
      "batch: 9250, loss: 0.23711220920085907\n",
      "batch: 9251, loss: 0.24931800365447998\n",
      "batch: 9252, loss: 0.13217194378376007\n",
      "batch: 9253, loss: 0.20140522718429565\n",
      "batch: 9254, loss: 0.08457554131746292\n",
      "batch: 9255, loss: 0.18459190428256989\n",
      "batch: 9256, loss: 0.12210601568222046\n",
      "batch: 9257, loss: 0.08783622086048126\n",
      "batch: 9258, loss: 0.1896142214536667\n",
      "batch: 9259, loss: 0.2003249228000641\n",
      "batch: 9260, loss: 0.15186360478401184\n",
      "batch: 9261, loss: 0.11950056999921799\n",
      "batch: 9262, loss: 0.12965962290763855\n",
      "batch: 9263, loss: 0.18249213695526123\n",
      "batch: 9264, loss: 0.1208280473947525\n",
      "batch: 9265, loss: 0.10859504342079163\n",
      "batch: 9266, loss: 0.19232989847660065\n",
      "batch: 9267, loss: 0.13685737550258636\n",
      "batch: 9268, loss: 0.20989134907722473\n",
      "batch: 9269, loss: 0.14581415057182312\n",
      "batch: 9270, loss: 0.2020636796951294\n",
      "batch: 9271, loss: 0.1494976282119751\n",
      "batch: 9272, loss: 0.09010796248912811\n",
      "batch: 9273, loss: 0.15117305517196655\n",
      "batch: 9274, loss: 0.11830557882785797\n",
      "batch: 9275, loss: 0.12729065120220184\n",
      "batch: 9276, loss: 0.1272096335887909\n",
      "batch: 9277, loss: 0.2405664026737213\n",
      "batch: 9278, loss: 0.10282158851623535\n",
      "batch: 9279, loss: 0.17318791151046753\n",
      "batch: 9280, loss: 0.18376289308071136\n",
      "batch: 9281, loss: 0.09732060134410858\n",
      "batch: 9282, loss: 0.19866108894348145\n",
      "batch: 9283, loss: 0.10022760182619095\n",
      "batch: 9284, loss: 0.08357670903205872\n",
      "batch: 9285, loss: 0.27735570073127747\n",
      "batch: 9286, loss: 0.16641227900981903\n",
      "batch: 9287, loss: 0.07702422142028809\n",
      "batch: 9288, loss: 0.09204137325286865\n",
      "batch: 9289, loss: 0.20358732342720032\n",
      "batch: 9290, loss: 0.13040095567703247\n",
      "batch: 9291, loss: 0.10566338151693344\n",
      "batch: 9292, loss: 0.10346460342407227\n",
      "batch: 9293, loss: 0.31402653455734253\n",
      "batch: 9294, loss: 0.3741070330142975\n",
      "batch: 9295, loss: 0.07831177115440369\n",
      "batch: 9296, loss: 0.16106382012367249\n",
      "batch: 9297, loss: 0.11678826808929443\n",
      "batch: 9298, loss: 0.07428216934204102\n",
      "batch: 9299, loss: 0.15593694150447845\n",
      "batch: 9300, loss: 0.0939817801117897\n",
      "model saved to ./saving/model.ckpt-93\n",
      "batch: 9301, loss: 0.14222434163093567\n",
      "batch: 9302, loss: 0.21615394949913025\n",
      "batch: 9303, loss: 0.056538987904787064\n",
      "batch: 9304, loss: 0.24237263202667236\n",
      "batch: 9305, loss: 0.140700563788414\n",
      "batch: 9306, loss: 0.13187697529792786\n",
      "batch: 9307, loss: 0.2633206248283386\n",
      "batch: 9308, loss: 0.15400227904319763\n",
      "batch: 9309, loss: 0.17177647352218628\n",
      "batch: 9310, loss: 0.10302944481372833\n",
      "batch: 9311, loss: 0.1603463888168335\n",
      "batch: 9312, loss: 0.14506056904792786\n",
      "batch: 9313, loss: 0.1947857141494751\n",
      "batch: 9314, loss: 0.07637473195791245\n",
      "batch: 9315, loss: 0.1268431842327118\n",
      "batch: 9316, loss: 0.2366272211074829\n",
      "batch: 9317, loss: 0.1983519196510315\n",
      "batch: 9318, loss: 0.11416555941104889\n",
      "batch: 9319, loss: 0.10323748737573624\n",
      "batch: 9320, loss: 0.1263013482093811\n",
      "batch: 9321, loss: 0.10802409797906876\n",
      "batch: 9322, loss: 0.04998459294438362\n",
      "batch: 9323, loss: 0.13873350620269775\n",
      "batch: 9324, loss: 0.21669676899909973\n",
      "batch: 9325, loss: 0.19473522901535034\n",
      "batch: 9326, loss: 0.13885772228240967\n",
      "batch: 9327, loss: 0.23832115530967712\n",
      "batch: 9328, loss: 0.08668086677789688\n",
      "batch: 9329, loss: 0.22804224491119385\n",
      "batch: 9330, loss: 0.17500530183315277\n",
      "batch: 9331, loss: 0.12705731391906738\n",
      "batch: 9332, loss: 0.06979446113109589\n",
      "batch: 9333, loss: 0.07683908939361572\n",
      "batch: 9334, loss: 0.09694863855838776\n",
      "batch: 9335, loss: 0.07155990600585938\n",
      "batch: 9336, loss: 0.11479292809963226\n",
      "batch: 9337, loss: 0.18353793025016785\n",
      "batch: 9338, loss: 0.15775106847286224\n",
      "batch: 9339, loss: 0.08023244142532349\n",
      "batch: 9340, loss: 0.21794843673706055\n",
      "batch: 9341, loss: 0.23877760767936707\n",
      "batch: 9342, loss: 0.20049172639846802\n",
      "batch: 9343, loss: 0.5458266139030457\n",
      "batch: 9344, loss: 0.08101138472557068\n",
      "batch: 9345, loss: 0.1238141804933548\n",
      "batch: 9346, loss: 0.09552586823701859\n",
      "batch: 9347, loss: 0.07784722745418549\n",
      "batch: 9348, loss: 0.12816767394542694\n",
      "batch: 9349, loss: 0.16412441432476044\n",
      "batch: 9350, loss: 0.05389395356178284\n",
      "batch: 9351, loss: 0.1540077030658722\n",
      "batch: 9352, loss: 0.20473936200141907\n",
      "batch: 9353, loss: 0.09981601685285568\n",
      "batch: 9354, loss: 0.0798226147890091\n",
      "batch: 9355, loss: 0.10719557851552963\n",
      "batch: 9356, loss: 0.08881162106990814\n",
      "batch: 9357, loss: 0.2298247218132019\n",
      "batch: 9358, loss: 0.21810901165008545\n",
      "batch: 9359, loss: 0.1219361275434494\n",
      "batch: 9360, loss: 0.19055816531181335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 9361, loss: 0.14279583096504211\n",
      "batch: 9362, loss: 0.16544407606124878\n",
      "batch: 9363, loss: 0.24441850185394287\n",
      "batch: 9364, loss: 0.1467534452676773\n",
      "batch: 9365, loss: 0.03631596267223358\n",
      "batch: 9366, loss: 0.16273604333400726\n",
      "batch: 9367, loss: 0.15271678566932678\n",
      "batch: 9368, loss: 0.17863991856575012\n",
      "batch: 9369, loss: 0.08230610936880112\n",
      "batch: 9370, loss: 0.12164433300495148\n",
      "batch: 9371, loss: 0.12817133963108063\n",
      "batch: 9372, loss: 0.09402438998222351\n",
      "batch: 9373, loss: 0.20758189260959625\n",
      "batch: 9374, loss: 0.15443220734596252\n",
      "batch: 9375, loss: 0.2063179612159729\n",
      "batch: 9376, loss: 0.2143544852733612\n",
      "batch: 9377, loss: 0.18492914736270905\n",
      "batch: 9378, loss: 0.10660666227340698\n",
      "batch: 9379, loss: 0.16294875741004944\n",
      "batch: 9380, loss: 0.13827966153621674\n",
      "batch: 9381, loss: 0.09948704391717911\n",
      "batch: 9382, loss: 0.08517104387283325\n",
      "batch: 9383, loss: 0.07437457889318466\n",
      "batch: 9384, loss: 0.14912578463554382\n",
      "batch: 9385, loss: 0.12091123312711716\n",
      "batch: 9386, loss: 0.32451149821281433\n",
      "batch: 9387, loss: 0.13762801885604858\n",
      "batch: 9388, loss: 0.12035605311393738\n",
      "batch: 9389, loss: 0.10600414872169495\n",
      "batch: 9390, loss: 0.1122724786400795\n",
      "batch: 9391, loss: 0.23198513686656952\n",
      "batch: 9392, loss: 0.26645180583000183\n",
      "batch: 9393, loss: 0.09154137969017029\n",
      "batch: 9394, loss: 0.12661710381507874\n",
      "batch: 9395, loss: 0.27498728036880493\n",
      "batch: 9396, loss: 0.24419385194778442\n",
      "batch: 9397, loss: 0.12945333123207092\n",
      "batch: 9398, loss: 0.15904581546783447\n",
      "batch: 9399, loss: 0.27433180809020996\n",
      "batch: 9400, loss: 0.2288171947002411\n",
      "model saved to ./saving/model.ckpt-94\n",
      "batch: 9401, loss: 0.10079287737607956\n",
      "batch: 9402, loss: 0.19878047704696655\n",
      "batch: 9403, loss: 0.18099641799926758\n",
      "batch: 9404, loss: 0.06686127185821533\n",
      "batch: 9405, loss: 0.23172853887081146\n",
      "batch: 9406, loss: 0.2044561505317688\n",
      "batch: 9407, loss: 0.18464435636997223\n",
      "batch: 9408, loss: 0.15496769547462463\n",
      "batch: 9409, loss: 0.08009380102157593\n",
      "batch: 9410, loss: 0.20808564126491547\n",
      "batch: 9411, loss: 0.13168179988861084\n",
      "batch: 9412, loss: 0.15778854489326477\n",
      "batch: 9413, loss: 0.09173780679702759\n",
      "batch: 9414, loss: 0.21830801665782928\n",
      "batch: 9415, loss: 0.25308194756507874\n",
      "batch: 9416, loss: 0.14401960372924805\n",
      "batch: 9417, loss: 0.1855628341436386\n",
      "batch: 9418, loss: 0.13234615325927734\n",
      "batch: 9419, loss: 0.21954180300235748\n",
      "batch: 9420, loss: 0.20210608839988708\n",
      "batch: 9421, loss: 0.10243792086839676\n",
      "batch: 9422, loss: 0.24437543749809265\n",
      "batch: 9423, loss: 0.1152334213256836\n",
      "batch: 9424, loss: 0.06402791291475296\n",
      "batch: 9425, loss: 0.12267708778381348\n",
      "batch: 9426, loss: 0.11344031989574432\n",
      "batch: 9427, loss: 0.0769965797662735\n",
      "batch: 9428, loss: 0.130813866853714\n",
      "batch: 9429, loss: 0.13391292095184326\n",
      "batch: 9430, loss: 0.10980873554944992\n",
      "batch: 9431, loss: 0.16491565108299255\n",
      "batch: 9432, loss: 0.18967390060424805\n",
      "batch: 9433, loss: 0.13698910176753998\n",
      "batch: 9434, loss: 0.1819431185722351\n",
      "batch: 9435, loss: 0.1215105801820755\n",
      "batch: 9436, loss: 0.1530877649784088\n",
      "batch: 9437, loss: 0.06876130402088165\n",
      "batch: 9438, loss: 0.1670619249343872\n",
      "batch: 9439, loss: 0.1799941062927246\n",
      "batch: 9440, loss: 0.17356102168560028\n",
      "batch: 9441, loss: 0.12538960576057434\n",
      "batch: 9442, loss: 0.37485364079475403\n",
      "batch: 9443, loss: 0.15305575728416443\n",
      "batch: 9444, loss: 0.19919821619987488\n",
      "batch: 9445, loss: 0.04608248919248581\n",
      "batch: 9446, loss: 0.15382203459739685\n",
      "batch: 9447, loss: 0.09878939390182495\n",
      "batch: 9448, loss: 0.1815265566110611\n",
      "batch: 9449, loss: 0.10687564313411713\n",
      "batch: 9450, loss: 0.2701074481010437\n",
      "batch: 9451, loss: 0.09803806245326996\n",
      "batch: 9452, loss: 0.10809168219566345\n",
      "batch: 9453, loss: 0.21289336681365967\n",
      "batch: 9454, loss: 0.22434452176094055\n",
      "batch: 9455, loss: 0.19881035387516022\n",
      "batch: 9456, loss: 0.09979771077632904\n",
      "batch: 9457, loss: 0.2845057249069214\n",
      "batch: 9458, loss: 0.15355463325977325\n",
      "batch: 9459, loss: 0.08950452506542206\n",
      "batch: 9460, loss: 0.2963044345378876\n",
      "batch: 9461, loss: 0.2863653600215912\n",
      "batch: 9462, loss: 0.19613835215568542\n",
      "batch: 9463, loss: 0.12709222733974457\n",
      "batch: 9464, loss: 0.11542932689189911\n",
      "batch: 9465, loss: 0.0724264457821846\n",
      "batch: 9466, loss: 0.15050256252288818\n",
      "batch: 9467, loss: 0.1165008693933487\n",
      "batch: 9468, loss: 0.1501338928937912\n",
      "batch: 9469, loss: 0.106963150203228\n",
      "batch: 9470, loss: 0.23915500938892365\n",
      "batch: 9471, loss: 0.2761734127998352\n",
      "batch: 9472, loss: 0.17814981937408447\n",
      "batch: 9473, loss: 0.08147728443145752\n",
      "batch: 9474, loss: 0.09047304838895798\n",
      "batch: 9475, loss: 0.1551487296819687\n",
      "batch: 9476, loss: 0.2619299292564392\n",
      "batch: 9477, loss: 0.07282005250453949\n",
      "batch: 9478, loss: 0.09885036945343018\n",
      "batch: 9479, loss: 0.2772417664527893\n",
      "batch: 9480, loss: 0.11043380945920944\n",
      "batch: 9481, loss: 0.3245263695716858\n",
      "batch: 9482, loss: 0.3243623673915863\n",
      "batch: 9483, loss: 0.07510723173618317\n",
      "batch: 9484, loss: 0.0882311463356018\n",
      "batch: 9485, loss: 0.09766937047243118\n",
      "batch: 9486, loss: 0.09224866330623627\n",
      "batch: 9487, loss: 0.06607261300086975\n",
      "batch: 9488, loss: 0.06481729447841644\n",
      "batch: 9489, loss: 0.20131254196166992\n",
      "batch: 9490, loss: 0.07514210045337677\n",
      "batch: 9491, loss: 0.11560520529747009\n",
      "batch: 9492, loss: 0.20977358520030975\n",
      "batch: 9493, loss: 0.08878590166568756\n",
      "batch: 9494, loss: 0.08144159615039825\n",
      "batch: 9495, loss: 0.19487787783145905\n",
      "batch: 9496, loss: 0.1707400381565094\n",
      "batch: 9497, loss: 0.09952640533447266\n",
      "batch: 9498, loss: 0.2563270926475525\n",
      "batch: 9499, loss: 0.04553378373384476\n",
      "batch: 9500, loss: 0.13124996423721313\n",
      "model saved to ./saving/model.ckpt-95\n",
      "batch: 9501, loss: 0.12156867980957031\n",
      "batch: 9502, loss: 0.12890300154685974\n",
      "batch: 9503, loss: 0.31471383571624756\n",
      "batch: 9504, loss: 0.20209214091300964\n",
      "batch: 9505, loss: 0.1403736174106598\n",
      "batch: 9506, loss: 0.18824850022792816\n",
      "batch: 9507, loss: 0.10416561365127563\n",
      "batch: 9508, loss: 0.17376556992530823\n",
      "batch: 9509, loss: 0.21149717271327972\n",
      "batch: 9510, loss: 0.28298717737197876\n",
      "batch: 9511, loss: 0.1033264547586441\n",
      "batch: 9512, loss: 0.18350031971931458\n",
      "batch: 9513, loss: 0.11155631393194199\n",
      "batch: 9514, loss: 0.13266295194625854\n",
      "batch: 9515, loss: 0.08334223181009293\n",
      "batch: 9516, loss: 0.10235095769166946\n",
      "batch: 9517, loss: 0.097110316157341\n",
      "batch: 9518, loss: 0.1051449179649353\n",
      "batch: 9519, loss: 0.06947662681341171\n",
      "batch: 9520, loss: 0.0788513720035553\n",
      "batch: 9521, loss: 0.2874937057495117\n",
      "batch: 9522, loss: 0.12841135263442993\n",
      "batch: 9523, loss: 0.1300991177558899\n",
      "batch: 9524, loss: 0.1635025590658188\n",
      "batch: 9525, loss: 0.06973596662282944\n",
      "batch: 9526, loss: 0.13551968336105347\n",
      "batch: 9527, loss: 0.08750826865434647\n",
      "batch: 9528, loss: 0.1355161964893341\n",
      "batch: 9529, loss: 0.07664652168750763\n",
      "batch: 9530, loss: 0.07698425650596619\n",
      "batch: 9531, loss: 0.24787743389606476\n",
      "batch: 9532, loss: 0.12803159654140472\n",
      "batch: 9533, loss: 0.07727017998695374\n",
      "batch: 9534, loss: 0.2448311448097229\n",
      "batch: 9535, loss: 0.18368247151374817\n",
      "batch: 9536, loss: 0.12728139758110046\n",
      "batch: 9537, loss: 0.10107138752937317\n",
      "batch: 9538, loss: 0.06450921297073364\n",
      "batch: 9539, loss: 0.2018222212791443\n",
      "batch: 9540, loss: 0.09766517579555511\n",
      "batch: 9541, loss: 0.23455087840557098\n",
      "batch: 9542, loss: 0.0917026549577713\n",
      "batch: 9543, loss: 0.06273115426301956\n",
      "batch: 9544, loss: 0.15353329479694366\n",
      "batch: 9545, loss: 0.10518350452184677\n",
      "batch: 9546, loss: 0.09072376787662506\n",
      "batch: 9547, loss: 0.0826721042394638\n",
      "batch: 9548, loss: 0.07986035943031311\n",
      "batch: 9549, loss: 0.06376584619283676\n",
      "batch: 9550, loss: 0.17078956961631775\n",
      "batch: 9551, loss: 0.08196881413459778\n",
      "batch: 9552, loss: 0.07509790360927582\n",
      "batch: 9553, loss: 0.09325608611106873\n",
      "batch: 9554, loss: 0.0896378606557846\n",
      "batch: 9555, loss: 0.13305409252643585\n",
      "batch: 9556, loss: 0.13789215683937073\n",
      "batch: 9557, loss: 0.21065667271614075\n",
      "batch: 9558, loss: 0.11104753613471985\n",
      "batch: 9559, loss: 0.18781667947769165\n",
      "batch: 9560, loss: 0.1677381992340088\n",
      "batch: 9561, loss: 0.08627775311470032\n",
      "batch: 9562, loss: 0.11620442569255829\n",
      "batch: 9563, loss: 0.20662754774093628\n",
      "batch: 9564, loss: 0.12811486423015594\n",
      "batch: 9565, loss: 0.12599869072437286\n",
      "batch: 9566, loss: 0.1576022505760193\n",
      "batch: 9567, loss: 0.09953904151916504\n",
      "batch: 9568, loss: 0.11093562096357346\n",
      "batch: 9569, loss: 0.16788655519485474\n",
      "batch: 9570, loss: 0.15177267789840698\n",
      "batch: 9571, loss: 0.1026378646492958\n",
      "batch: 9572, loss: 0.2735722064971924\n",
      "batch: 9573, loss: 0.07823668420314789\n",
      "batch: 9574, loss: 0.12329007685184479\n",
      "batch: 9575, loss: 0.1694643497467041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 9576, loss: 0.14872407913208008\n",
      "batch: 9577, loss: 0.09719207137823105\n",
      "batch: 9578, loss: 0.1277737021446228\n",
      "batch: 9579, loss: 0.057829007506370544\n",
      "batch: 9580, loss: 0.17125359177589417\n",
      "batch: 9581, loss: 0.243657186627388\n",
      "batch: 9582, loss: 0.24932771921157837\n",
      "batch: 9583, loss: 0.08932189643383026\n",
      "batch: 9584, loss: 0.10451137274503708\n",
      "batch: 9585, loss: 0.10728190094232559\n",
      "batch: 9586, loss: 0.08213580399751663\n",
      "batch: 9587, loss: 0.15227575600147247\n",
      "batch: 9588, loss: 0.11505108326673508\n",
      "batch: 9589, loss: 0.08929052203893661\n",
      "batch: 9590, loss: 0.13709738850593567\n",
      "batch: 9591, loss: 0.10548703372478485\n",
      "batch: 9592, loss: 0.13054603338241577\n",
      "batch: 9593, loss: 0.26669949293136597\n",
      "batch: 9594, loss: 0.10403650999069214\n",
      "batch: 9595, loss: 0.0966949313879013\n",
      "batch: 9596, loss: 0.0686522126197815\n",
      "batch: 9597, loss: 0.18782852590084076\n",
      "batch: 9598, loss: 0.05414235591888428\n",
      "batch: 9599, loss: 0.13489887118339539\n",
      "batch: 9600, loss: 0.15456950664520264\n",
      "model saved to ./saving/model.ckpt-96\n",
      "batch: 9601, loss: 0.04922522231936455\n",
      "batch: 9602, loss: 0.0586588978767395\n",
      "batch: 9603, loss: 0.22791485488414764\n",
      "batch: 9604, loss: 0.09689442813396454\n",
      "batch: 9605, loss: 0.15080450475215912\n",
      "batch: 9606, loss: 0.16795936226844788\n",
      "batch: 9607, loss: 0.07311375439167023\n",
      "batch: 9608, loss: 0.15641412138938904\n",
      "batch: 9609, loss: 0.08943288028240204\n",
      "batch: 9610, loss: 0.16487199068069458\n",
      "batch: 9611, loss: 0.041332144290208817\n",
      "batch: 9612, loss: 0.11597192287445068\n",
      "batch: 9613, loss: 0.19430261850357056\n",
      "batch: 9614, loss: 0.07009224593639374\n",
      "batch: 9615, loss: 0.13959257304668427\n",
      "batch: 9616, loss: 0.21059979498386383\n",
      "batch: 9617, loss: 0.1947852522134781\n",
      "batch: 9618, loss: 0.15027156472206116\n",
      "batch: 9619, loss: 0.2052886188030243\n",
      "batch: 9620, loss: 0.14890378713607788\n",
      "batch: 9621, loss: 0.1570184975862503\n",
      "batch: 9622, loss: 0.16364333033561707\n",
      "batch: 9623, loss: 0.16826488077640533\n",
      "batch: 9624, loss: 0.13830441236495972\n",
      "batch: 9625, loss: 0.1337159276008606\n",
      "batch: 9626, loss: 0.1177508682012558\n",
      "batch: 9627, loss: 0.1501597762107849\n",
      "batch: 9628, loss: 0.16575190424919128\n",
      "batch: 9629, loss: 0.07312863320112228\n",
      "batch: 9630, loss: 0.14639407396316528\n",
      "batch: 9631, loss: 0.12420639395713806\n",
      "batch: 9632, loss: 0.1628992259502411\n",
      "batch: 9633, loss: 0.17572054266929626\n",
      "batch: 9634, loss: 0.2123115360736847\n",
      "batch: 9635, loss: 0.09215988218784332\n",
      "batch: 9636, loss: 0.06848782300949097\n",
      "batch: 9637, loss: 0.09648570418357849\n",
      "batch: 9638, loss: 0.17477703094482422\n",
      "batch: 9639, loss: 0.17349015176296234\n",
      "batch: 9640, loss: 0.09085497260093689\n",
      "batch: 9641, loss: 0.26177090406417847\n",
      "batch: 9642, loss: 0.3393436670303345\n",
      "batch: 9643, loss: 0.10271693766117096\n",
      "batch: 9644, loss: 0.1425701230764389\n",
      "batch: 9645, loss: 0.14504429697990417\n",
      "batch: 9646, loss: 0.1520688831806183\n",
      "batch: 9647, loss: 0.16244255006313324\n",
      "batch: 9648, loss: 0.1090581938624382\n",
      "batch: 9649, loss: 0.11085492372512817\n",
      "batch: 9650, loss: 0.08735449612140656\n",
      "batch: 9651, loss: 0.0847206860780716\n",
      "batch: 9652, loss: 0.08536091446876526\n",
      "batch: 9653, loss: 0.2306821048259735\n",
      "batch: 9654, loss: 0.09009887278079987\n",
      "batch: 9655, loss: 0.20942100882530212\n",
      "batch: 9656, loss: 0.12669473886489868\n",
      "batch: 9657, loss: 0.16653189063072205\n",
      "batch: 9658, loss: 0.12574101984500885\n",
      "batch: 9659, loss: 0.08419820666313171\n",
      "batch: 9660, loss: 0.13063672184944153\n",
      "batch: 9661, loss: 0.0534973219037056\n",
      "batch: 9662, loss: 0.1121831089258194\n",
      "batch: 9663, loss: 0.1246340200304985\n",
      "batch: 9664, loss: 0.23666223883628845\n",
      "batch: 9665, loss: 0.08591704815626144\n",
      "batch: 9666, loss: 0.1350896656513214\n",
      "batch: 9667, loss: 0.14872202277183533\n",
      "batch: 9668, loss: 0.10542982816696167\n",
      "batch: 9669, loss: 0.12364070862531662\n",
      "batch: 9670, loss: 0.18058627843856812\n",
      "batch: 9671, loss: 0.19291847944259644\n",
      "batch: 9672, loss: 0.12254585325717926\n",
      "batch: 9673, loss: 0.06362877041101456\n",
      "batch: 9674, loss: 0.11377427726984024\n",
      "batch: 9675, loss: 0.10766582190990448\n",
      "batch: 9676, loss: 0.307639479637146\n",
      "batch: 9677, loss: 0.0937400758266449\n",
      "batch: 9678, loss: 0.17229361832141876\n",
      "batch: 9679, loss: 0.167547345161438\n",
      "batch: 9680, loss: 0.35391008853912354\n",
      "batch: 9681, loss: 0.16190719604492188\n",
      "batch: 9682, loss: 0.2636434733867645\n",
      "batch: 9683, loss: 0.24313056468963623\n",
      "batch: 9684, loss: 0.21777057647705078\n",
      "batch: 9685, loss: 0.17189767956733704\n",
      "batch: 9686, loss: 0.13638225197792053\n",
      "batch: 9687, loss: 0.2696862518787384\n",
      "batch: 9688, loss: 0.2909538745880127\n",
      "batch: 9689, loss: 0.0791197121143341\n",
      "batch: 9690, loss: 0.1803259253501892\n",
      "batch: 9691, loss: 0.050270624458789825\n",
      "batch: 9692, loss: 0.10250994563102722\n",
      "batch: 9693, loss: 0.14444081485271454\n",
      "batch: 9694, loss: 0.09396474808454514\n",
      "batch: 9695, loss: 0.08528946340084076\n",
      "batch: 9696, loss: 0.2215728461742401\n",
      "batch: 9697, loss: 0.10404036194086075\n",
      "batch: 9698, loss: 0.12573879957199097\n",
      "batch: 9699, loss: 0.16490894556045532\n",
      "batch: 9700, loss: 0.11473865807056427\n",
      "model saved to ./saving/model.ckpt-97\n",
      "batch: 9701, loss: 0.26010099053382874\n",
      "batch: 9702, loss: 0.24630209803581238\n",
      "batch: 9703, loss: 0.17159856855869293\n",
      "batch: 9704, loss: 0.11328677833080292\n",
      "batch: 9705, loss: 0.15685221552848816\n",
      "batch: 9706, loss: 0.1256108283996582\n",
      "batch: 9707, loss: 0.20893120765686035\n",
      "batch: 9708, loss: 0.17338620126247406\n",
      "batch: 9709, loss: 0.13003486394882202\n",
      "batch: 9710, loss: 0.1477554440498352\n",
      "batch: 9711, loss: 0.1671726256608963\n",
      "batch: 9712, loss: 0.07984577864408493\n",
      "batch: 9713, loss: 0.10262596607208252\n",
      "batch: 9714, loss: 0.12066484242677689\n",
      "batch: 9715, loss: 0.061988700181245804\n",
      "batch: 9716, loss: 0.0971616804599762\n",
      "batch: 9717, loss: 0.08521304279565811\n",
      "batch: 9718, loss: 0.09352609515190125\n",
      "batch: 9719, loss: 0.10754126310348511\n",
      "batch: 9720, loss: 0.09356272220611572\n",
      "batch: 9721, loss: 0.05807798355817795\n",
      "batch: 9722, loss: 0.15313029289245605\n",
      "batch: 9723, loss: 0.09123553335666656\n",
      "batch: 9724, loss: 0.0819879025220871\n",
      "batch: 9725, loss: 0.14668205380439758\n",
      "batch: 9726, loss: 0.12369533628225327\n",
      "batch: 9727, loss: 0.35072046518325806\n",
      "batch: 9728, loss: 0.04538246989250183\n",
      "batch: 9729, loss: 0.12894120812416077\n",
      "batch: 9730, loss: 0.08887758105993271\n",
      "batch: 9731, loss: 0.1217414140701294\n",
      "batch: 9732, loss: 0.10957841575145721\n",
      "batch: 9733, loss: 0.06268186867237091\n",
      "batch: 9734, loss: 0.14487288892269135\n",
      "batch: 9735, loss: 0.09803566336631775\n",
      "batch: 9736, loss: 0.08772735297679901\n",
      "batch: 9737, loss: 0.203474760055542\n",
      "batch: 9738, loss: 0.1755443513393402\n",
      "batch: 9739, loss: 0.3524931073188782\n",
      "batch: 9740, loss: 0.10484001040458679\n",
      "batch: 9741, loss: 0.15062765777111053\n",
      "batch: 9742, loss: 0.1285034567117691\n",
      "batch: 9743, loss: 0.07489512115716934\n",
      "batch: 9744, loss: 0.16285669803619385\n",
      "batch: 9745, loss: 0.15293318033218384\n",
      "batch: 9746, loss: 0.16379036009311676\n",
      "batch: 9747, loss: 0.08963017910718918\n",
      "batch: 9748, loss: 0.21441921591758728\n",
      "batch: 9749, loss: 0.1786319613456726\n",
      "batch: 9750, loss: 0.12086009979248047\n",
      "batch: 9751, loss: 0.1358090043067932\n",
      "batch: 9752, loss: 0.18854494392871857\n",
      "batch: 9753, loss: 0.08816428482532501\n",
      "batch: 9754, loss: 0.24783730506896973\n",
      "batch: 9755, loss: 0.09953495860099792\n",
      "batch: 9756, loss: 0.08574491739273071\n",
      "batch: 9757, loss: 0.08139315247535706\n",
      "batch: 9758, loss: 0.42305806279182434\n",
      "batch: 9759, loss: 0.1014324352145195\n",
      "batch: 9760, loss: 0.19941949844360352\n",
      "batch: 9761, loss: 0.07300825417041779\n",
      "batch: 9762, loss: 0.18121284246444702\n",
      "batch: 9763, loss: 0.20330515503883362\n",
      "batch: 9764, loss: 0.10294438898563385\n",
      "batch: 9765, loss: 0.06682899594306946\n",
      "batch: 9766, loss: 0.08446703106164932\n",
      "batch: 9767, loss: 0.31711962819099426\n",
      "batch: 9768, loss: 0.06273379921913147\n",
      "batch: 9769, loss: 0.10258777439594269\n",
      "batch: 9770, loss: 0.06473992764949799\n",
      "batch: 9771, loss: 0.11768604815006256\n",
      "batch: 9772, loss: 0.06140804663300514\n",
      "batch: 9773, loss: 0.06290753930807114\n",
      "batch: 9774, loss: 0.15612255036830902\n",
      "batch: 9775, loss: 0.10074719041585922\n",
      "batch: 9776, loss: 0.06646385788917542\n",
      "batch: 9777, loss: 0.17979399859905243\n",
      "batch: 9778, loss: 0.12697333097457886\n",
      "batch: 9779, loss: 0.15507064759731293\n",
      "batch: 9780, loss: 0.16630250215530396\n",
      "batch: 9781, loss: 0.229775071144104\n",
      "batch: 9782, loss: 0.15581703186035156\n",
      "batch: 9783, loss: 0.17117071151733398\n",
      "batch: 9784, loss: 0.08791165798902512\n",
      "batch: 9785, loss: 0.0717693418264389\n",
      "batch: 9786, loss: 0.12753042578697205\n",
      "batch: 9787, loss: 0.21286548674106598\n",
      "batch: 9788, loss: 0.08832764625549316\n",
      "batch: 9789, loss: 0.2363525927066803\n",
      "batch: 9790, loss: 0.15951095521450043\n",
      "batch: 9791, loss: 0.19233344495296478\n",
      "batch: 9792, loss: 0.19290149211883545\n",
      "batch: 9793, loss: 0.11461994051933289\n",
      "batch: 9794, loss: 0.16881364583969116\n",
      "batch: 9795, loss: 0.10385125130414963\n",
      "batch: 9796, loss: 0.2013091892004013\n",
      "batch: 9797, loss: 0.0956200361251831\n",
      "batch: 9798, loss: 0.1389913260936737\n",
      "batch: 9799, loss: 0.11493253707885742\n",
      "batch: 9800, loss: 0.07358149439096451\n",
      "model saved to ./saving/model.ckpt-98\n",
      "batch: 9801, loss: 0.2429385483264923\n",
      "batch: 9802, loss: 0.05570172518491745\n",
      "batch: 9803, loss: 0.20483165979385376\n",
      "batch: 9804, loss: 0.2383345514535904\n",
      "batch: 9805, loss: 0.16724641621112823\n",
      "batch: 9806, loss: 0.17440922558307648\n",
      "batch: 9807, loss: 0.08014391362667084\n",
      "batch: 9808, loss: 0.1299048662185669\n",
      "batch: 9809, loss: 0.23321135342121124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 9810, loss: 0.1125163659453392\n",
      "batch: 9811, loss: 0.11650611460208893\n",
      "batch: 9812, loss: 0.16459183394908905\n",
      "batch: 9813, loss: 0.29055750370025635\n",
      "batch: 9814, loss: 0.13322794437408447\n",
      "batch: 9815, loss: 0.08554942905902863\n",
      "batch: 9816, loss: 0.10895660519599915\n",
      "batch: 9817, loss: 0.14656703174114227\n",
      "batch: 9818, loss: 0.14355114102363586\n",
      "batch: 9819, loss: 0.11805695295333862\n",
      "batch: 9820, loss: 0.1250603049993515\n",
      "batch: 9821, loss: 0.06960133463144302\n",
      "batch: 9822, loss: 0.06622208654880524\n",
      "batch: 9823, loss: 0.0612444244325161\n",
      "batch: 9824, loss: 0.15929698944091797\n",
      "batch: 9825, loss: 0.1289210021495819\n",
      "batch: 9826, loss: 0.09975363314151764\n",
      "batch: 9827, loss: 0.0736316367983818\n",
      "batch: 9828, loss: 0.16895171999931335\n",
      "batch: 9829, loss: 0.16814807057380676\n",
      "batch: 9830, loss: 0.5311446189880371\n",
      "batch: 9831, loss: 0.09878905117511749\n",
      "batch: 9832, loss: 0.12544086575508118\n",
      "batch: 9833, loss: 0.19024965167045593\n",
      "batch: 9834, loss: 0.17130181193351746\n",
      "batch: 9835, loss: 0.1161031574010849\n",
      "batch: 9836, loss: 0.20525191724300385\n",
      "batch: 9837, loss: 0.09879317879676819\n",
      "batch: 9838, loss: 0.06059124320745468\n",
      "batch: 9839, loss: 0.12445268779993057\n",
      "batch: 9840, loss: 0.2734571695327759\n",
      "batch: 9841, loss: 0.26791083812713623\n",
      "batch: 9842, loss: 0.21941553056240082\n",
      "batch: 9843, loss: 0.34264564514160156\n",
      "batch: 9844, loss: 0.06159542500972748\n",
      "batch: 9845, loss: 0.1567324548959732\n",
      "batch: 9846, loss: 0.10289311408996582\n",
      "batch: 9847, loss: 0.19833222031593323\n",
      "batch: 9848, loss: 0.09493664652109146\n",
      "batch: 9849, loss: 0.18943572044372559\n",
      "batch: 9850, loss: 0.07293389737606049\n",
      "batch: 9851, loss: 0.13593105971813202\n",
      "batch: 9852, loss: 0.1260787397623062\n",
      "batch: 9853, loss: 0.14466023445129395\n",
      "batch: 9854, loss: 0.09337048977613449\n",
      "batch: 9855, loss: 0.12917886674404144\n",
      "batch: 9856, loss: 0.06613975018262863\n",
      "batch: 9857, loss: 0.0631399005651474\n",
      "batch: 9858, loss: 0.12430068850517273\n",
      "batch: 9859, loss: 0.1559247374534607\n",
      "batch: 9860, loss: 0.18541912734508514\n",
      "batch: 9861, loss: 0.14520283043384552\n",
      "batch: 9862, loss: 0.17818421125411987\n",
      "batch: 9863, loss: 0.16704098880290985\n",
      "batch: 9864, loss: 0.28273454308509827\n",
      "batch: 9865, loss: 0.1642390340566635\n",
      "batch: 9866, loss: 0.1629534363746643\n",
      "batch: 9867, loss: 0.09078307449817657\n",
      "batch: 9868, loss: 0.11922450363636017\n",
      "batch: 9869, loss: 0.12307795882225037\n",
      "batch: 9870, loss: 0.04972119629383087\n",
      "batch: 9871, loss: 0.043572667986154556\n",
      "batch: 9872, loss: 0.11603063344955444\n",
      "batch: 9873, loss: 0.2522645592689514\n",
      "batch: 9874, loss: 0.2187843918800354\n",
      "batch: 9875, loss: 0.13745665550231934\n",
      "batch: 9876, loss: 0.10656947642564774\n",
      "batch: 9877, loss: 0.0621819794178009\n",
      "batch: 9878, loss: 0.0999932661652565\n",
      "batch: 9879, loss: 0.0916137546300888\n",
      "batch: 9880, loss: 0.27091923356056213\n",
      "batch: 9881, loss: 0.057188455015420914\n",
      "batch: 9882, loss: 0.25242751836776733\n",
      "batch: 9883, loss: 0.15105459094047546\n",
      "batch: 9884, loss: 0.22632399201393127\n",
      "batch: 9885, loss: 0.06517209112644196\n",
      "batch: 9886, loss: 0.07181663066148758\n",
      "batch: 9887, loss: 0.15531373023986816\n",
      "batch: 9888, loss: 0.07270072400569916\n",
      "batch: 9889, loss: 0.16583400964736938\n",
      "batch: 9890, loss: 0.1012728363275528\n",
      "batch: 9891, loss: 0.16919179260730743\n",
      "batch: 9892, loss: 0.19307923316955566\n",
      "batch: 9893, loss: 0.10781305283308029\n",
      "batch: 9894, loss: 0.14773578941822052\n",
      "batch: 9895, loss: 0.11355516314506531\n",
      "batch: 9896, loss: 0.4432935118675232\n",
      "batch: 9897, loss: 0.15966150164604187\n",
      "batch: 9898, loss: 0.126937597990036\n",
      "batch: 9899, loss: 0.12683412432670593\n",
      "batch: 9900, loss: 0.16622433066368103\n",
      "model saved to ./saving/model.ckpt-99\n",
      "batch: 9901, loss: 0.17016051709651947\n",
      "batch: 9902, loss: 0.09676244854927063\n",
      "batch: 9903, loss: 0.056711919605731964\n",
      "batch: 9904, loss: 0.05979622155427933\n",
      "batch: 9905, loss: 0.08383744210004807\n",
      "batch: 9906, loss: 0.07396512478590012\n",
      "batch: 9907, loss: 0.10785974562168121\n",
      "batch: 9908, loss: 0.22051289677619934\n",
      "batch: 9909, loss: 0.16348597407341003\n",
      "batch: 9910, loss: 0.17726284265518188\n",
      "batch: 9911, loss: 0.09233954548835754\n",
      "batch: 9912, loss: 0.13747470080852509\n",
      "batch: 9913, loss: 0.2280082404613495\n",
      "batch: 9914, loss: 0.17459823191165924\n",
      "batch: 9915, loss: 0.13821545243263245\n",
      "batch: 9916, loss: 0.07799407839775085\n",
      "batch: 9917, loss: 0.14286072552204132\n",
      "batch: 9918, loss: 0.11701799184083939\n",
      "batch: 9919, loss: 0.10787889361381531\n",
      "batch: 9920, loss: 0.31337985396385193\n",
      "batch: 9921, loss: 0.09287722408771515\n",
      "batch: 9922, loss: 0.11195161193609238\n",
      "batch: 9923, loss: 0.09545117616653442\n",
      "batch: 9924, loss: 0.1449967324733734\n",
      "batch: 9925, loss: 0.16455915570259094\n",
      "batch: 9926, loss: 0.08650554716587067\n",
      "batch: 9927, loss: 0.22094789147377014\n",
      "batch: 9928, loss: 0.14952045679092407\n",
      "batch: 9929, loss: 0.0673079565167427\n",
      "batch: 9930, loss: 0.27268141508102417\n",
      "batch: 9931, loss: 0.2529261112213135\n",
      "batch: 9932, loss: 0.06112229451537132\n",
      "batch: 9933, loss: 0.09484581649303436\n",
      "batch: 9934, loss: 0.053390294313430786\n",
      "batch: 9935, loss: 0.09838028997182846\n",
      "batch: 9936, loss: 0.08622503280639648\n",
      "batch: 9937, loss: 0.17224328219890594\n",
      "batch: 9938, loss: 0.08071809262037277\n",
      "batch: 9939, loss: 0.06304038316011429\n",
      "batch: 9940, loss: 0.10441172122955322\n",
      "batch: 9941, loss: 0.144398033618927\n",
      "batch: 9942, loss: 0.1918671727180481\n",
      "batch: 9943, loss: 0.17405138909816742\n",
      "batch: 9944, loss: 0.08689074218273163\n",
      "batch: 9945, loss: 0.08737937361001968\n",
      "batch: 9946, loss: 0.06664124131202698\n",
      "batch: 9947, loss: 0.21111777424812317\n",
      "batch: 9948, loss: 0.0701354444026947\n",
      "batch: 9949, loss: 0.1569705307483673\n",
      "batch: 9950, loss: 0.1827269345521927\n",
      "batch: 9951, loss: 0.22855859994888306\n",
      "batch: 9952, loss: 0.04908734932541847\n",
      "batch: 9953, loss: 0.2087576538324356\n",
      "batch: 9954, loss: 0.05496630817651749\n",
      "batch: 9955, loss: 0.06647080183029175\n",
      "batch: 9956, loss: 0.09066452085971832\n",
      "batch: 9957, loss: 0.10419449955224991\n",
      "batch: 9958, loss: 0.08410715311765671\n",
      "batch: 9959, loss: 0.18873542547225952\n",
      "batch: 9960, loss: 0.13562290370464325\n",
      "batch: 9961, loss: 0.06698796153068542\n",
      "batch: 9962, loss: 0.10731139034032822\n",
      "batch: 9963, loss: 0.12294036149978638\n",
      "batch: 9964, loss: 0.12139273434877396\n",
      "batch: 9965, loss: 0.1607363075017929\n",
      "batch: 9966, loss: 0.11907258629798889\n",
      "batch: 9967, loss: 0.12723946571350098\n",
      "batch: 9968, loss: 0.0886906161904335\n",
      "batch: 9969, loss: 0.21462687849998474\n",
      "batch: 9970, loss: 0.062309715896844864\n",
      "batch: 9971, loss: 0.17806819081306458\n",
      "batch: 9972, loss: 0.09022743999958038\n",
      "batch: 9973, loss: 0.06762698292732239\n",
      "batch: 9974, loss: 0.18027463555335999\n",
      "batch: 9975, loss: 0.03831568360328674\n",
      "batch: 9976, loss: 0.1646401584148407\n",
      "batch: 9977, loss: 0.08977583050727844\n",
      "batch: 9978, loss: 0.33918526768684387\n",
      "batch: 9979, loss: 0.10899657011032104\n",
      "batch: 9980, loss: 0.07678443193435669\n",
      "batch: 9981, loss: 0.07137666642665863\n",
      "batch: 9982, loss: 0.2849231958389282\n",
      "batch: 9983, loss: 0.1702590435743332\n",
      "batch: 9984, loss: 0.07224752008914948\n",
      "batch: 9985, loss: 0.21192754805088043\n",
      "batch: 9986, loss: 0.10957050323486328\n",
      "batch: 9987, loss: 0.11166737973690033\n",
      "batch: 9988, loss: 0.24541983008384705\n",
      "batch: 9989, loss: 0.137279212474823\n",
      "batch: 9990, loss: 0.21106374263763428\n",
      "batch: 9991, loss: 0.13243207335472107\n",
      "batch: 9992, loss: 0.10183845460414886\n",
      "batch: 9993, loss: 0.20151874423027039\n",
      "batch: 9994, loss: 0.24533206224441528\n",
      "batch: 9995, loss: 0.09501475095748901\n",
      "batch: 9996, loss: 0.14773619174957275\n",
      "batch: 9997, loss: 0.09069141000509262\n",
      "batch: 9998, loss: 0.1297820806503296\n",
      "batch: 9999, loss: 0.16262668371200562\n",
      "batch: 10000, loss: 0.12727633118629456\n",
      "model saved to ./saving/model.ckpt-100\n",
      "batch: 10001, loss: 0.10424135625362396\n",
      "batch: 10002, loss: 0.1583748757839203\n",
      "batch: 10003, loss: 0.10245338827371597\n",
      "batch: 10004, loss: 0.19458597898483276\n",
      "batch: 10005, loss: 0.10314536094665527\n",
      "batch: 10006, loss: 0.11440795660018921\n",
      "batch: 10007, loss: 0.17714332044124603\n",
      "batch: 10008, loss: 0.226345956325531\n",
      "batch: 10009, loss: 0.08608550578355789\n",
      "batch: 10010, loss: 0.1069527268409729\n",
      "batch: 10011, loss: 0.18343180418014526\n",
      "batch: 10012, loss: 0.1867571771144867\n",
      "batch: 10013, loss: 0.11223458498716354\n",
      "batch: 10014, loss: 0.10666276514530182\n",
      "batch: 10015, loss: 0.16298189759254456\n",
      "batch: 10016, loss: 0.1184166744351387\n",
      "batch: 10017, loss: 0.1939644068479538\n",
      "batch: 10018, loss: 0.08785447478294373\n",
      "batch: 10019, loss: 0.17023161053657532\n",
      "batch: 10020, loss: 0.11012475192546844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 10021, loss: 0.0676659494638443\n",
      "batch: 10022, loss: 0.18481096625328064\n",
      "batch: 10023, loss: 0.2028403878211975\n",
      "batch: 10024, loss: 0.12637118995189667\n",
      "batch: 10025, loss: 0.16219918429851532\n",
      "batch: 10026, loss: 0.1732894480228424\n",
      "batch: 10027, loss: 0.20540133118629456\n",
      "batch: 10028, loss: 0.09488286077976227\n",
      "batch: 10029, loss: 0.0988357812166214\n",
      "batch: 10030, loss: 0.13152794539928436\n",
      "batch: 10031, loss: 0.09253986924886703\n",
      "batch: 10032, loss: 0.10795501619577408\n",
      "batch: 10033, loss: 0.12555649876594543\n",
      "batch: 10034, loss: 0.12852400541305542\n",
      "batch: 10035, loss: 0.1333695948123932\n",
      "batch: 10036, loss: 0.09603247791528702\n",
      "batch: 10037, loss: 0.0402214452624321\n",
      "batch: 10038, loss: 0.06333330273628235\n",
      "batch: 10039, loss: 0.16585923731327057\n",
      "batch: 10040, loss: 0.19910208880901337\n",
      "batch: 10041, loss: 0.09753866493701935\n",
      "batch: 10042, loss: 0.09012356400489807\n",
      "batch: 10043, loss: 0.2932804524898529\n",
      "batch: 10044, loss: 0.13842938840389252\n",
      "batch: 10045, loss: 0.12720465660095215\n",
      "batch: 10046, loss: 0.16160829365253448\n",
      "batch: 10047, loss: 0.06476771831512451\n",
      "batch: 10048, loss: 0.14477288722991943\n",
      "batch: 10049, loss: 0.04387330263853073\n",
      "batch: 10050, loss: 0.28195303678512573\n",
      "batch: 10051, loss: 0.1742681860923767\n",
      "batch: 10052, loss: 0.08857403695583344\n",
      "batch: 10053, loss: 0.022321267053484917\n",
      "batch: 10054, loss: 0.059388767927885056\n",
      "batch: 10055, loss: 0.11321070045232773\n",
      "batch: 10056, loss: 0.11540360003709793\n",
      "batch: 10057, loss: 0.16951894760131836\n",
      "batch: 10058, loss: 0.164308100938797\n",
      "batch: 10059, loss: 0.09911257028579712\n",
      "batch: 10060, loss: 0.06553669273853302\n",
      "batch: 10061, loss: 0.1671520620584488\n",
      "batch: 10062, loss: 0.15349337458610535\n",
      "batch: 10063, loss: 0.12882302701473236\n",
      "batch: 10064, loss: 0.30479806661605835\n",
      "batch: 10065, loss: 0.04709544777870178\n",
      "batch: 10066, loss: 0.3453359603881836\n",
      "batch: 10067, loss: 0.15245118737220764\n",
      "batch: 10068, loss: 0.04157384857535362\n",
      "batch: 10069, loss: 0.18383529782295227\n",
      "batch: 10070, loss: 0.14899173378944397\n",
      "batch: 10071, loss: 0.169256329536438\n",
      "batch: 10072, loss: 0.07142343372106552\n",
      "batch: 10073, loss: 0.07272794842720032\n",
      "batch: 10074, loss: 0.13760164380073547\n",
      "batch: 10075, loss: 0.11345861852169037\n",
      "batch: 10076, loss: 0.30934229493141174\n",
      "batch: 10077, loss: 0.10443872958421707\n",
      "batch: 10078, loss: 0.12685823440551758\n",
      "batch: 10079, loss: 0.1465095579624176\n",
      "batch: 10080, loss: 0.19673465192317963\n",
      "batch: 10081, loss: 0.1872917115688324\n",
      "batch: 10082, loss: 0.13366767764091492\n",
      "batch: 10083, loss: 0.20305050909519196\n",
      "batch: 10084, loss: 0.1798379123210907\n",
      "batch: 10085, loss: 0.12495218217372894\n",
      "batch: 10086, loss: 0.12865112721920013\n",
      "batch: 10087, loss: 0.06833957135677338\n",
      "batch: 10088, loss: 0.09716819226741791\n",
      "batch: 10089, loss: 0.4156897962093353\n",
      "batch: 10090, loss: 0.14464837312698364\n",
      "batch: 10091, loss: 0.19366279244422913\n",
      "batch: 10092, loss: 0.2765014171600342\n",
      "batch: 10093, loss: 0.07693473994731903\n",
      "batch: 10094, loss: 0.1715380698442459\n",
      "batch: 10095, loss: 0.1294328272342682\n",
      "batch: 10096, loss: 0.06086898222565651\n",
      "batch: 10097, loss: 0.0664953887462616\n",
      "batch: 10098, loss: 0.047513872385025024\n",
      "batch: 10099, loss: 0.17719513177871704\n",
      "batch: 10100, loss: 0.29316115379333496\n",
      "model saved to ./saving/model.ckpt-101\n",
      "batch: 10101, loss: 0.06184465438127518\n",
      "batch: 10102, loss: 0.17728707194328308\n",
      "batch: 10103, loss: 0.0786140114068985\n",
      "batch: 10104, loss: 0.0446750670671463\n",
      "batch: 10105, loss: 0.08616550266742706\n",
      "batch: 10106, loss: 0.13044491410255432\n",
      "batch: 10107, loss: 0.10150747001171112\n",
      "batch: 10108, loss: 0.0652720034122467\n",
      "batch: 10109, loss: 0.09047438204288483\n",
      "batch: 10110, loss: 0.08319120109081268\n",
      "batch: 10111, loss: 0.3375231623649597\n",
      "batch: 10112, loss: 0.17622163891792297\n",
      "batch: 10113, loss: 0.11093967407941818\n",
      "batch: 10114, loss: 0.15888436138629913\n",
      "batch: 10115, loss: 0.12985475361347198\n",
      "batch: 10116, loss: 0.14885511994361877\n",
      "batch: 10117, loss: 0.10168737173080444\n",
      "batch: 10118, loss: 0.24195629358291626\n",
      "batch: 10119, loss: 0.17939873039722443\n",
      "batch: 10120, loss: 0.07965891808271408\n",
      "batch: 10121, loss: 0.08114448934793472\n",
      "batch: 10122, loss: 0.14442017674446106\n",
      "batch: 10123, loss: 0.10634414851665497\n",
      "batch: 10124, loss: 0.16567173600196838\n",
      "batch: 10125, loss: 0.12088252604007721\n",
      "batch: 10126, loss: 0.08914932608604431\n",
      "batch: 10127, loss: 0.17200610041618347\n",
      "batch: 10128, loss: 0.1185525432229042\n",
      "batch: 10129, loss: 0.14879289269447327\n",
      "batch: 10130, loss: 0.15405899286270142\n",
      "batch: 10131, loss: 0.16840124130249023\n",
      "batch: 10132, loss: 0.13396000862121582\n",
      "batch: 10133, loss: 0.15663495659828186\n",
      "batch: 10134, loss: 0.1571134328842163\n",
      "batch: 10135, loss: 0.0743509829044342\n",
      "batch: 10136, loss: 0.05383141711354256\n",
      "batch: 10137, loss: 0.14953677356243134\n",
      "batch: 10138, loss: 0.14215455949306488\n",
      "batch: 10139, loss: 0.12843604385852814\n",
      "batch: 10140, loss: 0.12344832718372345\n",
      "batch: 10141, loss: 0.10312623530626297\n",
      "batch: 10142, loss: 0.13038183748722076\n",
      "batch: 10143, loss: 0.09120633453130722\n",
      "batch: 10144, loss: 0.09988196194171906\n",
      "batch: 10145, loss: 0.12811675667762756\n",
      "batch: 10146, loss: 0.24906843900680542\n",
      "batch: 10147, loss: 0.1260317713022232\n",
      "batch: 10148, loss: 0.16919481754302979\n",
      "batch: 10149, loss: 0.15420711040496826\n",
      "batch: 10150, loss: 0.14521197974681854\n",
      "batch: 10151, loss: 0.17618820071220398\n",
      "batch: 10152, loss: 0.044571299105882645\n",
      "batch: 10153, loss: 0.05523013323545456\n",
      "batch: 10154, loss: 0.15885403752326965\n",
      "batch: 10155, loss: 0.10740981251001358\n",
      "batch: 10156, loss: 0.12205642461776733\n",
      "batch: 10157, loss: 0.07805917412042618\n",
      "batch: 10158, loss: 0.17024554312229156\n",
      "batch: 10159, loss: 0.15130005776882172\n",
      "batch: 10160, loss: 0.14675237238407135\n",
      "batch: 10161, loss: 0.32760632038116455\n",
      "batch: 10162, loss: 0.10481839627027512\n",
      "batch: 10163, loss: 0.20245736837387085\n",
      "batch: 10164, loss: 0.19161604344844818\n",
      "batch: 10165, loss: 0.12905466556549072\n",
      "batch: 10166, loss: 0.21856090426445007\n",
      "batch: 10167, loss: 0.08727312088012695\n",
      "batch: 10168, loss: 0.13575851917266846\n",
      "batch: 10169, loss: 0.09441107511520386\n",
      "batch: 10170, loss: 0.07032259553670883\n",
      "batch: 10171, loss: 0.1263047158718109\n",
      "batch: 10172, loss: 0.09470461308956146\n",
      "batch: 10173, loss: 0.061268456280231476\n",
      "batch: 10174, loss: 0.1958802342414856\n",
      "batch: 10175, loss: 0.10168201476335526\n",
      "batch: 10176, loss: 0.06873413920402527\n",
      "batch: 10177, loss: 0.07438307255506516\n",
      "batch: 10178, loss: 0.08150488138198853\n",
      "batch: 10179, loss: 0.06972169131040573\n",
      "batch: 10180, loss: 0.05990006774663925\n",
      "batch: 10181, loss: 0.09682049602270126\n",
      "batch: 10182, loss: 0.14851117134094238\n",
      "batch: 10183, loss: 0.15990477800369263\n",
      "batch: 10184, loss: 0.10917516052722931\n",
      "batch: 10185, loss: 0.17598414421081543\n",
      "batch: 10186, loss: 0.231076180934906\n",
      "batch: 10187, loss: 0.19452421367168427\n",
      "batch: 10188, loss: 0.0900229811668396\n",
      "batch: 10189, loss: 0.181649848818779\n",
      "batch: 10190, loss: 0.13717684149742126\n",
      "batch: 10191, loss: 0.0896463543176651\n",
      "batch: 10192, loss: 0.11013111472129822\n",
      "batch: 10193, loss: 0.12075917422771454\n",
      "batch: 10194, loss: 0.2897319197654724\n",
      "batch: 10195, loss: 0.19834810495376587\n",
      "batch: 10196, loss: 0.12717795372009277\n",
      "batch: 10197, loss: 0.2744686007499695\n",
      "batch: 10198, loss: 0.16818320751190186\n",
      "batch: 10199, loss: 0.12846431136131287\n",
      "batch: 10200, loss: 0.10800735652446747\n",
      "model saved to ./saving/model.ckpt-102\n",
      "batch: 10201, loss: 0.11762874573469162\n",
      "batch: 10202, loss: 0.07443605363368988\n",
      "batch: 10203, loss: 0.2429356426000595\n",
      "batch: 10204, loss: 0.11025741696357727\n",
      "batch: 10205, loss: 0.13388200104236603\n",
      "batch: 10206, loss: 0.10067053139209747\n",
      "batch: 10207, loss: 0.13227270543575287\n",
      "batch: 10208, loss: 0.2033160775899887\n",
      "batch: 10209, loss: 0.11282619088888168\n",
      "batch: 10210, loss: 0.24398230016231537\n",
      "batch: 10211, loss: 0.2414187490940094\n",
      "batch: 10212, loss: 0.2382766157388687\n",
      "batch: 10213, loss: 0.1688854694366455\n",
      "batch: 10214, loss: 0.11666528880596161\n",
      "batch: 10215, loss: 0.2410385012626648\n",
      "batch: 10216, loss: 0.10267405211925507\n",
      "batch: 10217, loss: 0.1288161426782608\n",
      "batch: 10218, loss: 0.10733948647975922\n",
      "batch: 10219, loss: 0.060747891664505005\n",
      "batch: 10220, loss: 0.24978679418563843\n",
      "batch: 10221, loss: 0.10072331130504608\n",
      "batch: 10222, loss: 0.19155067205429077\n",
      "batch: 10223, loss: 0.1388656049966812\n",
      "batch: 10224, loss: 0.11835765093564987\n",
      "batch: 10225, loss: 0.07119623571634293\n",
      "batch: 10226, loss: 0.12125252187252045\n",
      "batch: 10227, loss: 0.18640165030956268\n",
      "batch: 10228, loss: 0.28478047251701355\n",
      "batch: 10229, loss: 0.2688474953174591\n",
      "batch: 10230, loss: 0.12784171104431152\n",
      "batch: 10231, loss: 0.12846845388412476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 10232, loss: 0.29633092880249023\n",
      "batch: 10233, loss: 0.10499260574579239\n",
      "batch: 10234, loss: 0.0692872405052185\n",
      "batch: 10235, loss: 0.13269104063510895\n",
      "batch: 10236, loss: 0.26254594326019287\n",
      "batch: 10237, loss: 0.22996273636817932\n",
      "batch: 10238, loss: 0.18673597276210785\n",
      "batch: 10239, loss: 0.13124999403953552\n",
      "batch: 10240, loss: 0.16364553570747375\n",
      "batch: 10241, loss: 0.03190493956208229\n",
      "batch: 10242, loss: 0.1148688793182373\n",
      "batch: 10243, loss: 0.08225014805793762\n",
      "batch: 10244, loss: 0.21340124309062958\n",
      "batch: 10245, loss: 0.17977362871170044\n",
      "batch: 10246, loss: 0.07148824632167816\n",
      "batch: 10247, loss: 0.06381319463253021\n",
      "batch: 10248, loss: 0.11010531336069107\n",
      "batch: 10249, loss: 0.12972623109817505\n",
      "batch: 10250, loss: 0.11967328190803528\n",
      "batch: 10251, loss: 0.11721952259540558\n",
      "batch: 10252, loss: 0.15130773186683655\n",
      "batch: 10253, loss: 0.09970583766698837\n",
      "batch: 10254, loss: 0.09655234962701797\n",
      "batch: 10255, loss: 0.307743102312088\n",
      "batch: 10256, loss: 0.14522838592529297\n",
      "batch: 10257, loss: 0.06998791545629501\n",
      "batch: 10258, loss: 0.188563734292984\n",
      "batch: 10259, loss: 0.1932404786348343\n",
      "batch: 10260, loss: 0.13854385912418365\n",
      "batch: 10261, loss: 0.07285359501838684\n",
      "batch: 10262, loss: 0.20941120386123657\n",
      "batch: 10263, loss: 0.4187195897102356\n",
      "batch: 10264, loss: 0.17447444796562195\n",
      "batch: 10265, loss: 0.12080137431621552\n",
      "batch: 10266, loss: 0.04948089271783829\n",
      "batch: 10267, loss: 0.23727117478847504\n",
      "batch: 10268, loss: 0.2367624044418335\n",
      "batch: 10269, loss: 0.3271007537841797\n",
      "batch: 10270, loss: 0.17721152305603027\n",
      "batch: 10271, loss: 0.05521036684513092\n",
      "batch: 10272, loss: 0.17595374584197998\n",
      "batch: 10273, loss: 0.0827581137418747\n",
      "batch: 10274, loss: 0.14055070281028748\n",
      "batch: 10275, loss: 0.3869243860244751\n",
      "batch: 10276, loss: 0.1252104938030243\n",
      "batch: 10277, loss: 0.39513781666755676\n",
      "batch: 10278, loss: 0.17986968159675598\n",
      "batch: 10279, loss: 0.15025508403778076\n",
      "batch: 10280, loss: 0.11615660041570663\n",
      "batch: 10281, loss: 0.06443049013614655\n",
      "batch: 10282, loss: 0.22056186199188232\n",
      "batch: 10283, loss: 0.1188768669962883\n",
      "batch: 10284, loss: 0.10902459919452667\n",
      "batch: 10285, loss: 0.16245365142822266\n",
      "batch: 10286, loss: 0.1467655599117279\n",
      "batch: 10287, loss: 0.12991052865982056\n",
      "batch: 10288, loss: 0.11599929630756378\n",
      "batch: 10289, loss: 0.24243447184562683\n",
      "batch: 10290, loss: 0.02231806516647339\n",
      "batch: 10291, loss: 0.17381370067596436\n",
      "batch: 10292, loss: 0.0933317020535469\n",
      "batch: 10293, loss: 0.1328851282596588\n",
      "batch: 10294, loss: 0.13335302472114563\n",
      "batch: 10295, loss: 0.07181577384471893\n",
      "batch: 10296, loss: 0.15749041736125946\n",
      "batch: 10297, loss: 0.12812811136245728\n",
      "batch: 10298, loss: 0.3069753348827362\n",
      "batch: 10299, loss: 0.0821724385023117\n",
      "batch: 10300, loss: 0.374036967754364\n",
      "model saved to ./saving/model.ckpt-103\n",
      "batch: 10301, loss: 0.1620761752128601\n",
      "batch: 10302, loss: 0.09424521028995514\n",
      "batch: 10303, loss: 0.2286466658115387\n",
      "batch: 10304, loss: 0.12424960732460022\n",
      "batch: 10305, loss: 0.16695979237556458\n",
      "batch: 10306, loss: 0.17128989100456238\n",
      "batch: 10307, loss: 0.05707446113228798\n",
      "batch: 10308, loss: 0.17794105410575867\n",
      "batch: 10309, loss: 0.08753721415996552\n",
      "batch: 10310, loss: 0.06598224490880966\n",
      "batch: 10311, loss: 0.11459987610578537\n",
      "batch: 10312, loss: 0.10857677459716797\n",
      "batch: 10313, loss: 0.054322294890880585\n",
      "batch: 10314, loss: 0.26058536767959595\n",
      "batch: 10315, loss: 0.148726224899292\n",
      "batch: 10316, loss: 0.33894744515419006\n",
      "batch: 10317, loss: 0.1163475513458252\n",
      "batch: 10318, loss: 0.11534179747104645\n",
      "batch: 10319, loss: 0.1404436230659485\n",
      "batch: 10320, loss: 0.2725943624973297\n",
      "batch: 10321, loss: 0.1902484893798828\n",
      "batch: 10322, loss: 0.11768865585327148\n",
      "batch: 10323, loss: 0.0825977623462677\n",
      "batch: 10324, loss: 0.11030972748994827\n",
      "batch: 10325, loss: 0.09314912557601929\n",
      "batch: 10326, loss: 0.13624565303325653\n",
      "batch: 10327, loss: 0.13795213401317596\n",
      "batch: 10328, loss: 0.15607018768787384\n",
      "batch: 10329, loss: 0.17677535116672516\n",
      "batch: 10330, loss: 0.0626334622502327\n",
      "batch: 10331, loss: 0.2148139327764511\n",
      "batch: 10332, loss: 0.24331828951835632\n",
      "batch: 10333, loss: 0.05925563722848892\n",
      "batch: 10334, loss: 0.16525278985500336\n",
      "batch: 10335, loss: 0.05471121147274971\n",
      "batch: 10336, loss: 0.13048577308654785\n",
      "batch: 10337, loss: 0.11496029794216156\n",
      "batch: 10338, loss: 0.07137484103441238\n",
      "batch: 10339, loss: 0.09964527189731598\n",
      "batch: 10340, loss: 0.09122087061405182\n",
      "batch: 10341, loss: 0.098952516913414\n",
      "batch: 10342, loss: 0.06756219267845154\n",
      "batch: 10343, loss: 0.0935090184211731\n",
      "batch: 10344, loss: 0.03822553530335426\n",
      "batch: 10345, loss: 0.06695546209812164\n",
      "batch: 10346, loss: 0.07009351253509521\n",
      "batch: 10347, loss: 0.04499661177396774\n",
      "batch: 10348, loss: 0.07711070775985718\n",
      "batch: 10349, loss: 0.06898904591798782\n",
      "batch: 10350, loss: 0.091671422123909\n",
      "batch: 10351, loss: 0.06943875551223755\n",
      "batch: 10352, loss: 0.11723536252975464\n",
      "batch: 10353, loss: 0.09108959883451462\n",
      "batch: 10354, loss: 0.20920588076114655\n",
      "batch: 10355, loss: 0.08773086220026016\n",
      "batch: 10356, loss: 0.1876922845840454\n",
      "batch: 10357, loss: 0.13853999972343445\n",
      "batch: 10358, loss: 0.17655381560325623\n",
      "batch: 10359, loss: 0.15822748839855194\n",
      "batch: 10360, loss: 0.036186374723911285\n",
      "batch: 10361, loss: 0.26407110691070557\n",
      "batch: 10362, loss: 0.07675774395465851\n",
      "batch: 10363, loss: 0.09186683595180511\n",
      "batch: 10364, loss: 0.09573721885681152\n",
      "batch: 10365, loss: 0.10210362076759338\n",
      "batch: 10366, loss: 0.12488916516304016\n",
      "batch: 10367, loss: 0.08755039423704147\n",
      "batch: 10368, loss: 0.11931543052196503\n",
      "batch: 10369, loss: 0.04932252690196037\n",
      "batch: 10370, loss: 0.10100536048412323\n",
      "batch: 10371, loss: 0.05748403072357178\n",
      "batch: 10372, loss: 0.24209867417812347\n",
      "batch: 10373, loss: 0.052018776535987854\n",
      "batch: 10374, loss: 0.11244980990886688\n",
      "batch: 10375, loss: 0.1373298466205597\n",
      "batch: 10376, loss: 0.17855332791805267\n",
      "batch: 10377, loss: 0.07261358201503754\n",
      "batch: 10378, loss: 0.21141605079174042\n",
      "batch: 10379, loss: 0.19510206580162048\n",
      "batch: 10380, loss: 0.09562784433364868\n",
      "batch: 10381, loss: 0.20894253253936768\n",
      "batch: 10382, loss: 0.06649467349052429\n",
      "batch: 10383, loss: 0.15268728137016296\n",
      "batch: 10384, loss: 0.15252216160297394\n",
      "batch: 10385, loss: 0.1976361721754074\n",
      "batch: 10386, loss: 0.21605831384658813\n",
      "batch: 10387, loss: 0.1651129424571991\n",
      "batch: 10388, loss: 0.17092359066009521\n",
      "batch: 10389, loss: 0.04883699119091034\n",
      "batch: 10390, loss: 0.14827895164489746\n",
      "batch: 10391, loss: 0.21124061942100525\n",
      "batch: 10392, loss: 0.13377371430397034\n",
      "batch: 10393, loss: 0.11477965116500854\n",
      "batch: 10394, loss: 0.0680321455001831\n",
      "batch: 10395, loss: 0.19534805417060852\n",
      "batch: 10396, loss: 0.11650190502405167\n",
      "batch: 10397, loss: 0.1073513776063919\n",
      "batch: 10398, loss: 0.060985952615737915\n",
      "batch: 10399, loss: 0.3404860198497772\n",
      "batch: 10400, loss: 0.20071251690387726\n",
      "model saved to ./saving/model.ckpt-104\n",
      "batch: 10401, loss: 0.17798717319965363\n",
      "batch: 10402, loss: 0.1860954910516739\n",
      "batch: 10403, loss: 0.08238521218299866\n",
      "batch: 10404, loss: 0.135901540517807\n",
      "batch: 10405, loss: 0.06738439947366714\n",
      "batch: 10406, loss: 0.11507819592952728\n",
      "batch: 10407, loss: 0.11490146070718765\n",
      "batch: 10408, loss: 0.16327440738677979\n",
      "batch: 10409, loss: 0.14562425017356873\n",
      "batch: 10410, loss: 0.03591666370630264\n",
      "batch: 10411, loss: 0.14310893416404724\n",
      "batch: 10412, loss: 0.17139650881290436\n",
      "batch: 10413, loss: 0.2381380945444107\n",
      "batch: 10414, loss: 0.1371729075908661\n",
      "batch: 10415, loss: 0.10516896098852158\n",
      "batch: 10416, loss: 0.43722671270370483\n",
      "batch: 10417, loss: 0.0709531307220459\n",
      "batch: 10418, loss: 0.14913883805274963\n",
      "batch: 10419, loss: 0.11727313697338104\n",
      "batch: 10420, loss: 0.25511395931243896\n",
      "batch: 10421, loss: 0.13082553446292877\n",
      "batch: 10422, loss: 0.06863440573215485\n",
      "batch: 10423, loss: 0.08506982028484344\n",
      "batch: 10424, loss: 0.15642808377742767\n",
      "batch: 10425, loss: 0.09071845561265945\n",
      "batch: 10426, loss: 0.1371845304965973\n",
      "batch: 10427, loss: 0.07680921256542206\n",
      "batch: 10428, loss: 0.04881846532225609\n",
      "batch: 10429, loss: 0.0663200318813324\n",
      "batch: 10430, loss: 0.07978551089763641\n",
      "batch: 10431, loss: 0.13936719298362732\n",
      "batch: 10432, loss: 0.14921848475933075\n",
      "batch: 10433, loss: 0.21076956391334534\n",
      "batch: 10434, loss: 0.21735511720180511\n",
      "batch: 10435, loss: 0.2546696662902832\n",
      "batch: 10436, loss: 0.0991825982928276\n",
      "batch: 10437, loss: 0.34432899951934814\n",
      "batch: 10438, loss: 0.06886404752731323\n",
      "batch: 10439, loss: 0.18068844079971313\n",
      "batch: 10440, loss: 0.13305118680000305\n",
      "batch: 10441, loss: 0.22257404029369354\n",
      "batch: 10442, loss: 0.20511013269424438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 10443, loss: 0.16850212216377258\n",
      "batch: 10444, loss: 0.04730590432882309\n",
      "batch: 10445, loss: 0.11501390486955643\n",
      "batch: 10446, loss: 0.0734347552061081\n",
      "batch: 10447, loss: 0.10377203673124313\n",
      "batch: 10448, loss: 0.22188623249530792\n",
      "batch: 10449, loss: 0.31687021255493164\n",
      "batch: 10450, loss: 0.10985560715198517\n",
      "batch: 10451, loss: 0.09138447046279907\n",
      "batch: 10452, loss: 0.08422070741653442\n",
      "batch: 10453, loss: 0.15855342149734497\n",
      "batch: 10454, loss: 0.11283539235591888\n",
      "batch: 10455, loss: 0.10709110647439957\n",
      "batch: 10456, loss: 0.08115550875663757\n",
      "batch: 10457, loss: 0.08728507161140442\n",
      "batch: 10458, loss: 0.0964750349521637\n",
      "batch: 10459, loss: 0.16121730208396912\n",
      "batch: 10460, loss: 0.10190871357917786\n",
      "batch: 10461, loss: 0.07768195867538452\n",
      "batch: 10462, loss: 0.08854351937770844\n",
      "batch: 10463, loss: 0.15482613444328308\n",
      "batch: 10464, loss: 0.09226951748132706\n",
      "batch: 10465, loss: 0.06807020306587219\n",
      "batch: 10466, loss: 0.13575059175491333\n",
      "batch: 10467, loss: 0.17876273393630981\n",
      "batch: 10468, loss: 0.10780467092990875\n",
      "batch: 10469, loss: 0.13663406670093536\n",
      "batch: 10470, loss: 0.1307750940322876\n",
      "batch: 10471, loss: 0.16977104544639587\n",
      "batch: 10472, loss: 0.2358626425266266\n",
      "batch: 10473, loss: 0.11873389035463333\n",
      "batch: 10474, loss: 0.07573190331459045\n",
      "batch: 10475, loss: 0.1205943375825882\n",
      "batch: 10476, loss: 0.18733862042427063\n",
      "batch: 10477, loss: 0.23257207870483398\n",
      "batch: 10478, loss: 0.07166525721549988\n",
      "batch: 10479, loss: 0.1188093051314354\n",
      "batch: 10480, loss: 0.08225907385349274\n",
      "batch: 10481, loss: 0.11579760164022446\n",
      "batch: 10482, loss: 0.09850306808948517\n",
      "batch: 10483, loss: 0.0967266708612442\n",
      "batch: 10484, loss: 0.1623012125492096\n",
      "batch: 10485, loss: 0.14464668929576874\n",
      "batch: 10486, loss: 0.1439022272825241\n",
      "batch: 10487, loss: 0.07610855251550674\n",
      "batch: 10488, loss: 0.14449548721313477\n",
      "batch: 10489, loss: 0.08701007813215256\n",
      "batch: 10490, loss: 0.09157776832580566\n",
      "batch: 10491, loss: 0.11485892534255981\n",
      "batch: 10492, loss: 0.2367250770330429\n",
      "batch: 10493, loss: 0.12355265766382217\n",
      "batch: 10494, loss: 0.16419528424739838\n",
      "batch: 10495, loss: 0.055597662925720215\n",
      "batch: 10496, loss: 0.19249892234802246\n",
      "batch: 10497, loss: 0.14709550142288208\n",
      "batch: 10498, loss: 0.19504636526107788\n",
      "batch: 10499, loss: 0.07810001075267792\n",
      "batch: 10500, loss: 0.10815844684839249\n",
      "model saved to ./saving/model.ckpt-105\n",
      "batch: 10501, loss: 0.11552475392818451\n",
      "batch: 10502, loss: 0.16895857453346252\n",
      "batch: 10503, loss: 0.12611261010169983\n",
      "batch: 10504, loss: 0.1487657129764557\n",
      "batch: 10505, loss: 0.15851116180419922\n",
      "batch: 10506, loss: 0.13602545857429504\n",
      "batch: 10507, loss: 0.18660417199134827\n",
      "batch: 10508, loss: 0.07042285799980164\n",
      "batch: 10509, loss: 0.18830566108226776\n",
      "batch: 10510, loss: 0.14734318852424622\n",
      "batch: 10511, loss: 0.16171997785568237\n",
      "batch: 10512, loss: 0.08143674582242966\n",
      "batch: 10513, loss: 0.07194600254297256\n",
      "batch: 10514, loss: 0.10751452296972275\n",
      "batch: 10515, loss: 0.11316584050655365\n",
      "batch: 10516, loss: 0.13463500142097473\n",
      "batch: 10517, loss: 0.21332809329032898\n",
      "batch: 10518, loss: 0.11383012682199478\n",
      "batch: 10519, loss: 0.1325133740901947\n",
      "batch: 10520, loss: 0.19633080065250397\n",
      "batch: 10521, loss: 0.06490585207939148\n",
      "batch: 10522, loss: 0.21809007227420807\n",
      "batch: 10523, loss: 0.17552798986434937\n",
      "batch: 10524, loss: 0.16509270668029785\n",
      "batch: 10525, loss: 0.2729825973510742\n",
      "batch: 10526, loss: 0.07052494585514069\n",
      "batch: 10527, loss: 0.1048354059457779\n",
      "batch: 10528, loss: 0.14036938548088074\n",
      "batch: 10529, loss: 0.258414626121521\n",
      "batch: 10530, loss: 0.10586297512054443\n",
      "batch: 10531, loss: 0.05335947498679161\n",
      "batch: 10532, loss: 0.034456364810466766\n",
      "batch: 10533, loss: 0.21015551686286926\n",
      "batch: 10534, loss: 0.06388646364212036\n",
      "batch: 10535, loss: 0.17893937230110168\n",
      "batch: 10536, loss: 0.2464233934879303\n",
      "batch: 10537, loss: 0.05421379208564758\n",
      "batch: 10538, loss: 0.2096642106771469\n",
      "batch: 10539, loss: 0.12246909737586975\n",
      "batch: 10540, loss: 0.1305588036775589\n",
      "batch: 10541, loss: 0.18803007900714874\n",
      "batch: 10542, loss: 0.14465980231761932\n",
      "batch: 10543, loss: 0.19850806891918182\n",
      "batch: 10544, loss: 0.14046962559223175\n",
      "batch: 10545, loss: 0.1469975709915161\n",
      "batch: 10546, loss: 0.12766069173812866\n",
      "batch: 10547, loss: 0.0790422260761261\n",
      "batch: 10548, loss: 0.09283396601676941\n",
      "batch: 10549, loss: 0.08998432755470276\n",
      "batch: 10550, loss: 0.1275104135274887\n",
      "batch: 10551, loss: 0.34807801246643066\n",
      "batch: 10552, loss: 0.23430293798446655\n",
      "batch: 10553, loss: 0.038006071001291275\n",
      "batch: 10554, loss: 0.1552010327577591\n",
      "batch: 10555, loss: 0.15132179856300354\n",
      "batch: 10556, loss: 0.16811004281044006\n",
      "batch: 10557, loss: 0.04823519289493561\n",
      "batch: 10558, loss: 0.23262499272823334\n",
      "batch: 10559, loss: 0.09963183104991913\n",
      "batch: 10560, loss: 0.11668650805950165\n",
      "batch: 10561, loss: 0.06608148664236069\n",
      "batch: 10562, loss: 0.12083495408296585\n",
      "batch: 10563, loss: 0.11700884997844696\n",
      "batch: 10564, loss: 0.15031155943870544\n",
      "batch: 10565, loss: 0.09688620269298553\n",
      "batch: 10566, loss: 0.10896632820367813\n",
      "batch: 10567, loss: 0.12992756068706512\n",
      "batch: 10568, loss: 0.16785532236099243\n",
      "batch: 10569, loss: 0.15594863891601562\n",
      "batch: 10570, loss: 0.10074155032634735\n",
      "batch: 10571, loss: 0.13036736845970154\n",
      "batch: 10572, loss: 0.1474948227405548\n",
      "batch: 10573, loss: 0.10036025196313858\n",
      "batch: 10574, loss: 0.1235908716917038\n",
      "batch: 10575, loss: 0.2098802924156189\n",
      "batch: 10576, loss: 0.15746422111988068\n",
      "batch: 10577, loss: 0.061125531792640686\n",
      "batch: 10578, loss: 0.09034474194049835\n",
      "batch: 10579, loss: 0.15538810193538666\n",
      "batch: 10580, loss: 0.15151336789131165\n",
      "batch: 10581, loss: 0.05641794204711914\n",
      "batch: 10582, loss: 0.16608718037605286\n",
      "batch: 10583, loss: 0.09727537631988525\n",
      "batch: 10584, loss: 0.2163805067539215\n",
      "batch: 10585, loss: 0.10632631182670593\n",
      "batch: 10586, loss: 0.07676214724779129\n",
      "batch: 10587, loss: 0.10759973526000977\n",
      "batch: 10588, loss: 0.14201276004314423\n",
      "batch: 10589, loss: 0.19972184300422668\n",
      "batch: 10590, loss: 0.07952208817005157\n",
      "batch: 10591, loss: 0.15762609243392944\n",
      "batch: 10592, loss: 0.25233787298202515\n",
      "batch: 10593, loss: 0.18780067563056946\n",
      "batch: 10594, loss: 0.28189146518707275\n",
      "batch: 10595, loss: 0.10246039181947708\n",
      "batch: 10596, loss: 0.07778674364089966\n",
      "batch: 10597, loss: 0.12389686703681946\n",
      "batch: 10598, loss: 0.06872658431529999\n",
      "batch: 10599, loss: 0.09706529974937439\n",
      "batch: 10600, loss: 0.23797385394573212\n",
      "model saved to ./saving/model.ckpt-106\n",
      "batch: 10601, loss: 0.08874683082103729\n",
      "batch: 10602, loss: 0.044700197875499725\n",
      "batch: 10603, loss: 0.13706505298614502\n",
      "batch: 10604, loss: 0.0942298173904419\n",
      "batch: 10605, loss: 0.04659326747059822\n",
      "batch: 10606, loss: 0.10818616300821304\n",
      "batch: 10607, loss: 0.1271742582321167\n",
      "batch: 10608, loss: 0.35709047317504883\n",
      "batch: 10609, loss: 0.15555953979492188\n",
      "batch: 10610, loss: 0.11406435072422028\n",
      "batch: 10611, loss: 0.16120386123657227\n",
      "batch: 10612, loss: 0.16471710801124573\n",
      "batch: 10613, loss: 0.12364953011274338\n",
      "batch: 10614, loss: 0.1157323345541954\n",
      "batch: 10615, loss: 0.08259962499141693\n",
      "batch: 10616, loss: 0.07238952815532684\n",
      "batch: 10617, loss: 0.15379798412322998\n",
      "batch: 10618, loss: 0.21043765544891357\n",
      "batch: 10619, loss: 0.13276219367980957\n",
      "batch: 10620, loss: 0.08793593943119049\n",
      "batch: 10621, loss: 0.08821547031402588\n",
      "batch: 10622, loss: 0.06524939090013504\n",
      "batch: 10623, loss: 0.09244412183761597\n",
      "batch: 10624, loss: 0.21939322352409363\n",
      "batch: 10625, loss: 0.14693902432918549\n",
      "batch: 10626, loss: 0.16279633343219757\n",
      "batch: 10627, loss: 0.06287483870983124\n",
      "batch: 10628, loss: 0.20951959490776062\n",
      "batch: 10629, loss: 0.14446434378623962\n",
      "batch: 10630, loss: 0.0957312360405922\n",
      "batch: 10631, loss: 0.10163912177085876\n",
      "batch: 10632, loss: 0.09363764524459839\n",
      "batch: 10633, loss: 0.14451627433300018\n",
      "batch: 10634, loss: 0.10192953795194626\n",
      "batch: 10635, loss: 0.11472412198781967\n",
      "batch: 10636, loss: 0.09056014567613602\n",
      "batch: 10637, loss: 0.08659444749355316\n",
      "batch: 10638, loss: 0.12120621651411057\n",
      "batch: 10639, loss: 0.16056489944458008\n",
      "batch: 10640, loss: 0.17291145026683807\n",
      "batch: 10641, loss: 0.06705423444509506\n",
      "batch: 10642, loss: 0.13627611100673676\n",
      "batch: 10643, loss: 0.1582321673631668\n",
      "batch: 10644, loss: 0.22660371661186218\n",
      "batch: 10645, loss: 0.09801209717988968\n",
      "batch: 10646, loss: 0.1849026381969452\n",
      "batch: 10647, loss: 0.09183369576931\n",
      "batch: 10648, loss: 0.1879233717918396\n",
      "batch: 10649, loss: 0.0662107840180397\n",
      "batch: 10650, loss: 0.16517338156700134\n",
      "batch: 10651, loss: 0.20105421543121338\n",
      "batch: 10652, loss: 0.0929563120007515\n",
      "batch: 10653, loss: 0.11896897107362747\n",
      "batch: 10654, loss: 0.2856355309486389\n",
      "batch: 10655, loss: 0.0891551598906517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 10656, loss: 0.13969232141971588\n",
      "batch: 10657, loss: 0.08607174456119537\n",
      "batch: 10658, loss: 0.09779440611600876\n",
      "batch: 10659, loss: 0.1340450942516327\n",
      "batch: 10660, loss: 0.12251241505146027\n",
      "batch: 10661, loss: 0.17609834671020508\n",
      "batch: 10662, loss: 0.10051983594894409\n",
      "batch: 10663, loss: 0.13634702563285828\n",
      "batch: 10664, loss: 0.09481929242610931\n",
      "batch: 10665, loss: 0.1181236132979393\n",
      "batch: 10666, loss: 0.11249223351478577\n",
      "batch: 10667, loss: 0.07703609019517899\n",
      "batch: 10668, loss: 0.1884709894657135\n",
      "batch: 10669, loss: 0.06328381597995758\n",
      "batch: 10670, loss: 0.08700138330459595\n",
      "batch: 10671, loss: 0.13112755119800568\n",
      "batch: 10672, loss: 0.3070227801799774\n",
      "batch: 10673, loss: 0.11419566720724106\n",
      "batch: 10674, loss: 0.21543601155281067\n",
      "batch: 10675, loss: 0.14405857026576996\n",
      "batch: 10676, loss: 0.1456936001777649\n",
      "batch: 10677, loss: 0.17263254523277283\n",
      "batch: 10678, loss: 0.24198384582996368\n",
      "batch: 10679, loss: 0.14933764934539795\n",
      "batch: 10680, loss: 0.11929267644882202\n",
      "batch: 10681, loss: 0.13914766907691956\n",
      "batch: 10682, loss: 0.1680375337600708\n",
      "batch: 10683, loss: 0.16042913496494293\n",
      "batch: 10684, loss: 0.11909184604883194\n",
      "batch: 10685, loss: 0.17673978209495544\n",
      "batch: 10686, loss: 0.08851445466279984\n",
      "batch: 10687, loss: 0.11989451199769974\n",
      "batch: 10688, loss: 0.20273196697235107\n",
      "batch: 10689, loss: 0.12826183438301086\n",
      "batch: 10690, loss: 0.2010570615530014\n",
      "batch: 10691, loss: 0.09326730668544769\n",
      "batch: 10692, loss: 0.16170960664749146\n",
      "batch: 10693, loss: 0.13205011188983917\n",
      "batch: 10694, loss: 0.27267083525657654\n",
      "batch: 10695, loss: 0.14664576947689056\n",
      "batch: 10696, loss: 0.2518502473831177\n",
      "batch: 10697, loss: 0.1494121551513672\n",
      "batch: 10698, loss: 0.24552574753761292\n",
      "batch: 10699, loss: 0.1543087512254715\n",
      "batch: 10700, loss: 0.08146759122610092\n",
      "model saved to ./saving/model.ckpt-107\n",
      "batch: 10701, loss: 0.09574925154447556\n",
      "batch: 10702, loss: 0.23431286215782166\n",
      "batch: 10703, loss: 0.1022900640964508\n",
      "batch: 10704, loss: 0.04742852598428726\n",
      "batch: 10705, loss: 0.28295713663101196\n",
      "batch: 10706, loss: 0.0737234354019165\n",
      "batch: 10707, loss: 0.1058075800538063\n",
      "batch: 10708, loss: 0.31646043062210083\n",
      "batch: 10709, loss: 0.0744289830327034\n",
      "batch: 10710, loss: 0.07830663025379181\n",
      "batch: 10711, loss: 0.05633778125047684\n",
      "batch: 10712, loss: 0.20005270838737488\n",
      "batch: 10713, loss: 0.13695070147514343\n",
      "batch: 10714, loss: 0.08411652594804764\n",
      "batch: 10715, loss: 0.1066553145647049\n",
      "batch: 10716, loss: 0.12570443749427795\n",
      "batch: 10717, loss: 0.13195231556892395\n",
      "batch: 10718, loss: 0.05163316801190376\n",
      "batch: 10719, loss: 0.18100862205028534\n",
      "batch: 10720, loss: 0.206182599067688\n",
      "batch: 10721, loss: 0.08865581452846527\n",
      "batch: 10722, loss: 0.1920432448387146\n",
      "batch: 10723, loss: 0.09482898563146591\n",
      "batch: 10724, loss: 0.12128874659538269\n",
      "batch: 10725, loss: 0.1459425389766693\n",
      "batch: 10726, loss: 0.1089320182800293\n",
      "batch: 10727, loss: 0.09087640047073364\n",
      "batch: 10728, loss: 0.18790119886398315\n",
      "batch: 10729, loss: 0.1437506228685379\n",
      "batch: 10730, loss: 0.31167054176330566\n",
      "batch: 10731, loss: 0.11774707585573196\n",
      "batch: 10732, loss: 0.09019447863101959\n",
      "batch: 10733, loss: 0.23496606945991516\n",
      "batch: 10734, loss: 0.11536993831396103\n",
      "batch: 10735, loss: 0.1333206743001938\n",
      "batch: 10736, loss: 0.09400252997875214\n",
      "batch: 10737, loss: 0.10898492485284805\n",
      "batch: 10738, loss: 0.20688700675964355\n",
      "batch: 10739, loss: 0.11046922206878662\n",
      "batch: 10740, loss: 0.25467395782470703\n",
      "batch: 10741, loss: 0.09699633717536926\n",
      "batch: 10742, loss: 0.173707515001297\n",
      "batch: 10743, loss: 0.2167011797428131\n",
      "batch: 10744, loss: 0.1159067302942276\n",
      "batch: 10745, loss: 0.049384184181690216\n",
      "batch: 10746, loss: 0.10820750147104263\n",
      "batch: 10747, loss: 0.14635199308395386\n",
      "batch: 10748, loss: 0.10378061234951019\n",
      "batch: 10749, loss: 0.14416418969631195\n",
      "batch: 10750, loss: 0.13631540536880493\n",
      "batch: 10751, loss: 0.039041317999362946\n",
      "batch: 10752, loss: 0.09434521943330765\n",
      "batch: 10753, loss: 0.11527380347251892\n",
      "batch: 10754, loss: 0.2648971676826477\n",
      "batch: 10755, loss: 0.0476832389831543\n",
      "batch: 10756, loss: 0.0898275300860405\n",
      "batch: 10757, loss: 0.08187475800514221\n",
      "batch: 10758, loss: 0.24830710887908936\n",
      "batch: 10759, loss: 0.15524551272392273\n",
      "batch: 10760, loss: 0.10023415088653564\n",
      "batch: 10761, loss: 0.10584942251443863\n",
      "batch: 10762, loss: 0.10316968709230423\n",
      "batch: 10763, loss: 0.16443687677383423\n",
      "batch: 10764, loss: 0.20632527768611908\n",
      "batch: 10765, loss: 0.04790505766868591\n",
      "batch: 10766, loss: 0.20487987995147705\n",
      "batch: 10767, loss: 0.16932052373886108\n",
      "batch: 10768, loss: 0.13171716034412384\n",
      "batch: 10769, loss: 0.0829068124294281\n",
      "batch: 10770, loss: 0.126449853181839\n",
      "batch: 10771, loss: 0.13534358143806458\n",
      "batch: 10772, loss: 0.21743272244930267\n",
      "batch: 10773, loss: 0.1203402578830719\n",
      "batch: 10774, loss: 0.10069866478443146\n",
      "batch: 10775, loss: 0.15442898869514465\n",
      "batch: 10776, loss: 0.13082998991012573\n",
      "batch: 10777, loss: 0.0936935693025589\n",
      "batch: 10778, loss: 0.08348383009433746\n",
      "batch: 10779, loss: 0.05812624469399452\n",
      "batch: 10780, loss: 0.06800876557826996\n",
      "batch: 10781, loss: 0.2061113864183426\n",
      "batch: 10782, loss: 0.10803928971290588\n",
      "batch: 10783, loss: 0.1991565227508545\n",
      "batch: 10784, loss: 0.06051645800471306\n",
      "batch: 10785, loss: 0.19574923813343048\n",
      "batch: 10786, loss: 0.05537310615181923\n",
      "batch: 10787, loss: 0.18962304294109344\n",
      "batch: 10788, loss: 0.10214681923389435\n",
      "batch: 10789, loss: 0.0688982829451561\n",
      "batch: 10790, loss: 0.0742906704545021\n",
      "batch: 10791, loss: 0.09194053709506989\n",
      "batch: 10792, loss: 0.10125317424535751\n",
      "batch: 10793, loss: 0.16601836681365967\n",
      "batch: 10794, loss: 0.13850030303001404\n",
      "batch: 10795, loss: 0.0990549772977829\n",
      "batch: 10796, loss: 0.20364859700202942\n",
      "batch: 10797, loss: 0.1762092560529709\n",
      "batch: 10798, loss: 0.19984915852546692\n",
      "batch: 10799, loss: 0.09525325149297714\n",
      "batch: 10800, loss: 0.071832075715065\n",
      "model saved to ./saving/model.ckpt-108\n",
      "batch: 10801, loss: 0.30993252992630005\n",
      "batch: 10802, loss: 0.17661112546920776\n",
      "batch: 10803, loss: 0.30422356724739075\n",
      "batch: 10804, loss: 0.12901446223258972\n",
      "batch: 10805, loss: 0.10428285598754883\n",
      "batch: 10806, loss: 0.08598650991916656\n",
      "batch: 10807, loss: 0.13219603896141052\n",
      "batch: 10808, loss: 0.10322405397891998\n",
      "batch: 10809, loss: 0.07460673153400421\n",
      "batch: 10810, loss: 0.14456017315387726\n",
      "batch: 10811, loss: 0.15814059972763062\n",
      "batch: 10812, loss: 0.15747474133968353\n",
      "batch: 10813, loss: 0.17721284925937653\n",
      "batch: 10814, loss: 0.11971165239810944\n",
      "batch: 10815, loss: 0.07517069578170776\n",
      "batch: 10816, loss: 0.17292481660842896\n",
      "batch: 10817, loss: 0.18969005346298218\n",
      "batch: 10818, loss: 0.09456013143062592\n",
      "batch: 10819, loss: 0.1891821175813675\n",
      "batch: 10820, loss: 0.11294566094875336\n",
      "batch: 10821, loss: 0.1295175403356552\n",
      "batch: 10822, loss: 0.10747846215963364\n",
      "batch: 10823, loss: 0.08540244400501251\n",
      "batch: 10824, loss: 0.12041063606739044\n",
      "batch: 10825, loss: 0.157434344291687\n",
      "batch: 10826, loss: 0.1256532520055771\n",
      "batch: 10827, loss: 0.2441682070493698\n",
      "batch: 10828, loss: 0.17113623023033142\n",
      "batch: 10829, loss: 0.1382516771554947\n",
      "batch: 10830, loss: 0.05349654331803322\n",
      "batch: 10831, loss: 0.09775605797767639\n",
      "batch: 10832, loss: 0.14945021271705627\n",
      "batch: 10833, loss: 0.09447471797466278\n",
      "batch: 10834, loss: 0.10645825415849686\n",
      "batch: 10835, loss: 0.2346729040145874\n",
      "batch: 10836, loss: 0.0837077647447586\n",
      "batch: 10837, loss: 0.06269331276416779\n",
      "batch: 10838, loss: 0.16926857829093933\n",
      "batch: 10839, loss: 0.18426527082920074\n",
      "batch: 10840, loss: 0.07410449534654617\n",
      "batch: 10841, loss: 0.11577332019805908\n",
      "batch: 10842, loss: 0.189459890127182\n",
      "batch: 10843, loss: 0.3046567440032959\n",
      "batch: 10844, loss: 0.060794442892074585\n",
      "batch: 10845, loss: 0.13054311275482178\n",
      "batch: 10846, loss: 0.08915099501609802\n",
      "batch: 10847, loss: 0.11409150809049606\n",
      "batch: 10848, loss: 0.19130556285381317\n",
      "batch: 10849, loss: 0.09337277710437775\n",
      "batch: 10850, loss: 0.12129893898963928\n",
      "batch: 10851, loss: 0.10157451778650284\n",
      "batch: 10852, loss: 0.07883736491203308\n",
      "batch: 10853, loss: 0.18420282006263733\n",
      "batch: 10854, loss: 0.04177258536219597\n",
      "batch: 10855, loss: 0.27584922313690186\n",
      "batch: 10856, loss: 0.17708821594715118\n",
      "batch: 10857, loss: 0.08876796066761017\n",
      "batch: 10858, loss: 0.16898131370544434\n",
      "batch: 10859, loss: 0.11480994522571564\n",
      "batch: 10860, loss: 0.17823290824890137\n",
      "batch: 10861, loss: 0.23481622338294983\n",
      "batch: 10862, loss: 0.2521914839744568\n",
      "batch: 10863, loss: 0.0661889985203743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 10864, loss: 0.16614416241645813\n",
      "batch: 10865, loss: 0.09094072133302689\n",
      "batch: 10866, loss: 0.0420130118727684\n",
      "batch: 10867, loss: 0.10218103229999542\n",
      "batch: 10868, loss: 0.09606097638607025\n",
      "batch: 10869, loss: 0.08067500591278076\n",
      "batch: 10870, loss: 0.10198860615491867\n",
      "batch: 10871, loss: 0.12812484800815582\n",
      "batch: 10872, loss: 0.10787567496299744\n",
      "batch: 10873, loss: 0.13373830914497375\n",
      "batch: 10874, loss: 0.3395072817802429\n",
      "batch: 10875, loss: 0.11658007651567459\n",
      "batch: 10876, loss: 0.14720821380615234\n",
      "batch: 10877, loss: 0.20883169770240784\n",
      "batch: 10878, loss: 0.07627313584089279\n",
      "batch: 10879, loss: 0.19421625137329102\n",
      "batch: 10880, loss: 0.1179729625582695\n",
      "batch: 10881, loss: 0.0632307231426239\n",
      "batch: 10882, loss: 0.20245099067687988\n",
      "batch: 10883, loss: 0.05557403340935707\n",
      "batch: 10884, loss: 0.10886027663946152\n",
      "batch: 10885, loss: 0.14345687627792358\n",
      "batch: 10886, loss: 0.11563228815793991\n",
      "batch: 10887, loss: 0.16966477036476135\n",
      "batch: 10888, loss: 0.0894029289484024\n",
      "batch: 10889, loss: 0.0761638730764389\n",
      "batch: 10890, loss: 0.07790397107601166\n",
      "batch: 10891, loss: 0.3019765019416809\n",
      "batch: 10892, loss: 0.07320533692836761\n",
      "batch: 10893, loss: 0.1235058605670929\n",
      "batch: 10894, loss: 0.0814988985657692\n",
      "batch: 10895, loss: 0.20141009986400604\n",
      "batch: 10896, loss: 0.10065664350986481\n",
      "batch: 10897, loss: 0.12803108990192413\n",
      "batch: 10898, loss: 0.25273263454437256\n",
      "batch: 10899, loss: 0.20505419373512268\n",
      "batch: 10900, loss: 0.11845392733812332\n",
      "model saved to ./saving/model.ckpt-109\n",
      "batch: 10901, loss: 0.14337965846061707\n",
      "batch: 10902, loss: 0.09326349198818207\n",
      "batch: 10903, loss: 0.10617287456989288\n",
      "batch: 10904, loss: 0.11594723165035248\n",
      "batch: 10905, loss: 0.11386404186487198\n",
      "batch: 10906, loss: 0.18438291549682617\n",
      "batch: 10907, loss: 0.04153263941407204\n",
      "batch: 10908, loss: 0.12419344484806061\n",
      "batch: 10909, loss: 0.10348466038703918\n",
      "batch: 10910, loss: 0.2812901735305786\n",
      "batch: 10911, loss: 0.12711991369724274\n",
      "batch: 10912, loss: 0.11725243180990219\n",
      "batch: 10913, loss: 0.03733503818511963\n",
      "batch: 10914, loss: 0.09228300303220749\n",
      "batch: 10915, loss: 0.15581291913986206\n",
      "batch: 10916, loss: 0.25998789072036743\n",
      "batch: 10917, loss: 0.15340572595596313\n",
      "batch: 10918, loss: 0.13171759247779846\n",
      "batch: 10919, loss: 0.06033439189195633\n",
      "batch: 10920, loss: 0.16441339254379272\n",
      "batch: 10921, loss: 0.2069968581199646\n",
      "batch: 10922, loss: 0.09524733573198318\n",
      "batch: 10923, loss: 0.1617729663848877\n",
      "batch: 10924, loss: 0.2160380780696869\n",
      "batch: 10925, loss: 0.11648993194103241\n",
      "batch: 10926, loss: 0.03986455500125885\n",
      "batch: 10927, loss: 0.11443231254816055\n",
      "batch: 10928, loss: 0.19446898996829987\n",
      "batch: 10929, loss: 0.1660393923521042\n",
      "batch: 10930, loss: 0.0661228820681572\n",
      "batch: 10931, loss: 0.20001152157783508\n",
      "batch: 10932, loss: 0.10683837532997131\n",
      "batch: 10933, loss: 0.05140208080410957\n",
      "batch: 10934, loss: 0.07603342831134796\n",
      "batch: 10935, loss: 0.06418720632791519\n",
      "batch: 10936, loss: 0.11056406795978546\n",
      "batch: 10937, loss: 0.10176602005958557\n",
      "batch: 10938, loss: 0.13336801528930664\n",
      "batch: 10939, loss: 0.14093232154846191\n",
      "batch: 10940, loss: 0.12372071295976639\n",
      "batch: 10941, loss: 0.12830489873886108\n",
      "batch: 10942, loss: 0.29164189100265503\n",
      "batch: 10943, loss: 0.07366510480642319\n",
      "batch: 10944, loss: 0.13590289652347565\n",
      "batch: 10945, loss: 0.10520000010728836\n",
      "batch: 10946, loss: 0.13972561061382294\n",
      "batch: 10947, loss: 0.12085048109292984\n",
      "batch: 10948, loss: 0.44712239503860474\n",
      "batch: 10949, loss: 0.09801755100488663\n",
      "batch: 10950, loss: 0.12264309078454971\n",
      "batch: 10951, loss: 0.05540984123945236\n",
      "batch: 10952, loss: 0.1361514776945114\n",
      "batch: 10953, loss: 0.3246113955974579\n",
      "batch: 10954, loss: 0.201145738363266\n",
      "batch: 10955, loss: 0.25425031781196594\n",
      "batch: 10956, loss: 0.09998152405023575\n",
      "batch: 10957, loss: 0.05858296900987625\n",
      "batch: 10958, loss: 0.15713483095169067\n",
      "batch: 10959, loss: 0.14200261235237122\n",
      "batch: 10960, loss: 0.1171480268239975\n",
      "batch: 10961, loss: 0.08462335914373398\n",
      "batch: 10962, loss: 0.13168387115001678\n",
      "batch: 10963, loss: 0.19525563716888428\n",
      "batch: 10964, loss: 0.042876947671175\n",
      "batch: 10965, loss: 0.047233372926712036\n",
      "batch: 10966, loss: 0.06458252668380737\n",
      "batch: 10967, loss: 0.08044257760047913\n",
      "batch: 10968, loss: 0.15481185913085938\n",
      "batch: 10969, loss: 0.09596045315265656\n",
      "batch: 10970, loss: 0.19179190695285797\n",
      "batch: 10971, loss: 0.14174948632717133\n",
      "batch: 10972, loss: 0.22161464393138885\n",
      "batch: 10973, loss: 0.0804506167769432\n",
      "batch: 10974, loss: 0.13186246156692505\n",
      "batch: 10975, loss: 0.09690751880407333\n",
      "batch: 10976, loss: 0.19076092541217804\n",
      "batch: 10977, loss: 0.09742899239063263\n",
      "batch: 10978, loss: 0.11429057270288467\n",
      "batch: 10979, loss: 0.20640283823013306\n",
      "batch: 10980, loss: 0.253690242767334\n",
      "batch: 10981, loss: 0.16244477033615112\n",
      "batch: 10982, loss: 0.22561490535736084\n",
      "batch: 10983, loss: 0.11537885665893555\n",
      "batch: 10984, loss: 0.07539194077253342\n",
      "batch: 10985, loss: 0.3375375270843506\n",
      "batch: 10986, loss: 0.09281539916992188\n",
      "batch: 10987, loss: 0.09358367323875427\n",
      "batch: 10988, loss: 0.06665682792663574\n",
      "batch: 10989, loss: 0.14708980917930603\n",
      "batch: 10990, loss: 0.26450827717781067\n",
      "batch: 10991, loss: 0.09418957680463791\n",
      "batch: 10992, loss: 0.1364842653274536\n",
      "batch: 10993, loss: 0.20721404254436493\n",
      "batch: 10994, loss: 0.07654748857021332\n",
      "batch: 10995, loss: 0.17912760376930237\n",
      "batch: 10996, loss: 0.1305306851863861\n",
      "batch: 10997, loss: 0.05585770308971405\n",
      "batch: 10998, loss: 0.08660399913787842\n",
      "batch: 10999, loss: 0.10934073477983475\n",
      "batch: 11000, loss: 0.1966484785079956\n",
      "model saved to ./saving/model.ckpt-110\n",
      "batch: 11001, loss: 0.16072985529899597\n",
      "batch: 11002, loss: 0.21808332204818726\n",
      "batch: 11003, loss: 0.1082003265619278\n",
      "batch: 11004, loss: 0.09226895868778229\n",
      "batch: 11005, loss: 0.21945613622665405\n",
      "batch: 11006, loss: 0.17847071588039398\n",
      "batch: 11007, loss: 0.05618342384696007\n",
      "batch: 11008, loss: 0.10650496929883957\n",
      "batch: 11009, loss: 0.12823861837387085\n",
      "batch: 11010, loss: 0.18649598956108093\n",
      "batch: 11011, loss: 0.05442991852760315\n",
      "batch: 11012, loss: 0.1692872941493988\n",
      "batch: 11013, loss: 0.1989038586616516\n",
      "batch: 11014, loss: 0.3667423725128174\n",
      "batch: 11015, loss: 0.27299827337265015\n",
      "batch: 11016, loss: 0.2315523326396942\n",
      "batch: 11017, loss: 0.14149697124958038\n",
      "batch: 11018, loss: 0.06819110363721848\n",
      "batch: 11019, loss: 0.10711005330085754\n",
      "batch: 11020, loss: 0.26046422123908997\n",
      "batch: 11021, loss: 0.08780904114246368\n",
      "batch: 11022, loss: 0.09349226951599121\n",
      "batch: 11023, loss: 0.24101823568344116\n",
      "batch: 11024, loss: 0.07913734018802643\n",
      "batch: 11025, loss: 0.13061603903770447\n",
      "batch: 11026, loss: 0.1720731556415558\n",
      "batch: 11027, loss: 0.08842504024505615\n",
      "batch: 11028, loss: 0.2088555097579956\n",
      "batch: 11029, loss: 0.27459925413131714\n",
      "batch: 11030, loss: 0.26237404346466064\n",
      "batch: 11031, loss: 0.08513979613780975\n",
      "batch: 11032, loss: 0.21791374683380127\n",
      "batch: 11033, loss: 0.08068086206912994\n",
      "batch: 11034, loss: 0.08728471398353577\n",
      "batch: 11035, loss: 0.20953647792339325\n",
      "batch: 11036, loss: 0.04753444343805313\n",
      "batch: 11037, loss: 0.07873731851577759\n",
      "batch: 11038, loss: 0.05250617861747742\n",
      "batch: 11039, loss: 0.2602653503417969\n",
      "batch: 11040, loss: 0.20475319027900696\n",
      "batch: 11041, loss: 0.16637259721755981\n",
      "batch: 11042, loss: 0.1744619905948639\n",
      "batch: 11043, loss: 0.16350938379764557\n",
      "batch: 11044, loss: 0.152662992477417\n",
      "batch: 11045, loss: 0.10571351647377014\n",
      "batch: 11046, loss: 0.1057043969631195\n",
      "batch: 11047, loss: 0.16719993948936462\n",
      "batch: 11048, loss: 0.1660543829202652\n",
      "batch: 11049, loss: 0.2508560121059418\n",
      "batch: 11050, loss: 0.14904563128948212\n",
      "batch: 11051, loss: 0.1535969227552414\n",
      "batch: 11052, loss: 0.09582550078630447\n",
      "batch: 11053, loss: 0.09304410219192505\n",
      "batch: 11054, loss: 0.042648009955883026\n",
      "batch: 11055, loss: 0.15712155401706696\n",
      "batch: 11056, loss: 0.15374214947223663\n",
      "batch: 11057, loss: 0.12232976406812668\n",
      "batch: 11058, loss: 0.12793751060962677\n",
      "batch: 11059, loss: 0.09392395615577698\n",
      "batch: 11060, loss: 0.18464836478233337\n",
      "batch: 11061, loss: 0.14092522859573364\n",
      "batch: 11062, loss: 0.1890507936477661\n",
      "batch: 11063, loss: 0.16807164251804352\n",
      "batch: 11064, loss: 0.08037982881069183\n",
      "batch: 11065, loss: 0.1400967389345169\n",
      "batch: 11066, loss: 0.29717081785202026\n",
      "batch: 11067, loss: 0.24364793300628662\n",
      "batch: 11068, loss: 0.05248107388615608\n",
      "batch: 11069, loss: 0.16210079193115234\n",
      "batch: 11070, loss: 0.0721750408411026\n",
      "batch: 11071, loss: 0.21723654866218567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 11072, loss: 0.13052381575107574\n",
      "batch: 11073, loss: 0.126105397939682\n",
      "batch: 11074, loss: 0.06654021888971329\n",
      "batch: 11075, loss: 0.16542872786521912\n",
      "batch: 11076, loss: 0.254934161901474\n",
      "batch: 11077, loss: 0.13268502056598663\n",
      "batch: 11078, loss: 0.15894103050231934\n",
      "batch: 11079, loss: 0.15078794956207275\n",
      "batch: 11080, loss: 0.08525896817445755\n",
      "batch: 11081, loss: 0.24480792880058289\n",
      "batch: 11082, loss: 0.18525809049606323\n",
      "batch: 11083, loss: 0.15442869067192078\n",
      "batch: 11084, loss: 0.12210327386856079\n",
      "batch: 11085, loss: 0.13603080809116364\n",
      "batch: 11086, loss: 0.17303156852722168\n",
      "batch: 11087, loss: 0.05077506601810455\n",
      "batch: 11088, loss: 0.14020399749279022\n",
      "batch: 11089, loss: 0.18767628073692322\n",
      "batch: 11090, loss: 0.1905711442232132\n",
      "batch: 11091, loss: 0.0854807198047638\n",
      "batch: 11092, loss: 0.09216761589050293\n",
      "batch: 11093, loss: 0.14680266380310059\n",
      "batch: 11094, loss: 0.20998886227607727\n",
      "batch: 11095, loss: 0.19996213912963867\n",
      "batch: 11096, loss: 0.13562257587909698\n",
      "batch: 11097, loss: 0.13090158998966217\n",
      "batch: 11098, loss: 0.2824013829231262\n",
      "batch: 11099, loss: 0.20313948392868042\n",
      "batch: 11100, loss: 0.2548166513442993\n",
      "model saved to ./saving/model.ckpt-111\n",
      "batch: 11101, loss: 0.20345541834831238\n",
      "batch: 11102, loss: 0.07801977545022964\n",
      "batch: 11103, loss: 0.08423203229904175\n",
      "batch: 11104, loss: 0.0911380723118782\n",
      "batch: 11105, loss: 0.10159631073474884\n",
      "batch: 11106, loss: 0.06832508742809296\n",
      "batch: 11107, loss: 0.06974988430738449\n",
      "batch: 11108, loss: 0.10782814025878906\n",
      "batch: 11109, loss: 0.34574317932128906\n",
      "batch: 11110, loss: 0.13092532753944397\n",
      "batch: 11111, loss: 0.05390394479036331\n",
      "batch: 11112, loss: 0.1474340707063675\n",
      "batch: 11113, loss: 0.153812438249588\n",
      "batch: 11114, loss: 0.11237724125385284\n",
      "batch: 11115, loss: 0.08915789425373077\n",
      "batch: 11116, loss: 0.0967036634683609\n",
      "batch: 11117, loss: 0.19425079226493835\n",
      "batch: 11118, loss: 0.12099295854568481\n",
      "batch: 11119, loss: 0.18354849517345428\n",
      "batch: 11120, loss: 0.16108062863349915\n",
      "batch: 11121, loss: 0.054655030369758606\n",
      "batch: 11122, loss: 0.0580035075545311\n",
      "batch: 11123, loss: 0.07415138930082321\n",
      "batch: 11124, loss: 0.23296868801116943\n",
      "batch: 11125, loss: 0.24703451991081238\n",
      "batch: 11126, loss: 0.12457865476608276\n",
      "batch: 11127, loss: 0.05029962584376335\n",
      "batch: 11128, loss: 0.051478028297424316\n",
      "batch: 11129, loss: 0.0840849056839943\n",
      "batch: 11130, loss: 0.06468583643436432\n",
      "batch: 11131, loss: 0.04619428887963295\n",
      "batch: 11132, loss: 0.22638341784477234\n",
      "batch: 11133, loss: 0.09522800892591476\n",
      "batch: 11134, loss: 0.0797291249036789\n",
      "batch: 11135, loss: 0.14806172251701355\n",
      "batch: 11136, loss: 0.09700359404087067\n",
      "batch: 11137, loss: 0.10285520553588867\n",
      "batch: 11138, loss: 0.0659976527094841\n",
      "batch: 11139, loss: 0.12557116150856018\n",
      "batch: 11140, loss: 0.1263311356306076\n",
      "batch: 11141, loss: 0.15052303671836853\n",
      "batch: 11142, loss: 0.13990962505340576\n",
      "batch: 11143, loss: 0.11067400872707367\n",
      "batch: 11144, loss: 0.09669305384159088\n",
      "batch: 11145, loss: 0.2219332456588745\n",
      "batch: 11146, loss: 0.1490720957517624\n",
      "batch: 11147, loss: 0.07304729521274567\n",
      "batch: 11148, loss: 0.13402053713798523\n",
      "batch: 11149, loss: 0.10159724950790405\n",
      "batch: 11150, loss: 0.06282791495323181\n",
      "batch: 11151, loss: 0.12104760110378265\n",
      "batch: 11152, loss: 0.14071986079216003\n",
      "batch: 11153, loss: 0.14278553426265717\n",
      "batch: 11154, loss: 0.05157588794827461\n",
      "batch: 11155, loss: 0.3130938708782196\n",
      "batch: 11156, loss: 0.11746098846197128\n",
      "batch: 11157, loss: 0.20115306973457336\n",
      "batch: 11158, loss: 0.25555309653282166\n",
      "batch: 11159, loss: 0.06826483458280563\n",
      "batch: 11160, loss: 0.11761584132909775\n",
      "batch: 11161, loss: 0.1484675407409668\n",
      "batch: 11162, loss: 0.14566507935523987\n",
      "batch: 11163, loss: 0.07584787160158157\n",
      "batch: 11164, loss: 0.05665222927927971\n",
      "batch: 11165, loss: 0.12202699482440948\n",
      "batch: 11166, loss: 0.07713160663843155\n",
      "batch: 11167, loss: 0.10874994844198227\n",
      "batch: 11168, loss: 0.10270832479000092\n",
      "batch: 11169, loss: 0.05197390168905258\n",
      "batch: 11170, loss: 0.28716060519218445\n",
      "batch: 11171, loss: 0.08554089069366455\n",
      "batch: 11172, loss: 0.10723309963941574\n",
      "batch: 11173, loss: 0.22639207541942596\n",
      "batch: 11174, loss: 0.17422321438789368\n",
      "batch: 11175, loss: 0.12446798384189606\n",
      "batch: 11176, loss: 0.148471400141716\n",
      "batch: 11177, loss: 0.10456396639347076\n",
      "batch: 11178, loss: 0.08965258300304413\n",
      "batch: 11179, loss: 0.09566965699195862\n",
      "batch: 11180, loss: 0.1968637853860855\n",
      "batch: 11181, loss: 0.135593980550766\n",
      "batch: 11182, loss: 0.21536587178707123\n",
      "batch: 11183, loss: 0.08106422424316406\n",
      "batch: 11184, loss: 0.1498129963874817\n",
      "batch: 11185, loss: 0.0879104882478714\n",
      "batch: 11186, loss: 0.2630302309989929\n",
      "batch: 11187, loss: 0.04448634758591652\n",
      "batch: 11188, loss: 0.11062991619110107\n",
      "batch: 11189, loss: 0.10171439498662949\n",
      "batch: 11190, loss: 0.16258862614631653\n",
      "batch: 11191, loss: 0.16275617480278015\n",
      "batch: 11192, loss: 0.044011201709508896\n",
      "batch: 11193, loss: 0.16762381792068481\n",
      "batch: 11194, loss: 0.08318401873111725\n",
      "batch: 11195, loss: 0.07272829115390778\n",
      "batch: 11196, loss: 0.05513765662908554\n",
      "batch: 11197, loss: 0.21758800745010376\n",
      "batch: 11198, loss: 0.104271799325943\n",
      "batch: 11199, loss: 0.1283605396747589\n",
      "batch: 11200, loss: 0.15045374631881714\n",
      "model saved to ./saving/model.ckpt-112\n",
      "batch: 11201, loss: 0.07095036655664444\n",
      "batch: 11202, loss: 0.24722573161125183\n",
      "batch: 11203, loss: 0.22553086280822754\n",
      "batch: 11204, loss: 0.14039063453674316\n",
      "batch: 11205, loss: 0.1255667805671692\n",
      "batch: 11206, loss: 0.037956058979034424\n",
      "batch: 11207, loss: 0.2683955729007721\n",
      "batch: 11208, loss: 0.08937739580869675\n",
      "batch: 11209, loss: 0.15141865611076355\n",
      "batch: 11210, loss: 0.1562652736902237\n",
      "batch: 11211, loss: 0.15800176560878754\n",
      "batch: 11212, loss: 0.2023833692073822\n",
      "batch: 11213, loss: 0.13897719979286194\n",
      "batch: 11214, loss: 0.10019578039646149\n",
      "batch: 11215, loss: 0.08140900731086731\n",
      "batch: 11216, loss: 0.146589457988739\n",
      "batch: 11217, loss: 0.1085573136806488\n",
      "batch: 11218, loss: 0.12955841422080994\n",
      "batch: 11219, loss: 0.27346357703208923\n",
      "batch: 11220, loss: 0.3162981867790222\n",
      "batch: 11221, loss: 0.051874659955501556\n",
      "batch: 11222, loss: 0.11389679461717606\n",
      "batch: 11223, loss: 0.1538797914981842\n",
      "batch: 11224, loss: 0.07282320410013199\n",
      "batch: 11225, loss: 0.11849569529294968\n",
      "batch: 11226, loss: 0.0975026860833168\n",
      "batch: 11227, loss: 0.13305145502090454\n",
      "batch: 11228, loss: 0.17016462981700897\n",
      "batch: 11229, loss: 0.07845942676067352\n",
      "batch: 11230, loss: 0.14103741943836212\n",
      "batch: 11231, loss: 0.17325782775878906\n",
      "batch: 11232, loss: 0.19191503524780273\n",
      "batch: 11233, loss: 0.10688552260398865\n",
      "batch: 11234, loss: 0.11175224930047989\n",
      "batch: 11235, loss: 0.0976727083325386\n",
      "batch: 11236, loss: 0.11957564204931259\n",
      "batch: 11237, loss: 0.2038373500108719\n",
      "batch: 11238, loss: 0.20348979532718658\n",
      "batch: 11239, loss: 0.049206946045160294\n",
      "batch: 11240, loss: 0.10227983444929123\n",
      "batch: 11241, loss: 0.1426398754119873\n",
      "batch: 11242, loss: 0.1225542426109314\n",
      "batch: 11243, loss: 0.05990719795227051\n",
      "batch: 11244, loss: 0.1333775818347931\n",
      "batch: 11245, loss: 0.1166268065571785\n",
      "batch: 11246, loss: 0.06859013438224792\n",
      "batch: 11247, loss: 0.09293919056653976\n",
      "batch: 11248, loss: 0.16194216907024384\n",
      "batch: 11249, loss: 0.10342954099178314\n",
      "batch: 11250, loss: 0.1056920513510704\n",
      "batch: 11251, loss: 0.14775797724723816\n",
      "batch: 11252, loss: 0.13800346851348877\n",
      "batch: 11253, loss: 0.05651026964187622\n",
      "batch: 11254, loss: 0.02197769284248352\n",
      "batch: 11255, loss: 0.0855184867978096\n",
      "batch: 11256, loss: 0.22251808643341064\n",
      "batch: 11257, loss: 0.11036978662014008\n",
      "batch: 11258, loss: 0.06341981887817383\n",
      "batch: 11259, loss: 0.156010702252388\n",
      "batch: 11260, loss: 0.04413221403956413\n",
      "batch: 11261, loss: 0.14513561129570007\n",
      "batch: 11262, loss: 0.08997752517461777\n",
      "batch: 11263, loss: 0.18306025862693787\n",
      "batch: 11264, loss: 0.08013449609279633\n",
      "batch: 11265, loss: 0.29822975397109985\n",
      "batch: 11266, loss: 0.052912451326847076\n",
      "batch: 11267, loss: 0.11112447082996368\n",
      "batch: 11268, loss: 0.15425831079483032\n",
      "batch: 11269, loss: 0.18050682544708252\n",
      "batch: 11270, loss: 0.31766924262046814\n",
      "batch: 11271, loss: 0.11899089813232422\n",
      "batch: 11272, loss: 0.11113276332616806\n",
      "batch: 11273, loss: 0.10924819111824036\n",
      "batch: 11274, loss: 0.054060813039541245\n",
      "batch: 11275, loss: 0.06090438365936279\n",
      "batch: 11276, loss: 0.05067497864365578\n",
      "batch: 11277, loss: 0.07698661088943481\n",
      "batch: 11278, loss: 0.21264463663101196\n",
      "batch: 11279, loss: 0.13489165902137756\n",
      "batch: 11280, loss: 0.1194625049829483\n",
      "batch: 11281, loss: 0.353868305683136\n",
      "batch: 11282, loss: 0.16778309643268585\n",
      "batch: 11283, loss: 0.12763932347297668\n",
      "batch: 11284, loss: 0.1684345006942749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 11285, loss: 0.12300971150398254\n",
      "batch: 11286, loss: 0.2078966200351715\n",
      "batch: 11287, loss: 0.04934293031692505\n",
      "batch: 11288, loss: 0.12759897112846375\n",
      "batch: 11289, loss: 0.07006346434354782\n",
      "batch: 11290, loss: 0.1752939373254776\n",
      "batch: 11291, loss: 0.10395726561546326\n",
      "batch: 11292, loss: 0.07565277814865112\n",
      "batch: 11293, loss: 0.18797984719276428\n",
      "batch: 11294, loss: 0.062188610434532166\n",
      "batch: 11295, loss: 0.07445511221885681\n",
      "batch: 11296, loss: 0.15488868951797485\n",
      "batch: 11297, loss: 0.3276161551475525\n",
      "batch: 11298, loss: 0.22068214416503906\n",
      "batch: 11299, loss: 0.13871963322162628\n",
      "batch: 11300, loss: 0.0995015799999237\n",
      "model saved to ./saving/model.ckpt-113\n",
      "batch: 11301, loss: 0.3611600995063782\n",
      "batch: 11302, loss: 0.23841063678264618\n",
      "batch: 11303, loss: 0.23923882842063904\n",
      "batch: 11304, loss: 0.0648476630449295\n",
      "batch: 11305, loss: 0.05850902944803238\n",
      "batch: 11306, loss: 0.09358979016542435\n",
      "batch: 11307, loss: 0.12519098818302155\n",
      "batch: 11308, loss: 0.3444848954677582\n",
      "batch: 11309, loss: 0.12742824852466583\n",
      "batch: 11310, loss: 0.17821437120437622\n",
      "batch: 11311, loss: 0.08071140944957733\n",
      "batch: 11312, loss: 0.14390675723552704\n",
      "batch: 11313, loss: 0.10034436732530594\n",
      "batch: 11314, loss: 0.13183143734931946\n",
      "batch: 11315, loss: 0.0642397403717041\n",
      "batch: 11316, loss: 0.17382484674453735\n",
      "batch: 11317, loss: 0.13786233961582184\n",
      "batch: 11318, loss: 0.11768200248479843\n",
      "batch: 11319, loss: 0.07440287619829178\n",
      "batch: 11320, loss: 0.08464108407497406\n",
      "batch: 11321, loss: 0.11897530406713486\n",
      "batch: 11322, loss: 0.12340377271175385\n",
      "batch: 11323, loss: 0.21573232114315033\n",
      "batch: 11324, loss: 0.11228577792644501\n",
      "batch: 11325, loss: 0.0916307345032692\n",
      "batch: 11326, loss: 0.11474961787462234\n",
      "batch: 11327, loss: 0.1892370879650116\n",
      "batch: 11328, loss: 0.20021547377109528\n",
      "batch: 11329, loss: 0.1699855476617813\n",
      "batch: 11330, loss: 0.04384871572256088\n",
      "batch: 11331, loss: 0.12125424295663834\n",
      "batch: 11332, loss: 0.15058287978172302\n",
      "batch: 11333, loss: 0.19348874688148499\n",
      "batch: 11334, loss: 0.08513565361499786\n",
      "batch: 11335, loss: 0.1519644409418106\n",
      "batch: 11336, loss: 0.18331274390220642\n",
      "batch: 11337, loss: 0.18761441111564636\n",
      "batch: 11338, loss: 0.22459325194358826\n",
      "batch: 11339, loss: 0.17683863639831543\n",
      "batch: 11340, loss: 0.11246058344841003\n",
      "batch: 11341, loss: 0.11364622414112091\n",
      "batch: 11342, loss: 0.10463638603687286\n",
      "batch: 11343, loss: 0.08423139154911041\n",
      "batch: 11344, loss: 0.15133601427078247\n",
      "batch: 11345, loss: 0.08440233021974564\n",
      "batch: 11346, loss: 0.16315308213233948\n",
      "batch: 11347, loss: 0.13701900839805603\n",
      "batch: 11348, loss: 0.06341131031513214\n",
      "batch: 11349, loss: 0.11584163457155228\n",
      "batch: 11350, loss: 0.24504035711288452\n",
      "batch: 11351, loss: 0.16846446692943573\n",
      "batch: 11352, loss: 0.04324495047330856\n",
      "batch: 11353, loss: 0.22458265721797943\n",
      "batch: 11354, loss: 0.16647213697433472\n",
      "batch: 11355, loss: 0.09479261934757233\n",
      "batch: 11356, loss: 0.20349863171577454\n",
      "batch: 11357, loss: 0.08519206196069717\n",
      "batch: 11358, loss: 0.08073363453149796\n",
      "batch: 11359, loss: 0.056423306465148926\n",
      "batch: 11360, loss: 0.15041384100914001\n",
      "batch: 11361, loss: 0.22965562343597412\n",
      "batch: 11362, loss: 0.10023552179336548\n",
      "batch: 11363, loss: 0.05280044674873352\n",
      "batch: 11364, loss: 0.3690132796764374\n",
      "batch: 11365, loss: 0.10536663234233856\n",
      "batch: 11366, loss: 0.0906410813331604\n",
      "batch: 11367, loss: 0.24196691811084747\n",
      "batch: 11368, loss: 0.04533275216817856\n",
      "batch: 11369, loss: 0.0932980328798294\n",
      "batch: 11370, loss: 0.16381996870040894\n",
      "batch: 11371, loss: 0.20739831030368805\n",
      "batch: 11372, loss: 0.0472324974834919\n",
      "batch: 11373, loss: 0.08369801193475723\n",
      "batch: 11374, loss: 0.22942990064620972\n",
      "batch: 11375, loss: 0.136064350605011\n",
      "batch: 11376, loss: 0.15292541682720184\n",
      "batch: 11377, loss: 0.13644680380821228\n",
      "batch: 11378, loss: 0.19495713710784912\n",
      "batch: 11379, loss: 0.13351181149482727\n",
      "batch: 11380, loss: 0.14957952499389648\n",
      "batch: 11381, loss: 0.07310447096824646\n",
      "batch: 11382, loss: 0.0889916718006134\n",
      "batch: 11383, loss: 0.16796740889549255\n",
      "batch: 11384, loss: 0.20675989985466003\n",
      "batch: 11385, loss: 0.1216551661491394\n",
      "batch: 11386, loss: 0.1689378023147583\n",
      "batch: 11387, loss: 0.08033279329538345\n",
      "batch: 11388, loss: 0.20750992000102997\n",
      "batch: 11389, loss: 0.1787327080965042\n",
      "batch: 11390, loss: 0.02793416567146778\n",
      "batch: 11391, loss: 0.30176669359207153\n",
      "batch: 11392, loss: 0.16141341626644135\n",
      "batch: 11393, loss: 0.16499921679496765\n",
      "batch: 11394, loss: 0.21092712879180908\n",
      "batch: 11395, loss: 0.14163219928741455\n",
      "batch: 11396, loss: 0.0824316143989563\n",
      "batch: 11397, loss: 0.18661171197891235\n",
      "batch: 11398, loss: 0.10077580064535141\n",
      "batch: 11399, loss: 0.10727253556251526\n",
      "batch: 11400, loss: 0.09201662987470627\n",
      "model saved to ./saving/model.ckpt-114\n",
      "batch: 11401, loss: 0.21440064907073975\n",
      "batch: 11402, loss: 0.051281921565532684\n",
      "batch: 11403, loss: 0.09720597416162491\n",
      "batch: 11404, loss: 0.15019279718399048\n",
      "batch: 11405, loss: 0.16186870634555817\n",
      "batch: 11406, loss: 0.1836136430501938\n",
      "batch: 11407, loss: 0.026100262999534607\n",
      "batch: 11408, loss: 0.11266026645898819\n",
      "batch: 11409, loss: 0.20415031909942627\n",
      "batch: 11410, loss: 0.12316589057445526\n",
      "batch: 11411, loss: 0.09825287759304047\n",
      "batch: 11412, loss: 0.17099858820438385\n",
      "batch: 11413, loss: 0.06931956112384796\n",
      "batch: 11414, loss: 0.11901189386844635\n",
      "batch: 11415, loss: 0.10258679836988449\n",
      "batch: 11416, loss: 0.10861235111951828\n",
      "batch: 11417, loss: 0.23203685879707336\n",
      "batch: 11418, loss: 0.17829981446266174\n",
      "batch: 11419, loss: 0.13452434539794922\n",
      "batch: 11420, loss: 0.20244833827018738\n",
      "batch: 11421, loss: 0.10804205387830734\n",
      "batch: 11422, loss: 0.2114502638578415\n",
      "batch: 11423, loss: 0.14309260249137878\n",
      "batch: 11424, loss: 0.11243199557065964\n",
      "batch: 11425, loss: 0.07579552382230759\n",
      "batch: 11426, loss: 0.058296650648117065\n",
      "batch: 11427, loss: 0.07579058408737183\n",
      "batch: 11428, loss: 0.1099013015627861\n",
      "batch: 11429, loss: 0.18363405764102936\n",
      "batch: 11430, loss: 0.06777901947498322\n",
      "batch: 11431, loss: 0.08152328431606293\n",
      "batch: 11432, loss: 0.16039930284023285\n",
      "batch: 11433, loss: 0.04589631035923958\n",
      "batch: 11434, loss: 0.07646939158439636\n",
      "batch: 11435, loss: 0.12197943031787872\n",
      "batch: 11436, loss: 0.06823232024908066\n",
      "batch: 11437, loss: 0.09065532684326172\n",
      "batch: 11438, loss: 0.09736625105142593\n",
      "batch: 11439, loss: 0.1381680965423584\n",
      "batch: 11440, loss: 0.21828192472457886\n",
      "batch: 11441, loss: 0.20685282349586487\n",
      "batch: 11442, loss: 0.10892850160598755\n",
      "batch: 11443, loss: 0.3325095474720001\n",
      "batch: 11444, loss: 0.08520088344812393\n",
      "batch: 11445, loss: 0.10766703635454178\n",
      "batch: 11446, loss: 0.1420036405324936\n",
      "batch: 11447, loss: 0.19954878091812134\n",
      "batch: 11448, loss: 0.14917226135730743\n",
      "batch: 11449, loss: 0.12856847047805786\n",
      "batch: 11450, loss: 0.055975086987018585\n",
      "batch: 11451, loss: 0.11739957332611084\n",
      "batch: 11452, loss: 0.063454270362854\n",
      "batch: 11453, loss: 0.053752295672893524\n",
      "batch: 11454, loss: 0.08461952209472656\n",
      "batch: 11455, loss: 0.085980623960495\n",
      "batch: 11456, loss: 0.27768170833587646\n",
      "batch: 11457, loss: 0.14957532286643982\n",
      "batch: 11458, loss: 0.07908519357442856\n",
      "batch: 11459, loss: 0.12680043280124664\n",
      "batch: 11460, loss: 0.1791476160287857\n",
      "batch: 11461, loss: 0.5119071006774902\n",
      "batch: 11462, loss: 0.11977817863225937\n",
      "batch: 11463, loss: 0.27226629853248596\n",
      "batch: 11464, loss: 0.12007280439138412\n",
      "batch: 11465, loss: 0.060962364077568054\n",
      "batch: 11466, loss: 0.1237776055932045\n",
      "batch: 11467, loss: 0.13056564331054688\n",
      "batch: 11468, loss: 0.12005162239074707\n",
      "batch: 11469, loss: 0.2907816171646118\n",
      "batch: 11470, loss: 0.04164775088429451\n",
      "batch: 11471, loss: 0.2011781632900238\n",
      "batch: 11472, loss: 0.049568839371204376\n",
      "batch: 11473, loss: 0.103268563747406\n",
      "batch: 11474, loss: 0.17960023880004883\n",
      "batch: 11475, loss: 0.08702406287193298\n",
      "batch: 11476, loss: 0.1052800640463829\n",
      "batch: 11477, loss: 0.08499053120613098\n",
      "batch: 11478, loss: 0.15550148487091064\n",
      "batch: 11479, loss: 0.04900369048118591\n",
      "batch: 11480, loss: 0.1443856954574585\n",
      "batch: 11481, loss: 0.2166365534067154\n",
      "batch: 11482, loss: 0.10839289426803589\n",
      "batch: 11483, loss: 0.16557958722114563\n",
      "batch: 11484, loss: 0.08599693328142166\n",
      "batch: 11485, loss: 0.1689327359199524\n",
      "batch: 11486, loss: 0.11331842839717865\n",
      "batch: 11487, loss: 0.1426202952861786\n",
      "batch: 11488, loss: 0.1157086193561554\n",
      "batch: 11489, loss: 0.04586406052112579\n",
      "batch: 11490, loss: 0.034006938338279724\n",
      "batch: 11491, loss: 0.17028549313545227\n",
      "batch: 11492, loss: 0.1055469810962677\n",
      "batch: 11493, loss: 0.05143727362155914\n",
      "batch: 11494, loss: 0.055096011608839035\n",
      "batch: 11495, loss: 0.10373492538928986\n",
      "batch: 11496, loss: 0.15912435948848724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 11497, loss: 0.15826106071472168\n",
      "batch: 11498, loss: 0.1576472520828247\n",
      "batch: 11499, loss: 0.08084526658058167\n",
      "batch: 11500, loss: 0.12833844125270844\n",
      "model saved to ./saving/model.ckpt-115\n",
      "batch: 11501, loss: 0.1265580654144287\n",
      "batch: 11502, loss: 0.09786352515220642\n",
      "batch: 11503, loss: 0.062654510140419\n",
      "batch: 11504, loss: 0.09411290287971497\n",
      "batch: 11505, loss: 0.14470577239990234\n",
      "batch: 11506, loss: 0.04614340513944626\n",
      "batch: 11507, loss: 0.19524718821048737\n",
      "batch: 11508, loss: 0.2151399552822113\n",
      "batch: 11509, loss: 0.12917982041835785\n",
      "batch: 11510, loss: 0.06395446509122849\n",
      "batch: 11511, loss: 0.16862134635448456\n",
      "batch: 11512, loss: 0.15408074855804443\n",
      "batch: 11513, loss: 0.19737555086612701\n",
      "batch: 11514, loss: 0.16968831419944763\n",
      "batch: 11515, loss: 0.21291708946228027\n",
      "batch: 11516, loss: 0.13692468404769897\n",
      "batch: 11517, loss: 0.09201017767190933\n",
      "batch: 11518, loss: 0.07188625633716583\n",
      "batch: 11519, loss: 0.12180641293525696\n",
      "batch: 11520, loss: 0.23640306293964386\n",
      "batch: 11521, loss: 0.15745405852794647\n",
      "batch: 11522, loss: 0.11300495266914368\n",
      "batch: 11523, loss: 0.2073001265525818\n",
      "batch: 11524, loss: 0.10151588916778564\n",
      "batch: 11525, loss: 0.09090600907802582\n",
      "batch: 11526, loss: 0.23673254251480103\n",
      "batch: 11527, loss: 0.2897288203239441\n",
      "batch: 11528, loss: 0.11489467322826385\n",
      "batch: 11529, loss: 0.0771406888961792\n",
      "batch: 11530, loss: 0.1746646612882614\n",
      "batch: 11531, loss: 0.08872858434915543\n",
      "batch: 11532, loss: 0.2130625694990158\n",
      "batch: 11533, loss: 0.09559345245361328\n",
      "batch: 11534, loss: 0.058329977095127106\n",
      "batch: 11535, loss: 0.14734962582588196\n",
      "batch: 11536, loss: 0.18023450672626495\n",
      "batch: 11537, loss: 0.24160319566726685\n",
      "batch: 11538, loss: 0.22981083393096924\n",
      "batch: 11539, loss: 0.1758066713809967\n",
      "batch: 11540, loss: 0.2684822082519531\n",
      "batch: 11541, loss: 0.08130181580781937\n",
      "batch: 11542, loss: 0.07625620812177658\n",
      "batch: 11543, loss: 0.10655966401100159\n",
      "batch: 11544, loss: 0.14300642907619476\n",
      "batch: 11545, loss: 0.34862345457077026\n",
      "batch: 11546, loss: 0.07267120480537415\n",
      "batch: 11547, loss: 0.19705034792423248\n",
      "batch: 11548, loss: 0.10139234364032745\n",
      "batch: 11549, loss: 0.10707178711891174\n",
      "batch: 11550, loss: 0.09013885259628296\n",
      "batch: 11551, loss: 0.08820027112960815\n",
      "batch: 11552, loss: 0.20334771275520325\n",
      "batch: 11553, loss: 0.20420217514038086\n",
      "batch: 11554, loss: 0.07195471227169037\n",
      "batch: 11555, loss: 0.1241762563586235\n",
      "batch: 11556, loss: 0.06397122889757156\n",
      "batch: 11557, loss: 0.18450160324573517\n",
      "batch: 11558, loss: 0.08163393288850784\n",
      "batch: 11559, loss: 0.050080541521310806\n",
      "batch: 11560, loss: 0.048788659274578094\n",
      "batch: 11561, loss: 0.05531178414821625\n",
      "batch: 11562, loss: 0.1482485830783844\n",
      "batch: 11563, loss: 0.22467713057994843\n",
      "batch: 11564, loss: 0.07726859301328659\n",
      "batch: 11565, loss: 0.28073662519454956\n",
      "batch: 11566, loss: 0.2678462266921997\n",
      "batch: 11567, loss: 0.22880446910858154\n",
      "batch: 11568, loss: 0.09649544954299927\n",
      "batch: 11569, loss: 0.1923062950372696\n",
      "batch: 11570, loss: 0.12323977053165436\n",
      "batch: 11571, loss: 0.18413448333740234\n",
      "batch: 11572, loss: 0.0634850412607193\n",
      "batch: 11573, loss: 0.10666154325008392\n",
      "batch: 11574, loss: 0.17355318367481232\n",
      "batch: 11575, loss: 0.15588343143463135\n",
      "batch: 11576, loss: 0.24793723225593567\n",
      "batch: 11577, loss: 0.38213643431663513\n",
      "batch: 11578, loss: 0.15727868676185608\n",
      "batch: 11579, loss: 0.3522112965583801\n",
      "batch: 11580, loss: 0.25338104367256165\n",
      "batch: 11581, loss: 0.1499064862728119\n",
      "batch: 11582, loss: 0.2551336884498596\n",
      "batch: 11583, loss: 0.08017592132091522\n",
      "batch: 11584, loss: 0.055175308138132095\n",
      "batch: 11585, loss: 0.14724278450012207\n",
      "batch: 11586, loss: 0.12367629259824753\n",
      "batch: 11587, loss: 0.14174845814704895\n",
      "batch: 11588, loss: 0.1162906065583229\n",
      "batch: 11589, loss: 0.10546854138374329\n",
      "batch: 11590, loss: 0.08475024998188019\n",
      "batch: 11591, loss: 0.1511097401380539\n",
      "batch: 11592, loss: 0.22590593993663788\n",
      "batch: 11593, loss: 0.41384685039520264\n",
      "batch: 11594, loss: 0.0518423430621624\n",
      "batch: 11595, loss: 0.12015986442565918\n",
      "batch: 11596, loss: 0.05640522018074989\n",
      "batch: 11597, loss: 0.0568246953189373\n",
      "batch: 11598, loss: 0.12645938992500305\n",
      "batch: 11599, loss: 0.07713083177804947\n",
      "batch: 11600, loss: 0.08148026466369629\n",
      "model saved to ./saving/model.ckpt-116\n",
      "batch: 11601, loss: 0.12226592749357224\n",
      "batch: 11602, loss: 0.2591681182384491\n",
      "batch: 11603, loss: 0.1637670248746872\n",
      "batch: 11604, loss: 0.11407719552516937\n",
      "batch: 11605, loss: 0.09014417231082916\n",
      "batch: 11606, loss: 0.08348862081766129\n",
      "batch: 11607, loss: 0.0774230882525444\n",
      "batch: 11608, loss: 0.14298374950885773\n",
      "batch: 11609, loss: 0.1131085455417633\n",
      "batch: 11610, loss: 0.14893858134746552\n",
      "batch: 11611, loss: 0.0791497528553009\n",
      "batch: 11612, loss: 0.06890536099672318\n",
      "batch: 11613, loss: 0.10743190348148346\n",
      "batch: 11614, loss: 0.04801841825246811\n",
      "batch: 11615, loss: 0.1135159432888031\n",
      "batch: 11616, loss: 0.07854606211185455\n",
      "batch: 11617, loss: 0.0757361650466919\n",
      "batch: 11618, loss: 0.0952116996049881\n",
      "batch: 11619, loss: 0.09076687693595886\n",
      "batch: 11620, loss: 0.27555882930755615\n",
      "batch: 11621, loss: 0.14122869074344635\n",
      "batch: 11622, loss: 0.21678034961223602\n",
      "batch: 11623, loss: 0.10790276527404785\n",
      "batch: 11624, loss: 0.1086030900478363\n",
      "batch: 11625, loss: 0.09511517733335495\n",
      "batch: 11626, loss: 0.05867399275302887\n",
      "batch: 11627, loss: 0.16518521308898926\n",
      "batch: 11628, loss: 0.1719365417957306\n",
      "batch: 11629, loss: 0.08195200562477112\n",
      "batch: 11630, loss: 0.09880797564983368\n",
      "batch: 11631, loss: 0.08916521072387695\n",
      "batch: 11632, loss: 0.14373646676540375\n",
      "batch: 11633, loss: 0.09997718781232834\n",
      "batch: 11634, loss: 0.12312807142734528\n",
      "batch: 11635, loss: 0.0736888125538826\n",
      "batch: 11636, loss: 0.17249293625354767\n",
      "batch: 11637, loss: 0.1902785450220108\n",
      "batch: 11638, loss: 0.05091340094804764\n",
      "batch: 11639, loss: 0.3347589373588562\n",
      "batch: 11640, loss: 0.03880365937948227\n",
      "batch: 11641, loss: 0.09064214676618576\n",
      "batch: 11642, loss: 0.1802734136581421\n",
      "batch: 11643, loss: 0.12996059656143188\n",
      "batch: 11644, loss: 0.09299274533987045\n",
      "batch: 11645, loss: 0.1665804535150528\n",
      "batch: 11646, loss: 0.2718026638031006\n",
      "batch: 11647, loss: 0.18018481135368347\n",
      "batch: 11648, loss: 0.2534208297729492\n",
      "batch: 11649, loss: 0.07540303468704224\n",
      "batch: 11650, loss: 0.11900030821561813\n",
      "batch: 11651, loss: 0.06528373062610626\n",
      "batch: 11652, loss: 0.24849390983581543\n",
      "batch: 11653, loss: 0.21380504965782166\n",
      "batch: 11654, loss: 0.348591685295105\n",
      "batch: 11655, loss: 0.09784086048603058\n",
      "batch: 11656, loss: 0.14512915909290314\n",
      "batch: 11657, loss: 0.039004553109407425\n",
      "batch: 11658, loss: 0.10088956356048584\n",
      "batch: 11659, loss: 0.25312113761901855\n",
      "batch: 11660, loss: 0.26792052388191223\n",
      "batch: 11661, loss: 0.24003425240516663\n",
      "batch: 11662, loss: 0.11939236521720886\n",
      "batch: 11663, loss: 0.17741379141807556\n",
      "batch: 11664, loss: 0.10706950724124908\n",
      "batch: 11665, loss: 0.17457355558872223\n",
      "batch: 11666, loss: 0.13339829444885254\n",
      "batch: 11667, loss: 0.09708653390407562\n",
      "batch: 11668, loss: 0.07846783101558685\n",
      "batch: 11669, loss: 0.04843582957983017\n",
      "batch: 11670, loss: 0.14614948630332947\n",
      "batch: 11671, loss: 0.22963717579841614\n",
      "batch: 11672, loss: 0.21587547659873962\n",
      "batch: 11673, loss: 0.07902082800865173\n",
      "batch: 11674, loss: 0.16108623147010803\n",
      "batch: 11675, loss: 0.09487326443195343\n",
      "batch: 11676, loss: 0.10914672911167145\n",
      "batch: 11677, loss: 0.17206183075904846\n",
      "batch: 11678, loss: 0.05912307649850845\n",
      "batch: 11679, loss: 0.10791495442390442\n",
      "batch: 11680, loss: 0.08011695742607117\n",
      "batch: 11681, loss: 0.07393350452184677\n",
      "batch: 11682, loss: 0.22246265411376953\n",
      "batch: 11683, loss: 0.1595860719680786\n",
      "batch: 11684, loss: 0.22030727565288544\n",
      "batch: 11685, loss: 0.04882507771253586\n",
      "batch: 11686, loss: 0.1293746381998062\n",
      "batch: 11687, loss: 0.1389973908662796\n",
      "batch: 11688, loss: 0.12135306000709534\n",
      "batch: 11689, loss: 0.2659617066383362\n",
      "batch: 11690, loss: 0.11721483618021011\n",
      "batch: 11691, loss: 0.09426434338092804\n",
      "batch: 11692, loss: 0.041980668902397156\n",
      "batch: 11693, loss: 0.0902411937713623\n",
      "batch: 11694, loss: 0.1649332344532013\n",
      "batch: 11695, loss: 0.09588317573070526\n",
      "batch: 11696, loss: 0.060711465775966644\n",
      "batch: 11697, loss: 0.13156822323799133\n",
      "batch: 11698, loss: 0.0792018473148346\n",
      "batch: 11699, loss: 0.06357957422733307\n",
      "batch: 11700, loss: 0.03774416074156761\n",
      "model saved to ./saving/model.ckpt-117\n",
      "batch: 11701, loss: 0.07790951430797577\n",
      "batch: 11702, loss: 0.1004597395658493\n",
      "batch: 11703, loss: 0.11373017728328705\n",
      "batch: 11704, loss: 0.20146290957927704\n",
      "batch: 11705, loss: 0.05571218580007553\n",
      "batch: 11706, loss: 0.07009783387184143\n",
      "batch: 11707, loss: 0.1967707872390747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 11708, loss: 0.07752326130867004\n",
      "batch: 11709, loss: 0.044677138328552246\n",
      "batch: 11710, loss: 0.14943593740463257\n",
      "batch: 11711, loss: 0.09310834109783173\n",
      "batch: 11712, loss: 0.1439441740512848\n",
      "batch: 11713, loss: 0.11447034776210785\n",
      "batch: 11714, loss: 0.07346786558628082\n",
      "batch: 11715, loss: 0.09121310710906982\n",
      "batch: 11716, loss: 0.20813286304473877\n",
      "batch: 11717, loss: 0.22459471225738525\n",
      "batch: 11718, loss: 0.10323522984981537\n",
      "batch: 11719, loss: 0.0759829580783844\n",
      "batch: 11720, loss: 0.23991698026657104\n",
      "batch: 11721, loss: 0.19509252905845642\n",
      "batch: 11722, loss: 0.06514149904251099\n",
      "batch: 11723, loss: 0.11274655908346176\n",
      "batch: 11724, loss: 0.16057312488555908\n",
      "batch: 11725, loss: 0.29024696350097656\n",
      "batch: 11726, loss: 0.15127068758010864\n",
      "batch: 11727, loss: 0.08361730724573135\n",
      "batch: 11728, loss: 0.11556753516197205\n",
      "batch: 11729, loss: 0.23539075255393982\n",
      "batch: 11730, loss: 0.12010157853364944\n",
      "batch: 11731, loss: 0.12031534314155579\n",
      "batch: 11732, loss: 0.09810775518417358\n",
      "batch: 11733, loss: 0.07807508111000061\n",
      "batch: 11734, loss: 0.12281571328639984\n",
      "batch: 11735, loss: 0.0663662776350975\n",
      "batch: 11736, loss: 0.08433250337839127\n",
      "batch: 11737, loss: 0.2609191834926605\n",
      "batch: 11738, loss: 0.11827820539474487\n",
      "batch: 11739, loss: 0.17884334921836853\n",
      "batch: 11740, loss: 0.18444018065929413\n",
      "batch: 11741, loss: 0.16945554316043854\n",
      "batch: 11742, loss: 0.2801295518875122\n",
      "batch: 11743, loss: 0.1631767749786377\n",
      "batch: 11744, loss: 0.05441431328654289\n",
      "batch: 11745, loss: 0.16443036496639252\n",
      "batch: 11746, loss: 0.1502457559108734\n",
      "batch: 11747, loss: 0.17891129851341248\n",
      "batch: 11748, loss: 0.21144291758537292\n",
      "batch: 11749, loss: 0.17390142381191254\n",
      "batch: 11750, loss: 0.04339853674173355\n",
      "batch: 11751, loss: 0.2288801372051239\n",
      "batch: 11752, loss: 0.14530450105667114\n",
      "batch: 11753, loss: 0.1748737096786499\n",
      "batch: 11754, loss: 0.03698508441448212\n",
      "batch: 11755, loss: 0.2471865564584732\n",
      "batch: 11756, loss: 0.12695275247097015\n",
      "batch: 11757, loss: 0.07436884939670563\n",
      "batch: 11758, loss: 0.1322835385799408\n",
      "batch: 11759, loss: 0.14681902527809143\n",
      "batch: 11760, loss: 0.04346039146184921\n",
      "batch: 11761, loss: 0.049358855932950974\n",
      "batch: 11762, loss: 0.18163058161735535\n",
      "batch: 11763, loss: 0.14156244695186615\n",
      "batch: 11764, loss: 0.06555413454771042\n",
      "batch: 11765, loss: 0.11683957278728485\n",
      "batch: 11766, loss: 0.14846785366535187\n",
      "batch: 11767, loss: 0.10655426979064941\n",
      "batch: 11768, loss: 0.08604102581739426\n",
      "batch: 11769, loss: 0.1383032649755478\n",
      "batch: 11770, loss: 0.07740889489650726\n",
      "batch: 11771, loss: 0.13954132795333862\n",
      "batch: 11772, loss: 0.07590024173259735\n",
      "batch: 11773, loss: 0.10547716915607452\n",
      "batch: 11774, loss: 0.10995997488498688\n",
      "batch: 11775, loss: 0.15238440036773682\n",
      "batch: 11776, loss: 0.08605273813009262\n",
      "batch: 11777, loss: 0.1215955913066864\n",
      "batch: 11778, loss: 0.0915975347161293\n",
      "batch: 11779, loss: 0.08610820770263672\n",
      "batch: 11780, loss: 0.19779498875141144\n",
      "batch: 11781, loss: 0.08021754026412964\n",
      "batch: 11782, loss: 0.04985988885164261\n",
      "batch: 11783, loss: 0.16161981225013733\n",
      "batch: 11784, loss: 0.09982617199420929\n",
      "batch: 11785, loss: 0.11978371441364288\n",
      "batch: 11786, loss: 0.10244981199502945\n",
      "batch: 11787, loss: 0.2695440649986267\n",
      "batch: 11788, loss: 0.1709313839673996\n",
      "batch: 11789, loss: 0.08318555355072021\n",
      "batch: 11790, loss: 0.12219506502151489\n",
      "batch: 11791, loss: 0.06863241642713547\n",
      "batch: 11792, loss: 0.0646522119641304\n",
      "batch: 11793, loss: 0.11755352467298508\n",
      "batch: 11794, loss: 0.10780948400497437\n",
      "batch: 11795, loss: 0.17517465353012085\n",
      "batch: 11796, loss: 0.08682583272457123\n",
      "batch: 11797, loss: 0.15354953706264496\n",
      "batch: 11798, loss: 0.26696687936782837\n",
      "batch: 11799, loss: 0.05653619021177292\n",
      "batch: 11800, loss: 0.17075911164283752\n",
      "model saved to ./saving/model.ckpt-118\n",
      "batch: 11801, loss: 0.22133737802505493\n",
      "batch: 11802, loss: 0.12368477880954742\n",
      "batch: 11803, loss: 0.2948511838912964\n",
      "batch: 11804, loss: 0.1405341476202011\n",
      "batch: 11805, loss: 0.08457716554403305\n",
      "batch: 11806, loss: 0.07859048247337341\n",
      "batch: 11807, loss: 0.1850130409002304\n",
      "batch: 11808, loss: 0.19465065002441406\n",
      "batch: 11809, loss: 0.05572514608502388\n",
      "batch: 11810, loss: 0.1524326205253601\n",
      "batch: 11811, loss: 0.15620486438274384\n",
      "batch: 11812, loss: 0.2721046209335327\n",
      "batch: 11813, loss: 0.16840916872024536\n",
      "batch: 11814, loss: 0.1167764961719513\n",
      "batch: 11815, loss: 0.05559263378381729\n",
      "batch: 11816, loss: 0.07673518359661102\n",
      "batch: 11817, loss: 0.25745144486427307\n",
      "batch: 11818, loss: 0.2561262249946594\n",
      "batch: 11819, loss: 0.09899212419986725\n",
      "batch: 11820, loss: 0.09712360799312592\n",
      "batch: 11821, loss: 0.11004561930894852\n",
      "batch: 11822, loss: 0.10275782644748688\n",
      "batch: 11823, loss: 0.0707692950963974\n",
      "batch: 11824, loss: 0.201555997133255\n",
      "batch: 11825, loss: 0.061876386404037476\n",
      "batch: 11826, loss: 0.21491093933582306\n",
      "batch: 11827, loss: 0.12942197918891907\n",
      "batch: 11828, loss: 0.05825592204928398\n",
      "batch: 11829, loss: 0.1369887888431549\n",
      "batch: 11830, loss: 0.09836582094430923\n",
      "batch: 11831, loss: 0.08845818787813187\n",
      "batch: 11832, loss: 0.13052842020988464\n",
      "batch: 11833, loss: 0.11750862747430801\n",
      "batch: 11834, loss: 0.12212028354406357\n",
      "batch: 11835, loss: 0.2405712902545929\n",
      "batch: 11836, loss: 0.15333816409111023\n",
      "batch: 11837, loss: 0.1406470388174057\n",
      "batch: 11838, loss: 0.17063675820827484\n",
      "batch: 11839, loss: 0.06985314190387726\n",
      "batch: 11840, loss: 0.24041974544525146\n",
      "batch: 11841, loss: 0.09851668775081635\n",
      "batch: 11842, loss: 0.16999465227127075\n",
      "batch: 11843, loss: 0.03581615164875984\n",
      "batch: 11844, loss: 0.053683772683143616\n",
      "batch: 11845, loss: 0.2317558079957962\n",
      "batch: 11846, loss: 0.096421979367733\n",
      "batch: 11847, loss: 0.07757081091403961\n",
      "batch: 11848, loss: 0.18845096230506897\n",
      "batch: 11849, loss: 0.0480056032538414\n",
      "batch: 11850, loss: 0.1642586886882782\n",
      "batch: 11851, loss: 0.10553675889968872\n",
      "batch: 11852, loss: 0.10583890974521637\n",
      "batch: 11853, loss: 0.17452920973300934\n",
      "batch: 11854, loss: 0.1873771846294403\n",
      "batch: 11855, loss: 0.0488957017660141\n",
      "batch: 11856, loss: 0.10439455509185791\n",
      "batch: 11857, loss: 0.1494809240102768\n",
      "batch: 11858, loss: 0.08288644254207611\n",
      "batch: 11859, loss: 0.09420795738697052\n",
      "batch: 11860, loss: 0.1148524358868599\n",
      "batch: 11861, loss: 0.10879166424274445\n",
      "batch: 11862, loss: 0.16102057695388794\n",
      "batch: 11863, loss: 0.06086094304919243\n",
      "batch: 11864, loss: 0.1973564773797989\n",
      "batch: 11865, loss: 0.09548301994800568\n",
      "batch: 11866, loss: 0.16604775190353394\n",
      "batch: 11867, loss: 0.09015487134456635\n",
      "batch: 11868, loss: 0.1175532415509224\n",
      "batch: 11869, loss: 0.05832163617014885\n",
      "batch: 11870, loss: 0.08461253345012665\n",
      "batch: 11871, loss: 0.08416294306516647\n",
      "batch: 11872, loss: 0.32324516773223877\n",
      "batch: 11873, loss: 0.07731948792934418\n",
      "batch: 11874, loss: 0.2216024398803711\n",
      "batch: 11875, loss: 0.09105589985847473\n",
      "batch: 11876, loss: 0.04061417654156685\n",
      "batch: 11877, loss: 0.035805266350507736\n",
      "batch: 11878, loss: 0.13365840911865234\n",
      "batch: 11879, loss: 0.1219245046377182\n",
      "batch: 11880, loss: 0.0701628103852272\n",
      "batch: 11881, loss: 0.2707863450050354\n",
      "batch: 11882, loss: 0.2686154246330261\n",
      "batch: 11883, loss: 0.10943147540092468\n",
      "batch: 11884, loss: 0.10266926884651184\n",
      "batch: 11885, loss: 0.11111599951982498\n",
      "batch: 11886, loss: 0.05723009631037712\n",
      "batch: 11887, loss: 0.08087989687919617\n",
      "batch: 11888, loss: 0.06857330352067947\n",
      "batch: 11889, loss: 0.048306792974472046\n",
      "batch: 11890, loss: 0.09329879283905029\n",
      "batch: 11891, loss: 0.1805804967880249\n",
      "batch: 11892, loss: 0.18129439651966095\n",
      "batch: 11893, loss: 0.16229963302612305\n",
      "batch: 11894, loss: 0.07894293963909149\n",
      "batch: 11895, loss: 0.25494077801704407\n",
      "batch: 11896, loss: 0.17021752893924713\n",
      "batch: 11897, loss: 0.06743782758712769\n",
      "batch: 11898, loss: 0.1305769830942154\n",
      "batch: 11899, loss: 0.26966843008995056\n",
      "batch: 11900, loss: 0.21823875606060028\n",
      "model saved to ./saving/model.ckpt-119\n",
      "batch: 11901, loss: 0.12088455259799957\n",
      "batch: 11902, loss: 0.25509732961654663\n",
      "batch: 11903, loss: 0.1167752668261528\n",
      "batch: 11904, loss: 0.1493881344795227\n",
      "batch: 11905, loss: 0.09342019259929657\n",
      "batch: 11906, loss: 0.16244328022003174\n",
      "batch: 11907, loss: 0.10068117082118988\n",
      "batch: 11908, loss: 0.20761752128601074\n",
      "batch: 11909, loss: 0.09732189774513245\n",
      "batch: 11910, loss: 0.10760334134101868\n",
      "batch: 11911, loss: 0.04511747509241104\n",
      "batch: 11912, loss: 0.11257289350032806\n",
      "batch: 11913, loss: 0.16740521788597107\n",
      "batch: 11914, loss: 0.08698295056819916\n",
      "batch: 11915, loss: 0.11294032633304596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 11916, loss: 0.1836639940738678\n",
      "batch: 11917, loss: 0.1455911546945572\n",
      "batch: 11918, loss: 0.20078806579113007\n",
      "batch: 11919, loss: 0.09997673332691193\n",
      "batch: 11920, loss: 0.07823474705219269\n",
      "batch: 11921, loss: 0.14000046253204346\n",
      "batch: 11922, loss: 0.10445237159729004\n",
      "batch: 11923, loss: 0.15525245666503906\n",
      "batch: 11924, loss: 0.14729154109954834\n",
      "batch: 11925, loss: 0.10468659549951553\n",
      "batch: 11926, loss: 0.1298113763332367\n",
      "batch: 11927, loss: 0.12032080441713333\n",
      "batch: 11928, loss: 0.12353832274675369\n",
      "batch: 11929, loss: 0.10591000318527222\n",
      "batch: 11930, loss: 0.08924292027950287\n",
      "batch: 11931, loss: 0.08606148511171341\n",
      "batch: 11932, loss: 0.12181941419839859\n",
      "batch: 11933, loss: 0.06997200846672058\n",
      "batch: 11934, loss: 0.06627127528190613\n",
      "batch: 11935, loss: 0.10168035328388214\n",
      "batch: 11936, loss: 0.15984928607940674\n",
      "batch: 11937, loss: 0.07101128995418549\n",
      "batch: 11938, loss: 0.3409282863140106\n",
      "batch: 11939, loss: 0.14394275844097137\n",
      "batch: 11940, loss: 0.10037541389465332\n",
      "batch: 11941, loss: 0.11699485778808594\n",
      "batch: 11942, loss: 0.11210298538208008\n",
      "batch: 11943, loss: 0.22706831991672516\n",
      "batch: 11944, loss: 0.043237969279289246\n",
      "batch: 11945, loss: 0.137821227312088\n",
      "batch: 11946, loss: 0.10917838662862778\n",
      "batch: 11947, loss: 0.17079073190689087\n",
      "batch: 11948, loss: 0.06815269589424133\n",
      "batch: 11949, loss: 0.15938910841941833\n",
      "batch: 11950, loss: 0.05933251976966858\n",
      "batch: 11951, loss: 0.17924848198890686\n",
      "batch: 11952, loss: 0.15713170170783997\n",
      "batch: 11953, loss: 0.10064637660980225\n",
      "batch: 11954, loss: 0.18307258188724518\n",
      "batch: 11955, loss: 0.0696297436952591\n",
      "batch: 11956, loss: 0.20703351497650146\n",
      "batch: 11957, loss: 0.11239832639694214\n",
      "batch: 11958, loss: 0.12854118645191193\n",
      "batch: 11959, loss: 0.19039404392242432\n",
      "batch: 11960, loss: 0.14811484515666962\n",
      "batch: 11961, loss: 0.1928441971540451\n",
      "batch: 11962, loss: 0.167322039604187\n",
      "batch: 11963, loss: 0.12710843980312347\n",
      "batch: 11964, loss: 0.14917947351932526\n",
      "batch: 11965, loss: 0.11549703031778336\n",
      "batch: 11966, loss: 0.09812857955694199\n",
      "batch: 11967, loss: 0.0669257640838623\n",
      "batch: 11968, loss: 0.1530509889125824\n",
      "batch: 11969, loss: 0.11814998835325241\n",
      "batch: 11970, loss: 0.1784493625164032\n",
      "batch: 11971, loss: 0.09021303057670593\n",
      "batch: 11972, loss: 0.1300281584262848\n",
      "batch: 11973, loss: 0.14883649349212646\n",
      "batch: 11974, loss: 0.12472420930862427\n",
      "batch: 11975, loss: 0.11454164981842041\n",
      "batch: 11976, loss: 0.14784811437129974\n",
      "batch: 11977, loss: 0.13360418379306793\n",
      "batch: 11978, loss: 0.07066522538661957\n",
      "batch: 11979, loss: 0.11407960951328278\n",
      "batch: 11980, loss: 0.1367315798997879\n",
      "batch: 11981, loss: 0.18563953042030334\n",
      "batch: 11982, loss: 0.06858152896165848\n",
      "batch: 11983, loss: 0.09021010994911194\n",
      "batch: 11984, loss: 0.12011513113975525\n",
      "batch: 11985, loss: 0.22494813799858093\n",
      "batch: 11986, loss: 0.14671146869659424\n",
      "batch: 11987, loss: 0.09700526297092438\n",
      "batch: 11988, loss: 0.15843968093395233\n",
      "batch: 11989, loss: 0.26032519340515137\n",
      "batch: 11990, loss: 0.09089881181716919\n",
      "batch: 11991, loss: 0.05627813935279846\n",
      "batch: 11992, loss: 0.09892575442790985\n",
      "batch: 11993, loss: 0.10674919188022614\n",
      "batch: 11994, loss: 0.1076994463801384\n",
      "batch: 11995, loss: 0.16651590168476105\n",
      "batch: 11996, loss: 0.2554008364677429\n",
      "batch: 11997, loss: 0.1912485957145691\n",
      "batch: 11998, loss: 0.16870179772377014\n",
      "batch: 11999, loss: 0.038470856845378876\n",
      "batch: 12000, loss: 0.1211056336760521\n",
      "model saved to ./saving/model.ckpt-120\n",
      "batch: 12001, loss: 0.08527744561433792\n",
      "batch: 12002, loss: 0.14433664083480835\n",
      "batch: 12003, loss: 0.08668598532676697\n",
      "batch: 12004, loss: 0.17313359677791595\n",
      "batch: 12005, loss: 0.07065050303936005\n",
      "batch: 12006, loss: 0.12905541062355042\n",
      "batch: 12007, loss: 0.2051878273487091\n",
      "batch: 12008, loss: 0.10020965337753296\n",
      "batch: 12009, loss: 0.03551868349313736\n",
      "batch: 12010, loss: 0.11349223554134369\n",
      "batch: 12011, loss: 0.1499490737915039\n",
      "batch: 12012, loss: 0.04737934470176697\n",
      "batch: 12013, loss: 0.16193166375160217\n",
      "batch: 12014, loss: 0.1578419953584671\n",
      "batch: 12015, loss: 0.09743504226207733\n",
      "batch: 12016, loss: 0.20271079242229462\n",
      "batch: 12017, loss: 0.10517752170562744\n",
      "batch: 12018, loss: 0.10031071305274963\n",
      "batch: 12019, loss: 0.24460074305534363\n",
      "batch: 12020, loss: 0.041188083589076996\n",
      "batch: 12021, loss: 0.1606510579586029\n",
      "batch: 12022, loss: 0.05324004217982292\n",
      "batch: 12023, loss: 0.041464053094387054\n",
      "batch: 12024, loss: 0.09250133484601974\n",
      "batch: 12025, loss: 0.07497571408748627\n",
      "batch: 12026, loss: 0.02382628619670868\n",
      "batch: 12027, loss: 0.12812036275863647\n",
      "batch: 12028, loss: 0.06317475438117981\n",
      "batch: 12029, loss: 0.16536933183670044\n",
      "batch: 12030, loss: 0.07003910094499588\n",
      "batch: 12031, loss: 0.11407394707202911\n",
      "batch: 12032, loss: 0.14658650755882263\n",
      "batch: 12033, loss: 0.2468123733997345\n",
      "batch: 12034, loss: 0.11848274618387222\n",
      "batch: 12035, loss: 0.07056954503059387\n",
      "batch: 12036, loss: 0.02513151615858078\n",
      "batch: 12037, loss: 0.09552004188299179\n",
      "batch: 12038, loss: 0.09810931235551834\n",
      "batch: 12039, loss: 0.14656762778759003\n",
      "batch: 12040, loss: 0.10863534361124039\n",
      "batch: 12041, loss: 0.15309089422225952\n",
      "batch: 12042, loss: 0.09605471789836884\n",
      "batch: 12043, loss: 0.13028395175933838\n",
      "batch: 12044, loss: 0.11057062447071075\n",
      "batch: 12045, loss: 0.2537558674812317\n",
      "batch: 12046, loss: 0.15147918462753296\n",
      "batch: 12047, loss: 0.122886061668396\n",
      "batch: 12048, loss: 0.07081045210361481\n",
      "batch: 12049, loss: 0.11438821256160736\n",
      "batch: 12050, loss: 0.11478638648986816\n",
      "batch: 12051, loss: 0.12096381187438965\n",
      "batch: 12052, loss: 0.08867848664522171\n",
      "batch: 12053, loss: 0.21584340929985046\n",
      "batch: 12054, loss: 0.1852632462978363\n",
      "batch: 12055, loss: 0.244652658700943\n",
      "batch: 12056, loss: 0.1393989622592926\n",
      "batch: 12057, loss: 0.09269332140684128\n",
      "batch: 12058, loss: 0.12736332416534424\n",
      "batch: 12059, loss: 0.1574377417564392\n",
      "batch: 12060, loss: 0.10311570763587952\n",
      "batch: 12061, loss: 0.05945180356502533\n",
      "batch: 12062, loss: 0.04142530634999275\n",
      "batch: 12063, loss: 0.12341635674238205\n",
      "batch: 12064, loss: 0.22708982229232788\n",
      "batch: 12065, loss: 0.1132204681634903\n",
      "batch: 12066, loss: 0.04662499204277992\n",
      "batch: 12067, loss: 0.20913666486740112\n",
      "batch: 12068, loss: 0.07251562178134918\n",
      "batch: 12069, loss: 0.1810327023267746\n",
      "batch: 12070, loss: 0.09589097648859024\n",
      "batch: 12071, loss: 0.08725495636463165\n",
      "batch: 12072, loss: 0.12933038175106049\n",
      "batch: 12073, loss: 0.038789473474025726\n",
      "batch: 12074, loss: 0.13471415638923645\n",
      "batch: 12075, loss: 0.10373252630233765\n",
      "batch: 12076, loss: 0.06241253390908241\n",
      "batch: 12077, loss: 0.10756412148475647\n",
      "batch: 12078, loss: 0.12446876615285873\n",
      "batch: 12079, loss: 0.09878553450107574\n",
      "batch: 12080, loss: 0.04961608350276947\n",
      "batch: 12081, loss: 0.05152992904186249\n",
      "batch: 12082, loss: 0.07776433974504471\n",
      "batch: 12083, loss: 0.1832476407289505\n",
      "batch: 12084, loss: 0.0626908391714096\n",
      "batch: 12085, loss: 0.07675882428884506\n",
      "batch: 12086, loss: 0.055939629673957825\n",
      "batch: 12087, loss: 0.08628551661968231\n",
      "batch: 12088, loss: 0.10809016227722168\n",
      "batch: 12089, loss: 0.19756561517715454\n",
      "batch: 12090, loss: 0.1832667738199234\n",
      "batch: 12091, loss: 0.20474231243133545\n",
      "batch: 12092, loss: 0.05271124839782715\n",
      "batch: 12093, loss: 0.04780410975217819\n",
      "batch: 12094, loss: 0.0726480484008789\n",
      "batch: 12095, loss: 0.1809949427843094\n",
      "batch: 12096, loss: 0.19739282131195068\n",
      "batch: 12097, loss: 0.05937007814645767\n",
      "batch: 12098, loss: 0.04801478236913681\n",
      "batch: 12099, loss: 0.10554572939872742\n",
      "batch: 12100, loss: 0.10258420556783676\n",
      "model saved to ./saving/model.ckpt-121\n",
      "batch: 12101, loss: 0.13566350936889648\n",
      "batch: 12102, loss: 0.1428280621767044\n",
      "batch: 12103, loss: 0.16890683770179749\n",
      "batch: 12104, loss: 0.08969846367835999\n",
      "batch: 12105, loss: 0.11927848309278488\n",
      "batch: 12106, loss: 0.09973107278347015\n",
      "batch: 12107, loss: 0.11703164875507355\n",
      "batch: 12108, loss: 0.0802915096282959\n",
      "batch: 12109, loss: 0.18741819262504578\n",
      "batch: 12110, loss: 0.21126501262187958\n",
      "batch: 12111, loss: 0.13589084148406982\n",
      "batch: 12112, loss: 0.16712936758995056\n",
      "batch: 12113, loss: 0.1625472605228424\n",
      "batch: 12114, loss: 0.12181153893470764\n",
      "batch: 12115, loss: 0.1247759610414505\n",
      "batch: 12116, loss: 0.07670018076896667\n",
      "batch: 12117, loss: 0.10996697098016739\n",
      "batch: 12118, loss: 0.3411877155303955\n",
      "batch: 12119, loss: 0.07440219819545746\n",
      "batch: 12120, loss: 0.2674747407436371\n",
      "batch: 12121, loss: 0.06539443880319595\n",
      "batch: 12122, loss: 0.3203473687171936\n",
      "batch: 12123, loss: 0.24428918957710266\n",
      "batch: 12124, loss: 0.10959982126951218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 12125, loss: 0.30433011054992676\n",
      "batch: 12126, loss: 0.1063680350780487\n",
      "batch: 12127, loss: 0.16894012689590454\n",
      "batch: 12128, loss: 0.10158340632915497\n",
      "batch: 12129, loss: 0.07910434901714325\n",
      "batch: 12130, loss: 0.0965212732553482\n",
      "batch: 12131, loss: 0.06782441586256027\n",
      "batch: 12132, loss: 0.05723132565617561\n",
      "batch: 12133, loss: 0.08265797048807144\n",
      "batch: 12134, loss: 0.08106494694948196\n",
      "batch: 12135, loss: 0.07578497380018234\n",
      "batch: 12136, loss: 0.06617489457130432\n",
      "batch: 12137, loss: 0.2576872706413269\n",
      "batch: 12138, loss: 0.31327497959136963\n",
      "batch: 12139, loss: 0.06396554410457611\n",
      "batch: 12140, loss: 0.09538081288337708\n",
      "batch: 12141, loss: 0.1539400815963745\n",
      "batch: 12142, loss: 0.1144397184252739\n",
      "batch: 12143, loss: 0.11108693480491638\n",
      "batch: 12144, loss: 0.18078003823757172\n",
      "batch: 12145, loss: 0.03208144009113312\n",
      "batch: 12146, loss: 0.05790393799543381\n",
      "batch: 12147, loss: 0.08505919575691223\n",
      "batch: 12148, loss: 0.13827942311763763\n",
      "batch: 12149, loss: 0.055828824639320374\n",
      "batch: 12150, loss: 0.08232183754444122\n",
      "batch: 12151, loss: 0.21053245663642883\n",
      "batch: 12152, loss: 0.31317973136901855\n",
      "batch: 12153, loss: 0.10536029934883118\n",
      "batch: 12154, loss: 0.19400829076766968\n",
      "batch: 12155, loss: 0.039170533418655396\n",
      "batch: 12156, loss: 0.12024250626564026\n",
      "batch: 12157, loss: 0.1495063602924347\n",
      "batch: 12158, loss: 0.14136344194412231\n",
      "batch: 12159, loss: 0.10362116992473602\n",
      "batch: 12160, loss: 0.11023148894309998\n",
      "batch: 12161, loss: 0.06221959367394447\n",
      "batch: 12162, loss: 0.07214653491973877\n",
      "batch: 12163, loss: 0.1974356323480606\n",
      "batch: 12164, loss: 0.044342391192913055\n",
      "batch: 12165, loss: 0.17238476872444153\n",
      "batch: 12166, loss: 0.1625775694847107\n",
      "batch: 12167, loss: 0.07025761902332306\n",
      "batch: 12168, loss: 0.06282968819141388\n",
      "batch: 12169, loss: 0.11887175589799881\n",
      "batch: 12170, loss: 0.14393949508666992\n",
      "batch: 12171, loss: 0.06361281126737595\n",
      "batch: 12172, loss: 0.18440014123916626\n",
      "batch: 12173, loss: 0.21694473922252655\n",
      "batch: 12174, loss: 0.07334430515766144\n",
      "batch: 12175, loss: 0.11360977590084076\n",
      "batch: 12176, loss: 0.165381520986557\n",
      "batch: 12177, loss: 0.16217827796936035\n",
      "batch: 12178, loss: 0.1810091733932495\n",
      "batch: 12179, loss: 0.12835994362831116\n",
      "batch: 12180, loss: 0.22459350526332855\n",
      "batch: 12181, loss: 0.07136572152376175\n",
      "batch: 12182, loss: 0.06615658104419708\n",
      "batch: 12183, loss: 0.17033520340919495\n",
      "batch: 12184, loss: 0.17802098393440247\n",
      "batch: 12185, loss: 0.22613415122032166\n",
      "batch: 12186, loss: 0.13744200766086578\n",
      "batch: 12187, loss: 0.2516535222530365\n",
      "batch: 12188, loss: 0.1519358605146408\n",
      "batch: 12189, loss: 0.04775140434503555\n",
      "batch: 12190, loss: 0.3265995681285858\n",
      "batch: 12191, loss: 0.05434869974851608\n",
      "batch: 12192, loss: 0.12993301451206207\n",
      "batch: 12193, loss: 0.22474977374076843\n",
      "batch: 12194, loss: 0.09224909543991089\n",
      "batch: 12195, loss: 0.30973929166793823\n",
      "batch: 12196, loss: 0.32114338874816895\n",
      "batch: 12197, loss: 0.06418890506029129\n",
      "batch: 12198, loss: 0.1737208366394043\n",
      "batch: 12199, loss: 0.1664849817752838\n",
      "batch: 12200, loss: 0.11886093765497208\n",
      "model saved to ./saving/model.ckpt-122\n",
      "batch: 12201, loss: 0.13361889123916626\n",
      "batch: 12202, loss: 0.127893328666687\n",
      "batch: 12203, loss: 0.22590096294879913\n",
      "batch: 12204, loss: 0.0733194649219513\n",
      "batch: 12205, loss: 0.07787445187568665\n",
      "batch: 12206, loss: 0.13033032417297363\n",
      "batch: 12207, loss: 0.10397502779960632\n",
      "batch: 12208, loss: 0.08534935116767883\n",
      "batch: 12209, loss: 0.1484687328338623\n",
      "batch: 12210, loss: 0.16194243729114532\n",
      "batch: 12211, loss: 0.0493001863360405\n",
      "batch: 12212, loss: 0.062453366816043854\n",
      "batch: 12213, loss: 0.1540367603302002\n",
      "batch: 12214, loss: 0.14658044278621674\n",
      "batch: 12215, loss: 0.1897381991147995\n",
      "batch: 12216, loss: 0.048649776726961136\n",
      "batch: 12217, loss: 0.11952197551727295\n",
      "batch: 12218, loss: 0.1055804118514061\n",
      "batch: 12219, loss: 0.109340600669384\n",
      "batch: 12220, loss: 0.10544335842132568\n",
      "batch: 12221, loss: 0.15116414427757263\n",
      "batch: 12222, loss: 0.15807273983955383\n",
      "batch: 12223, loss: 0.12793101370334625\n",
      "batch: 12224, loss: 0.29420924186706543\n",
      "batch: 12225, loss: 0.14365187287330627\n",
      "batch: 12226, loss: 0.06979500502347946\n",
      "batch: 12227, loss: 0.0341043546795845\n",
      "batch: 12228, loss: 0.1050410121679306\n",
      "batch: 12229, loss: 0.13552331924438477\n",
      "batch: 12230, loss: 0.21198400855064392\n",
      "batch: 12231, loss: 0.1021498367190361\n",
      "batch: 12232, loss: 0.1837833970785141\n",
      "batch: 12233, loss: 0.1756124347448349\n",
      "batch: 12234, loss: 0.08796253800392151\n",
      "batch: 12235, loss: 0.0537908710539341\n",
      "batch: 12236, loss: 0.10632124543190002\n",
      "batch: 12237, loss: 0.11709121614694595\n",
      "batch: 12238, loss: 0.0730244368314743\n",
      "batch: 12239, loss: 0.13776180148124695\n",
      "batch: 12240, loss: 0.10547007620334625\n",
      "batch: 12241, loss: 0.06738382577896118\n",
      "batch: 12242, loss: 0.0987943708896637\n",
      "batch: 12243, loss: 0.12105810642242432\n",
      "batch: 12244, loss: 0.21197977662086487\n",
      "batch: 12245, loss: 0.08118671178817749\n",
      "batch: 12246, loss: 0.10889972746372223\n",
      "batch: 12247, loss: 0.09348149597644806\n",
      "batch: 12248, loss: 0.3327046036720276\n",
      "batch: 12249, loss: 0.10190179198980331\n",
      "batch: 12250, loss: 0.0926920622587204\n",
      "batch: 12251, loss: 0.1602366417646408\n",
      "batch: 12252, loss: 0.04981308430433273\n",
      "batch: 12253, loss: 0.2302531599998474\n",
      "batch: 12254, loss: 0.10995576530694962\n",
      "batch: 12255, loss: 0.1779399961233139\n",
      "batch: 12256, loss: 0.1966453194618225\n",
      "batch: 12257, loss: 0.05011053383350372\n",
      "batch: 12258, loss: 0.1413990557193756\n",
      "batch: 12259, loss: 0.28754138946533203\n",
      "batch: 12260, loss: 0.07746739685535431\n",
      "batch: 12261, loss: 0.10601522773504257\n",
      "batch: 12262, loss: 0.19750382006168365\n",
      "batch: 12263, loss: 0.16894736886024475\n",
      "batch: 12264, loss: 0.2737864851951599\n",
      "batch: 12265, loss: 0.11228303611278534\n",
      "batch: 12266, loss: 0.06094524264335632\n",
      "batch: 12267, loss: 0.11258704960346222\n",
      "batch: 12268, loss: 0.26405102014541626\n",
      "batch: 12269, loss: 0.17305150628089905\n",
      "batch: 12270, loss: 0.09949483722448349\n",
      "batch: 12271, loss: 0.1803673803806305\n",
      "batch: 12272, loss: 0.09628169238567352\n",
      "batch: 12273, loss: 0.08005818724632263\n",
      "batch: 12274, loss: 0.07266470044851303\n",
      "batch: 12275, loss: 0.12023378908634186\n",
      "batch: 12276, loss: 0.09803749620914459\n",
      "batch: 12277, loss: 0.0851801186800003\n",
      "batch: 12278, loss: 0.08192984014749527\n",
      "batch: 12279, loss: 0.07630272209644318\n",
      "batch: 12280, loss: 0.09400586783885956\n",
      "batch: 12281, loss: 0.13462640345096588\n",
      "batch: 12282, loss: 0.17178001999855042\n",
      "batch: 12283, loss: 0.1491045355796814\n",
      "batch: 12284, loss: 0.08532837778329849\n",
      "batch: 12285, loss: 0.23259030282497406\n",
      "batch: 12286, loss: 0.11893723160028458\n",
      "batch: 12287, loss: 0.12112624198198318\n",
      "batch: 12288, loss: 0.26572883129119873\n",
      "batch: 12289, loss: 0.0617651604115963\n",
      "batch: 12290, loss: 0.062445178627967834\n",
      "batch: 12291, loss: 0.09900979697704315\n",
      "batch: 12292, loss: 0.09316279739141464\n",
      "batch: 12293, loss: 0.08218754082918167\n",
      "batch: 12294, loss: 0.09046769142150879\n",
      "batch: 12295, loss: 0.12883852422237396\n",
      "batch: 12296, loss: 0.06371913105249405\n",
      "batch: 12297, loss: 0.2671358585357666\n",
      "batch: 12298, loss: 0.24994195997714996\n",
      "batch: 12299, loss: 0.10330643504858017\n",
      "batch: 12300, loss: 0.25446972250938416\n",
      "model saved to ./saving/model.ckpt-123\n",
      "batch: 12301, loss: 0.1050875186920166\n",
      "batch: 12302, loss: 0.11491521447896957\n",
      "batch: 12303, loss: 0.129306361079216\n",
      "batch: 12304, loss: 0.2310384213924408\n",
      "batch: 12305, loss: 0.0834052711725235\n",
      "batch: 12306, loss: 0.07696060091257095\n",
      "batch: 12307, loss: 0.14564035832881927\n",
      "batch: 12308, loss: 0.21801118552684784\n",
      "batch: 12309, loss: 0.16793836653232574\n",
      "batch: 12310, loss: 0.11002214252948761\n",
      "batch: 12311, loss: 0.16196998953819275\n",
      "batch: 12312, loss: 0.10947276651859283\n",
      "batch: 12313, loss: 0.12257258594036102\n",
      "batch: 12314, loss: 0.13147708773612976\n",
      "batch: 12315, loss: 0.14251606166362762\n",
      "batch: 12316, loss: 0.09392502903938293\n",
      "batch: 12317, loss: 0.1204184889793396\n",
      "batch: 12318, loss: 0.1921698898077011\n",
      "batch: 12319, loss: 0.06332726776599884\n",
      "batch: 12320, loss: 0.08140726387500763\n",
      "batch: 12321, loss: 0.1816423237323761\n",
      "batch: 12322, loss: 0.08177110552787781\n",
      "batch: 12323, loss: 0.06792948395013809\n",
      "batch: 12324, loss: 0.36055439710617065\n",
      "batch: 12325, loss: 0.10826276987791061\n",
      "batch: 12326, loss: 0.07003596425056458\n",
      "batch: 12327, loss: 0.13858872652053833\n",
      "batch: 12328, loss: 0.1290137618780136\n",
      "batch: 12329, loss: 0.08685154467821121\n",
      "batch: 12330, loss: 0.03983253985643387\n",
      "batch: 12331, loss: 0.06442760676145554\n",
      "batch: 12332, loss: 0.1189049780368805\n",
      "batch: 12333, loss: 0.06555551290512085\n",
      "batch: 12334, loss: 0.22340601682662964\n",
      "batch: 12335, loss: 0.24085809290409088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 12336, loss: 0.10878053307533264\n",
      "batch: 12337, loss: 0.0805731788277626\n",
      "batch: 12338, loss: 0.04978015273809433\n",
      "batch: 12339, loss: 0.20776858925819397\n",
      "batch: 12340, loss: 0.14019739627838135\n",
      "batch: 12341, loss: 0.11030933260917664\n",
      "batch: 12342, loss: 0.082613006234169\n",
      "batch: 12343, loss: 0.11685944348573685\n",
      "batch: 12344, loss: 0.16228026151657104\n",
      "batch: 12345, loss: 0.05333908647298813\n",
      "batch: 12346, loss: 0.0934428721666336\n",
      "batch: 12347, loss: 0.07482077926397324\n",
      "batch: 12348, loss: 0.06858804821968079\n",
      "batch: 12349, loss: 0.09790816158056259\n",
      "batch: 12350, loss: 0.1189957782626152\n",
      "batch: 12351, loss: 0.14762338995933533\n",
      "batch: 12352, loss: 0.2267383337020874\n",
      "batch: 12353, loss: 0.13558806478977203\n",
      "batch: 12354, loss: 0.10869161039590836\n",
      "batch: 12355, loss: 0.14946219325065613\n",
      "batch: 12356, loss: 0.24369926750659943\n",
      "batch: 12357, loss: 0.18364018201828003\n",
      "batch: 12358, loss: 0.12240095436573029\n",
      "batch: 12359, loss: 0.06760667264461517\n",
      "batch: 12360, loss: 0.11080624163150787\n",
      "batch: 12361, loss: 0.09798414260149002\n",
      "batch: 12362, loss: 0.09655439853668213\n",
      "batch: 12363, loss: 0.09249578416347504\n",
      "batch: 12364, loss: 0.24978479743003845\n",
      "batch: 12365, loss: 0.2668997347354889\n",
      "batch: 12366, loss: 0.11289510130882263\n",
      "batch: 12367, loss: 0.16987383365631104\n",
      "batch: 12368, loss: 0.2343059629201889\n",
      "batch: 12369, loss: 0.07456690073013306\n",
      "batch: 12370, loss: 0.10798253864049911\n",
      "batch: 12371, loss: 0.1024567261338234\n",
      "batch: 12372, loss: 0.07490874081850052\n",
      "batch: 12373, loss: 0.1268899291753769\n",
      "batch: 12374, loss: 0.18353043496608734\n",
      "batch: 12375, loss: 0.06808815896511078\n",
      "batch: 12376, loss: 0.08910174667835236\n",
      "batch: 12377, loss: 0.10956619679927826\n",
      "batch: 12378, loss: 0.20066431164741516\n",
      "batch: 12379, loss: 0.05350325256586075\n",
      "batch: 12380, loss: 0.06195396929979324\n",
      "batch: 12381, loss: 0.09607762098312378\n",
      "batch: 12382, loss: 0.031624212861061096\n",
      "batch: 12383, loss: 0.07265979796648026\n",
      "batch: 12384, loss: 0.133475661277771\n",
      "batch: 12385, loss: 0.07562366127967834\n",
      "batch: 12386, loss: 0.08632948249578476\n",
      "batch: 12387, loss: 0.11016356199979782\n",
      "batch: 12388, loss: 0.146927148103714\n",
      "batch: 12389, loss: 0.15479730069637299\n",
      "batch: 12390, loss: 0.15264931321144104\n",
      "batch: 12391, loss: 0.052083227783441544\n",
      "batch: 12392, loss: 0.06369969248771667\n",
      "batch: 12393, loss: 0.1291654407978058\n",
      "batch: 12394, loss: 0.07237901538610458\n",
      "batch: 12395, loss: 0.16155552864074707\n",
      "batch: 12396, loss: 0.1569463610649109\n",
      "batch: 12397, loss: 0.03462263569235802\n",
      "batch: 12398, loss: 0.12920622527599335\n",
      "batch: 12399, loss: 0.06852757185697556\n",
      "batch: 12400, loss: 0.2251204550266266\n",
      "model saved to ./saving/model.ckpt-124\n",
      "batch: 12401, loss: 0.11726634204387665\n",
      "batch: 12402, loss: 0.11651867628097534\n",
      "batch: 12403, loss: 0.2874321937561035\n",
      "batch: 12404, loss: 0.14334098994731903\n",
      "batch: 12405, loss: 0.13742798566818237\n",
      "batch: 12406, loss: 0.08993940055370331\n",
      "batch: 12407, loss: 0.0469435416162014\n",
      "batch: 12408, loss: 0.10378149151802063\n",
      "batch: 12409, loss: 0.09678283333778381\n",
      "batch: 12410, loss: 0.21441929042339325\n",
      "batch: 12411, loss: 0.03937063738703728\n",
      "batch: 12412, loss: 0.15831318497657776\n",
      "batch: 12413, loss: 0.059489741921424866\n",
      "batch: 12414, loss: 0.07925595343112946\n",
      "batch: 12415, loss: 0.14328089356422424\n",
      "batch: 12416, loss: 0.1329898089170456\n",
      "batch: 12417, loss: 0.106693796813488\n",
      "batch: 12418, loss: 0.07510510087013245\n",
      "batch: 12419, loss: 0.08768625557422638\n",
      "batch: 12420, loss: 0.17059938609600067\n",
      "batch: 12421, loss: 0.22428679466247559\n",
      "batch: 12422, loss: 0.05455382168292999\n",
      "batch: 12423, loss: 0.0631345584988594\n",
      "batch: 12424, loss: 0.17136415839195251\n",
      "batch: 12425, loss: 0.18982501327991486\n",
      "batch: 12426, loss: 0.13430637121200562\n",
      "batch: 12427, loss: 0.11100144684314728\n",
      "batch: 12428, loss: 0.0862683355808258\n",
      "batch: 12429, loss: 0.08305531740188599\n",
      "batch: 12430, loss: 0.0671987310051918\n",
      "batch: 12431, loss: 0.06044682115316391\n",
      "batch: 12432, loss: 0.08666635304689407\n",
      "batch: 12433, loss: 0.0819588303565979\n",
      "batch: 12434, loss: 0.10147922486066818\n",
      "batch: 12435, loss: 0.18344387412071228\n",
      "batch: 12436, loss: 0.05780068784952164\n",
      "batch: 12437, loss: 0.1344430148601532\n",
      "batch: 12438, loss: 0.07273383438587189\n",
      "batch: 12439, loss: 0.19865809381008148\n",
      "batch: 12440, loss: 0.18503879010677338\n",
      "batch: 12441, loss: 0.22474107146263123\n",
      "batch: 12442, loss: 0.10747801512479782\n",
      "batch: 12443, loss: 0.10013166069984436\n",
      "batch: 12444, loss: 0.31567251682281494\n",
      "batch: 12445, loss: 0.09138818085193634\n",
      "batch: 12446, loss: 0.11065644770860672\n",
      "batch: 12447, loss: 0.07327067852020264\n",
      "batch: 12448, loss: 0.05895068123936653\n",
      "batch: 12449, loss: 0.1643226146697998\n",
      "batch: 12450, loss: 0.1018126904964447\n",
      "batch: 12451, loss: 0.10202297568321228\n",
      "batch: 12452, loss: 0.19529962539672852\n",
      "batch: 12453, loss: 0.14919377863407135\n",
      "batch: 12454, loss: 0.04006367176771164\n",
      "batch: 12455, loss: 0.1261696219444275\n",
      "batch: 12456, loss: 0.12011115252971649\n",
      "batch: 12457, loss: 0.07020677626132965\n",
      "batch: 12458, loss: 0.08651140332221985\n",
      "batch: 12459, loss: 0.17783182859420776\n",
      "batch: 12460, loss: 0.15962742269039154\n",
      "batch: 12461, loss: 0.1555703580379486\n",
      "batch: 12462, loss: 0.035479605197906494\n",
      "batch: 12463, loss: 0.09802791476249695\n",
      "batch: 12464, loss: 0.1450815498828888\n",
      "batch: 12465, loss: 0.09803661704063416\n",
      "batch: 12466, loss: 0.2491590976715088\n",
      "batch: 12467, loss: 0.08247135579586029\n",
      "batch: 12468, loss: 0.11053686589002609\n",
      "batch: 12469, loss: 0.11827492713928223\n",
      "batch: 12470, loss: 0.09205300360918045\n",
      "batch: 12471, loss: 0.15060429275035858\n",
      "batch: 12472, loss: 0.11355212330818176\n",
      "batch: 12473, loss: 0.060525715351104736\n",
      "batch: 12474, loss: 0.11642254889011383\n",
      "batch: 12475, loss: 0.09253618121147156\n",
      "batch: 12476, loss: 0.21895955502986908\n",
      "batch: 12477, loss: 0.08473926782608032\n",
      "batch: 12478, loss: 0.13505685329437256\n",
      "batch: 12479, loss: 0.035557813942432404\n",
      "batch: 12480, loss: 0.06859087944030762\n",
      "batch: 12481, loss: 0.10060085356235504\n",
      "batch: 12482, loss: 0.11661993712186813\n",
      "batch: 12483, loss: 0.03554058074951172\n",
      "batch: 12484, loss: 0.07313469052314758\n",
      "batch: 12485, loss: 0.16949224472045898\n",
      "batch: 12486, loss: 0.20088648796081543\n",
      "batch: 12487, loss: 0.0396992564201355\n",
      "batch: 12488, loss: 0.15659311413764954\n",
      "batch: 12489, loss: 0.23119035363197327\n",
      "batch: 12490, loss: 0.11432594805955887\n",
      "batch: 12491, loss: 0.09310969710350037\n",
      "batch: 12492, loss: 0.14816072583198547\n",
      "batch: 12493, loss: 0.12109796702861786\n",
      "batch: 12494, loss: 0.11007095873355865\n",
      "batch: 12495, loss: 0.10701683163642883\n",
      "batch: 12496, loss: 0.13416409492492676\n",
      "batch: 12497, loss: 0.20473943650722504\n",
      "batch: 12498, loss: 0.08671560883522034\n",
      "batch: 12499, loss: 0.11240129172801971\n",
      "batch: 12500, loss: 0.11684545129537582\n",
      "model saved to ./saving/model.ckpt-125\n",
      "batch: 12501, loss: 0.13684532046318054\n",
      "batch: 12502, loss: 0.051638420671224594\n",
      "batch: 12503, loss: 0.06305357068777084\n",
      "batch: 12504, loss: 0.043758146464824677\n",
      "batch: 12505, loss: 0.3668585419654846\n",
      "batch: 12506, loss: 0.07053209096193314\n",
      "batch: 12507, loss: 0.17661836743354797\n",
      "batch: 12508, loss: 0.1900276392698288\n",
      "batch: 12509, loss: 0.10900477319955826\n",
      "batch: 12510, loss: 0.08984542638063431\n",
      "batch: 12511, loss: 0.2571153938770294\n",
      "batch: 12512, loss: 0.18944473564624786\n",
      "batch: 12513, loss: 0.07389110326766968\n",
      "batch: 12514, loss: 0.12969720363616943\n",
      "batch: 12515, loss: 0.04973168671131134\n",
      "batch: 12516, loss: 0.13019418716430664\n",
      "batch: 12517, loss: 0.11539875715970993\n",
      "batch: 12518, loss: 0.10508802533149719\n",
      "batch: 12519, loss: 0.11413637548685074\n",
      "batch: 12520, loss: 0.2532162666320801\n",
      "batch: 12521, loss: 0.09390008449554443\n",
      "batch: 12522, loss: 0.06210866570472717\n",
      "batch: 12523, loss: 0.15840737521648407\n",
      "batch: 12524, loss: 0.07605911046266556\n",
      "batch: 12525, loss: 0.05860394611954689\n",
      "batch: 12526, loss: 0.10160353034734726\n",
      "batch: 12527, loss: 0.04832061380147934\n",
      "batch: 12528, loss: 0.09060170501470566\n",
      "batch: 12529, loss: 0.12828513979911804\n",
      "batch: 12530, loss: 0.1202978640794754\n",
      "batch: 12531, loss: 0.18663723766803741\n",
      "batch: 12532, loss: 0.15653479099273682\n",
      "batch: 12533, loss: 0.34062761068344116\n",
      "batch: 12534, loss: 0.06728588789701462\n",
      "batch: 12535, loss: 0.1431845724582672\n",
      "batch: 12536, loss: 0.07141102850437164\n",
      "batch: 12537, loss: 0.10700790584087372\n",
      "batch: 12538, loss: 0.09112417697906494\n",
      "batch: 12539, loss: 0.11666126549243927\n",
      "batch: 12540, loss: 0.061494745314121246\n",
      "batch: 12541, loss: 0.09178377687931061\n",
      "batch: 12542, loss: 0.07393093407154083\n",
      "batch: 12543, loss: 0.044183120131492615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 12544, loss: 0.13463664054870605\n",
      "batch: 12545, loss: 0.08224035054445267\n",
      "batch: 12546, loss: 0.12611865997314453\n",
      "batch: 12547, loss: 0.19185318052768707\n",
      "batch: 12548, loss: 0.16448956727981567\n",
      "batch: 12549, loss: 0.426807701587677\n",
      "batch: 12550, loss: 0.14913183450698853\n",
      "batch: 12551, loss: 0.12875911593437195\n",
      "batch: 12552, loss: 0.08244918286800385\n",
      "batch: 12553, loss: 0.06148674711585045\n",
      "batch: 12554, loss: 0.15373672544956207\n",
      "batch: 12555, loss: 0.11381509155035019\n",
      "batch: 12556, loss: 0.16705238819122314\n",
      "batch: 12557, loss: 0.06507141143083572\n",
      "batch: 12558, loss: 0.15374767780303955\n",
      "batch: 12559, loss: 0.19533546268939972\n",
      "batch: 12560, loss: 0.21973460912704468\n",
      "batch: 12561, loss: 0.23461055755615234\n",
      "batch: 12562, loss: 0.079105906188488\n",
      "batch: 12563, loss: 0.09357398003339767\n",
      "batch: 12564, loss: 0.05410374701023102\n",
      "batch: 12565, loss: 0.13752466440200806\n",
      "batch: 12566, loss: 0.1806521713733673\n",
      "batch: 12567, loss: 0.16376176476478577\n",
      "batch: 12568, loss: 0.12909002602100372\n",
      "batch: 12569, loss: 0.07224591076374054\n",
      "batch: 12570, loss: 0.276123046875\n",
      "batch: 12571, loss: 0.08993475139141083\n",
      "batch: 12572, loss: 0.08427014946937561\n",
      "batch: 12573, loss: 0.19347774982452393\n",
      "batch: 12574, loss: 0.10081875324249268\n",
      "batch: 12575, loss: 0.11442539095878601\n",
      "batch: 12576, loss: 0.1498449146747589\n",
      "batch: 12577, loss: 0.10903678834438324\n",
      "batch: 12578, loss: 0.14969122409820557\n",
      "batch: 12579, loss: 0.1257028579711914\n",
      "batch: 12580, loss: 0.11718352138996124\n",
      "batch: 12581, loss: 0.10045141726732254\n",
      "batch: 12582, loss: 0.17375138401985168\n",
      "batch: 12583, loss: 0.06594988703727722\n",
      "batch: 12584, loss: 0.08845827728509903\n",
      "batch: 12585, loss: 0.1755431592464447\n",
      "batch: 12586, loss: 0.06632012128829956\n",
      "batch: 12587, loss: 0.046805109828710556\n",
      "batch: 12588, loss: 0.332150936126709\n",
      "batch: 12589, loss: 0.08212850987911224\n",
      "batch: 12590, loss: 0.11379886418581009\n",
      "batch: 12591, loss: 0.33084824681282043\n",
      "batch: 12592, loss: 0.1398935317993164\n",
      "batch: 12593, loss: 0.13108833134174347\n",
      "batch: 12594, loss: 0.13904720544815063\n",
      "batch: 12595, loss: 0.15723606944084167\n",
      "batch: 12596, loss: 0.12120336294174194\n",
      "batch: 12597, loss: 0.08067053556442261\n",
      "batch: 12598, loss: 0.08537019044160843\n",
      "batch: 12599, loss: 0.12466617673635483\n",
      "batch: 12600, loss: 0.23371616005897522\n",
      "model saved to ./saving/model.ckpt-126\n",
      "batch: 12601, loss: 0.10791092365980148\n",
      "batch: 12602, loss: 0.26269346475601196\n",
      "batch: 12603, loss: 0.11618335545063019\n",
      "batch: 12604, loss: 0.32509225606918335\n",
      "batch: 12605, loss: 0.1679982990026474\n",
      "batch: 12606, loss: 0.06013473495841026\n",
      "batch: 12607, loss: 0.07687988877296448\n",
      "batch: 12608, loss: 0.10586908459663391\n",
      "batch: 12609, loss: 0.19294683635234833\n",
      "batch: 12610, loss: 0.17970913648605347\n",
      "batch: 12611, loss: 0.06477051228284836\n",
      "batch: 12612, loss: 0.0837189257144928\n",
      "batch: 12613, loss: 0.15474833548069\n",
      "batch: 12614, loss: 0.049044955521821976\n",
      "batch: 12615, loss: 0.10371389240026474\n",
      "batch: 12616, loss: 0.055187590420246124\n",
      "batch: 12617, loss: 0.10545753687620163\n",
      "batch: 12618, loss: 0.053734131157398224\n",
      "batch: 12619, loss: 0.1026635468006134\n",
      "batch: 12620, loss: 0.12127555161714554\n",
      "batch: 12621, loss: 0.12903626263141632\n",
      "batch: 12622, loss: 0.04523257166147232\n",
      "batch: 12623, loss: 0.1563691943883896\n",
      "batch: 12624, loss: 0.1307394802570343\n",
      "batch: 12625, loss: 0.1315833032131195\n",
      "batch: 12626, loss: 0.08708421140909195\n",
      "batch: 12627, loss: 0.17759357392787933\n",
      "batch: 12628, loss: 0.14923018217086792\n",
      "batch: 12629, loss: 0.025960765779018402\n",
      "batch: 12630, loss: 0.12229426205158234\n",
      "batch: 12631, loss: 0.09274912625551224\n",
      "batch: 12632, loss: 0.08283968269824982\n",
      "batch: 12633, loss: 0.12121380865573883\n",
      "batch: 12634, loss: 0.18367402255535126\n",
      "batch: 12635, loss: 0.21453514695167542\n",
      "batch: 12636, loss: 0.09702318906784058\n",
      "batch: 12637, loss: 0.16178055107593536\n",
      "batch: 12638, loss: 0.1225607767701149\n",
      "batch: 12639, loss: 0.18117830157279968\n",
      "batch: 12640, loss: 0.06367961317300797\n",
      "batch: 12641, loss: 0.11318181455135345\n",
      "batch: 12642, loss: 0.07762041687965393\n",
      "batch: 12643, loss: 0.07473491877317429\n",
      "batch: 12644, loss: 0.12886664271354675\n",
      "batch: 12645, loss: 0.15075638890266418\n",
      "batch: 12646, loss: 0.12673772871494293\n",
      "batch: 12647, loss: 0.1217024177312851\n",
      "batch: 12648, loss: 0.131478950381279\n",
      "batch: 12649, loss: 0.13715782761573792\n",
      "batch: 12650, loss: 0.11903572827577591\n",
      "batch: 12651, loss: 0.08853714168071747\n",
      "batch: 12652, loss: 0.08115141838788986\n",
      "batch: 12653, loss: 0.1740863025188446\n",
      "batch: 12654, loss: 0.05252974480390549\n",
      "batch: 12655, loss: 0.04895583912730217\n",
      "batch: 12656, loss: 0.18616551160812378\n",
      "batch: 12657, loss: 0.08843287825584412\n",
      "batch: 12658, loss: 0.07662271708250046\n",
      "batch: 12659, loss: 0.15172742307186127\n",
      "batch: 12660, loss: 0.1309516578912735\n",
      "batch: 12661, loss: 0.14676180481910706\n",
      "batch: 12662, loss: 0.1343410611152649\n",
      "batch: 12663, loss: 0.07708883285522461\n",
      "batch: 12664, loss: 0.10254210233688354\n",
      "batch: 12665, loss: 0.11694064736366272\n",
      "batch: 12666, loss: 0.17848852276802063\n",
      "batch: 12667, loss: 0.11630468815565109\n",
      "batch: 12668, loss: 0.06115684285759926\n",
      "batch: 12669, loss: 0.1729465126991272\n",
      "batch: 12670, loss: 0.050630513578653336\n",
      "batch: 12671, loss: 0.08224302530288696\n",
      "batch: 12672, loss: 0.14656755328178406\n",
      "batch: 12673, loss: 0.24655508995056152\n",
      "batch: 12674, loss: 0.11389382928609848\n",
      "batch: 12675, loss: 0.2548620104789734\n",
      "batch: 12676, loss: 0.12006472051143646\n",
      "batch: 12677, loss: 0.04501783102750778\n",
      "batch: 12678, loss: 0.07668602466583252\n",
      "batch: 12679, loss: 0.06493310630321503\n",
      "batch: 12680, loss: 0.0862288773059845\n",
      "batch: 12681, loss: 0.14759455621242523\n",
      "batch: 12682, loss: 0.2754073739051819\n",
      "batch: 12683, loss: 0.16623198986053467\n",
      "batch: 12684, loss: 0.1035676896572113\n",
      "batch: 12685, loss: 0.05291745811700821\n",
      "batch: 12686, loss: 0.09905217587947845\n",
      "batch: 12687, loss: 0.045199982821941376\n",
      "batch: 12688, loss: 0.08609142899513245\n",
      "batch: 12689, loss: 0.14043329656124115\n",
      "batch: 12690, loss: 0.047002941370010376\n",
      "batch: 12691, loss: 0.11775431036949158\n",
      "batch: 12692, loss: 0.06734548509120941\n",
      "batch: 12693, loss: 0.07474833726882935\n",
      "batch: 12694, loss: 0.07903022319078445\n",
      "batch: 12695, loss: 0.13121716678142548\n",
      "batch: 12696, loss: 0.07414871454238892\n",
      "batch: 12697, loss: 0.09985652565956116\n",
      "batch: 12698, loss: 0.06495346128940582\n",
      "batch: 12699, loss: 0.029631704092025757\n",
      "batch: 12700, loss: 0.1381913423538208\n",
      "model saved to ./saving/model.ckpt-127\n",
      "batch: 12701, loss: 0.07394511252641678\n",
      "batch: 12702, loss: 0.12453920394182205\n",
      "batch: 12703, loss: 0.1678285002708435\n",
      "batch: 12704, loss: 0.1462988555431366\n",
      "batch: 12705, loss: 0.19479045271873474\n",
      "batch: 12706, loss: 0.04433990642428398\n",
      "batch: 12707, loss: 0.12253956496715546\n",
      "batch: 12708, loss: 0.1911950260400772\n",
      "batch: 12709, loss: 0.08058393746614456\n",
      "batch: 12710, loss: 0.06211923062801361\n",
      "batch: 12711, loss: 0.2999056577682495\n",
      "batch: 12712, loss: 0.039764173328876495\n",
      "batch: 12713, loss: 0.08368705958127975\n",
      "batch: 12714, loss: 0.18684670329093933\n",
      "batch: 12715, loss: 0.09151922166347504\n",
      "batch: 12716, loss: 0.0683194175362587\n",
      "batch: 12717, loss: 0.1273057758808136\n",
      "batch: 12718, loss: 0.07665051519870758\n",
      "batch: 12719, loss: 0.07885035872459412\n",
      "batch: 12720, loss: 0.09824445843696594\n",
      "batch: 12721, loss: 0.07278852164745331\n",
      "batch: 12722, loss: 0.057744644582271576\n",
      "batch: 12723, loss: 0.14123812317848206\n",
      "batch: 12724, loss: 0.05707641318440437\n",
      "batch: 12725, loss: 0.08128868788480759\n",
      "batch: 12726, loss: 0.04802439734339714\n",
      "batch: 12727, loss: 0.24521349370479584\n",
      "batch: 12728, loss: 0.07770612090826035\n",
      "batch: 12729, loss: 0.0890611931681633\n",
      "batch: 12730, loss: 0.13069981336593628\n",
      "batch: 12731, loss: 0.1442497819662094\n",
      "batch: 12732, loss: 0.06999006867408752\n",
      "batch: 12733, loss: 0.04789510369300842\n",
      "batch: 12734, loss: 0.08983026444911957\n",
      "batch: 12735, loss: 0.16279926896095276\n",
      "batch: 12736, loss: 0.09302519261837006\n",
      "batch: 12737, loss: 0.12994620203971863\n",
      "batch: 12738, loss: 0.1046387329697609\n",
      "batch: 12739, loss: 0.16995030641555786\n",
      "batch: 12740, loss: 0.06713778525590897\n",
      "batch: 12741, loss: 0.13096418976783752\n",
      "batch: 12742, loss: 0.1247137263417244\n",
      "batch: 12743, loss: 0.14357951283454895\n",
      "batch: 12744, loss: 0.06544262170791626\n",
      "batch: 12745, loss: 0.10881787538528442\n",
      "batch: 12746, loss: 0.03679294139146805\n",
      "batch: 12747, loss: 0.045507319271564484\n",
      "batch: 12748, loss: 0.08660091459751129\n",
      "batch: 12749, loss: 0.12904547154903412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 12750, loss: 0.05641603842377663\n",
      "batch: 12751, loss: 0.146007239818573\n",
      "batch: 12752, loss: 0.033848557621240616\n",
      "batch: 12753, loss: 0.050207287073135376\n",
      "batch: 12754, loss: 0.04045106843113899\n",
      "batch: 12755, loss: 0.08480816334486008\n",
      "batch: 12756, loss: 0.06630104035139084\n",
      "batch: 12757, loss: 0.2409350723028183\n",
      "batch: 12758, loss: 0.119484081864357\n",
      "batch: 12759, loss: 0.1399504542350769\n",
      "batch: 12760, loss: 0.10328473150730133\n",
      "batch: 12761, loss: 0.06079446151852608\n",
      "batch: 12762, loss: 0.06205236166715622\n",
      "batch: 12763, loss: 0.10955601930618286\n",
      "batch: 12764, loss: 0.14383351802825928\n",
      "batch: 12765, loss: 0.13448582589626312\n",
      "batch: 12766, loss: 0.09857595711946487\n",
      "batch: 12767, loss: 0.16436102986335754\n",
      "batch: 12768, loss: 0.14393532276153564\n",
      "batch: 12769, loss: 0.1461784839630127\n",
      "batch: 12770, loss: 0.04917342960834503\n",
      "batch: 12771, loss: 0.09762690961360931\n",
      "batch: 12772, loss: 0.08300670236349106\n",
      "batch: 12773, loss: 0.13991142809391022\n",
      "batch: 12774, loss: 0.10645205527544022\n",
      "batch: 12775, loss: 0.06622527539730072\n",
      "batch: 12776, loss: 0.09963170439004898\n",
      "batch: 12777, loss: 0.1319178342819214\n",
      "batch: 12778, loss: 0.06191330403089523\n",
      "batch: 12779, loss: 0.053822409361600876\n",
      "batch: 12780, loss: 0.10819950699806213\n",
      "batch: 12781, loss: 0.11264073848724365\n",
      "batch: 12782, loss: 0.17013861238956451\n",
      "batch: 12783, loss: 0.1723376214504242\n",
      "batch: 12784, loss: 0.04974494129419327\n",
      "batch: 12785, loss: 0.15349885821342468\n",
      "batch: 12786, loss: 0.2196395844221115\n",
      "batch: 12787, loss: 0.12002614885568619\n",
      "batch: 12788, loss: 0.09937001764774323\n",
      "batch: 12789, loss: 0.0950704962015152\n",
      "batch: 12790, loss: 0.175636425614357\n",
      "batch: 12791, loss: 0.1029028668999672\n",
      "batch: 12792, loss: 0.07712600380182266\n",
      "batch: 12793, loss: 0.08269615471363068\n",
      "batch: 12794, loss: 0.0760720819234848\n",
      "batch: 12795, loss: 0.28638309240341187\n",
      "batch: 12796, loss: 0.16684649884700775\n",
      "batch: 12797, loss: 0.07149817794561386\n",
      "batch: 12798, loss: 0.04952181130647659\n",
      "batch: 12799, loss: 0.11628910899162292\n",
      "batch: 12800, loss: 0.0705961063504219\n",
      "model saved to ./saving/model.ckpt-128\n",
      "batch: 12801, loss: 0.055350929498672485\n",
      "batch: 12802, loss: 0.11664701998233795\n",
      "batch: 12803, loss: 0.08832529187202454\n",
      "batch: 12804, loss: 0.10226832330226898\n",
      "batch: 12805, loss: 0.06202643737196922\n",
      "batch: 12806, loss: 0.1482016146183014\n",
      "batch: 12807, loss: 0.24116207659244537\n",
      "batch: 12808, loss: 0.0979321300983429\n",
      "batch: 12809, loss: 0.10350298136472702\n",
      "batch: 12810, loss: 0.036554303020238876\n",
      "batch: 12811, loss: 0.14892949163913727\n",
      "batch: 12812, loss: 0.20712649822235107\n",
      "batch: 12813, loss: 0.06652602553367615\n",
      "batch: 12814, loss: 0.09077556431293488\n",
      "batch: 12815, loss: 0.250954806804657\n",
      "batch: 12816, loss: 0.3194189667701721\n",
      "batch: 12817, loss: 0.09925620257854462\n",
      "batch: 12818, loss: 0.17521317303180695\n",
      "batch: 12819, loss: 0.02890900894999504\n",
      "batch: 12820, loss: 0.10532483458518982\n",
      "batch: 12821, loss: 0.09667401015758514\n",
      "batch: 12822, loss: 0.15935981273651123\n",
      "batch: 12823, loss: 0.0728607028722763\n",
      "batch: 12824, loss: 0.18112501502037048\n",
      "batch: 12825, loss: 0.1867521107196808\n",
      "batch: 12826, loss: 0.03995708003640175\n",
      "batch: 12827, loss: 0.04533950239419937\n",
      "batch: 12828, loss: 0.0831061452627182\n",
      "batch: 12829, loss: 0.13570398092269897\n",
      "batch: 12830, loss: 0.17531193792819977\n",
      "batch: 12831, loss: 0.11256974190473557\n",
      "batch: 12832, loss: 0.19513767957687378\n",
      "batch: 12833, loss: 0.08104315400123596\n",
      "batch: 12834, loss: 0.06113661453127861\n",
      "batch: 12835, loss: 0.13371703028678894\n",
      "batch: 12836, loss: 0.06955669820308685\n",
      "batch: 12837, loss: 0.12374222278594971\n",
      "batch: 12838, loss: 0.27630510926246643\n",
      "batch: 12839, loss: 0.15952014923095703\n",
      "batch: 12840, loss: 0.18190711736679077\n",
      "batch: 12841, loss: 0.11213502287864685\n",
      "batch: 12842, loss: 0.11862946301698685\n",
      "batch: 12843, loss: 0.10656319558620453\n",
      "batch: 12844, loss: 0.04834827780723572\n",
      "batch: 12845, loss: 0.14928826689720154\n",
      "batch: 12846, loss: 0.05563677102327347\n",
      "batch: 12847, loss: 0.1326393336057663\n",
      "batch: 12848, loss: 0.1745019257068634\n",
      "batch: 12849, loss: 0.12605157494544983\n",
      "batch: 12850, loss: 0.08491221070289612\n",
      "batch: 12851, loss: 0.03959769755601883\n",
      "batch: 12852, loss: 0.12997420132160187\n",
      "batch: 12853, loss: 0.14953303337097168\n",
      "batch: 12854, loss: 0.0498664528131485\n",
      "batch: 12855, loss: 0.04484408348798752\n",
      "batch: 12856, loss: 0.10629171878099442\n",
      "batch: 12857, loss: 0.151791051030159\n",
      "batch: 12858, loss: 0.23571015894412994\n",
      "batch: 12859, loss: 0.08854597806930542\n",
      "batch: 12860, loss: 0.05519769340753555\n",
      "batch: 12861, loss: 0.10617008805274963\n",
      "batch: 12862, loss: 0.06623852252960205\n",
      "batch: 12863, loss: 0.1912902295589447\n",
      "batch: 12864, loss: 0.1763220578432083\n",
      "batch: 12865, loss: 0.09925902634859085\n",
      "batch: 12866, loss: 0.06853952258825302\n",
      "batch: 12867, loss: 0.07292831689119339\n",
      "batch: 12868, loss: 0.18914057314395905\n",
      "batch: 12869, loss: 0.20959515869617462\n",
      "batch: 12870, loss: 0.1898295283317566\n",
      "batch: 12871, loss: 0.05817587673664093\n",
      "batch: 12872, loss: 0.1510556936264038\n",
      "batch: 12873, loss: 0.05959518998861313\n",
      "batch: 12874, loss: 0.11869227886199951\n",
      "batch: 12875, loss: 0.10736034065485\n",
      "batch: 12876, loss: 0.06689313054084778\n",
      "batch: 12877, loss: 0.09938021004199982\n",
      "batch: 12878, loss: 0.06519613415002823\n",
      "batch: 12879, loss: 0.2456846386194229\n",
      "batch: 12880, loss: 0.044710829854011536\n",
      "batch: 12881, loss: 0.15038223564624786\n",
      "batch: 12882, loss: 0.03995503485202789\n",
      "batch: 12883, loss: 0.04951649159193039\n",
      "batch: 12884, loss: 0.10942938178777695\n",
      "batch: 12885, loss: 0.3147646486759186\n",
      "batch: 12886, loss: 0.0752454623579979\n",
      "batch: 12887, loss: 0.1045134961605072\n",
      "batch: 12888, loss: 0.19136929512023926\n",
      "batch: 12889, loss: 0.1063082218170166\n",
      "batch: 12890, loss: 0.13681507110595703\n",
      "batch: 12891, loss: 0.14934967458248138\n",
      "batch: 12892, loss: 0.15265190601348877\n",
      "batch: 12893, loss: 0.16095282137393951\n",
      "batch: 12894, loss: 0.06250223517417908\n",
      "batch: 12895, loss: 0.11531475186347961\n",
      "batch: 12896, loss: 0.12960797548294067\n",
      "batch: 12897, loss: 0.16787351667881012\n",
      "batch: 12898, loss: 0.09126372635364532\n",
      "batch: 12899, loss: 0.10577075183391571\n",
      "batch: 12900, loss: 0.06053600832819939\n",
      "model saved to ./saving/model.ckpt-129\n",
      "batch: 12901, loss: 0.2679190933704376\n",
      "batch: 12902, loss: 0.10321305692195892\n",
      "batch: 12903, loss: 0.065634585916996\n",
      "batch: 12904, loss: 0.10166620463132858\n",
      "batch: 12905, loss: 0.09875298291444778\n",
      "batch: 12906, loss: 0.17289775609970093\n",
      "batch: 12907, loss: 0.08335288614034653\n",
      "batch: 12908, loss: 0.023596784099936485\n",
      "batch: 12909, loss: 0.04580340534448624\n",
      "batch: 12910, loss: 0.09391096234321594\n",
      "batch: 12911, loss: 0.06263735890388489\n",
      "batch: 12912, loss: 0.12994854152202606\n",
      "batch: 12913, loss: 0.06747332215309143\n",
      "batch: 12914, loss: 0.10627026855945587\n",
      "batch: 12915, loss: 0.07093039900064468\n",
      "batch: 12916, loss: 0.1134108155965805\n",
      "batch: 12917, loss: 0.087591253221035\n",
      "batch: 12918, loss: 0.10163885354995728\n",
      "batch: 12919, loss: 0.04582998529076576\n",
      "batch: 12920, loss: 0.10056515038013458\n",
      "batch: 12921, loss: 0.0351402573287487\n",
      "batch: 12922, loss: 0.10337503999471664\n",
      "batch: 12923, loss: 0.10815487056970596\n",
      "batch: 12924, loss: 0.11493080854415894\n",
      "batch: 12925, loss: 0.13782204687595367\n",
      "batch: 12926, loss: 0.239423930644989\n",
      "batch: 12927, loss: 0.16172222793102264\n",
      "batch: 12928, loss: 0.10816198587417603\n",
      "batch: 12929, loss: 0.05862627923488617\n",
      "batch: 12930, loss: 0.060825370252132416\n",
      "batch: 12931, loss: 0.10927638411521912\n",
      "batch: 12932, loss: 0.03051593340933323\n",
      "batch: 12933, loss: 0.07632371038198471\n",
      "batch: 12934, loss: 0.1723681390285492\n",
      "batch: 12935, loss: 0.1337948739528656\n",
      "batch: 12936, loss: 0.12619376182556152\n",
      "batch: 12937, loss: 0.11138655245304108\n",
      "batch: 12938, loss: 0.17213928699493408\n",
      "batch: 12939, loss: 0.16834351420402527\n",
      "batch: 12940, loss: 0.09587308764457703\n",
      "batch: 12941, loss: 0.15881496667861938\n",
      "batch: 12942, loss: 0.11423800885677338\n",
      "batch: 12943, loss: 0.1206970065832138\n",
      "batch: 12944, loss: 0.04627629369497299\n",
      "batch: 12945, loss: 0.05597078800201416\n",
      "batch: 12946, loss: 0.10108092427253723\n",
      "batch: 12947, loss: 0.09091714024543762\n",
      "batch: 12948, loss: 0.10809106379747391\n",
      "batch: 12949, loss: 0.07743266969919205\n",
      "batch: 12950, loss: 0.1175684854388237\n",
      "batch: 12951, loss: 0.126864492893219\n",
      "batch: 12952, loss: 0.12314551323652267\n",
      "batch: 12953, loss: 0.06065058708190918\n",
      "batch: 12954, loss: 0.0910361036658287\n",
      "batch: 12955, loss: 0.1628415435552597\n",
      "batch: 12956, loss: 0.03864579647779465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 12957, loss: 0.1208062544465065\n",
      "batch: 12958, loss: 0.07748226076364517\n",
      "batch: 12959, loss: 0.2047646939754486\n",
      "batch: 12960, loss: 0.029529165476560593\n",
      "batch: 12961, loss: 0.06417503952980042\n",
      "batch: 12962, loss: 0.12616556882858276\n",
      "batch: 12963, loss: 0.06361214816570282\n",
      "batch: 12964, loss: 0.10159715265035629\n",
      "batch: 12965, loss: 0.1705167442560196\n",
      "batch: 12966, loss: 0.057524122297763824\n",
      "batch: 12967, loss: 0.08225896954536438\n",
      "batch: 12968, loss: 0.11460300534963608\n",
      "batch: 12969, loss: 0.022143231704831123\n",
      "batch: 12970, loss: 0.09199978411197662\n",
      "batch: 12971, loss: 0.20541207492351532\n",
      "batch: 12972, loss: 0.19606104493141174\n",
      "batch: 12973, loss: 0.12496325373649597\n",
      "batch: 12974, loss: 0.11843527853488922\n",
      "batch: 12975, loss: 0.27004411816596985\n",
      "batch: 12976, loss: 0.022156184539198875\n",
      "batch: 12977, loss: 0.13146542012691498\n",
      "batch: 12978, loss: 0.23862223327159882\n",
      "batch: 12979, loss: 0.1382034420967102\n",
      "batch: 12980, loss: 0.09369400143623352\n",
      "batch: 12981, loss: 0.11022907495498657\n",
      "batch: 12982, loss: 0.17640800774097443\n",
      "batch: 12983, loss: 0.11534515023231506\n",
      "batch: 12984, loss: 0.15481001138687134\n",
      "batch: 12985, loss: 0.054430220276117325\n",
      "batch: 12986, loss: 0.20605073869228363\n",
      "batch: 12987, loss: 0.07047154009342194\n",
      "batch: 12988, loss: 0.09122130274772644\n",
      "batch: 12989, loss: 0.20426417887210846\n",
      "batch: 12990, loss: 0.10127952694892883\n",
      "batch: 12991, loss: 0.1368573158979416\n",
      "batch: 12992, loss: 0.05695551261305809\n",
      "batch: 12993, loss: 0.2581186890602112\n",
      "batch: 12994, loss: 0.11316440999507904\n",
      "batch: 12995, loss: 0.1539618968963623\n",
      "batch: 12996, loss: 0.18398445844650269\n",
      "batch: 12997, loss: 0.11638282984495163\n",
      "batch: 12998, loss: 0.10427479445934296\n",
      "batch: 12999, loss: 0.27275246381759644\n",
      "batch: 13000, loss: 0.2140614092350006\n",
      "model saved to ./saving/model.ckpt-130\n",
      "batch: 13001, loss: 0.06168339028954506\n",
      "batch: 13002, loss: 0.07021008431911469\n",
      "batch: 13003, loss: 0.06278863549232483\n",
      "batch: 13004, loss: 0.03984181210398674\n",
      "batch: 13005, loss: 0.08734754472970963\n",
      "batch: 13006, loss: 0.0702822357416153\n",
      "batch: 13007, loss: 0.09184552729129791\n",
      "batch: 13008, loss: 0.11438634991645813\n",
      "batch: 13009, loss: 0.03565645217895508\n",
      "batch: 13010, loss: 0.03746914863586426\n",
      "batch: 13011, loss: 0.11111568659543991\n",
      "batch: 13012, loss: 0.18593338131904602\n",
      "batch: 13013, loss: 0.07656674087047577\n",
      "batch: 13014, loss: 0.20649762451648712\n",
      "batch: 13015, loss: 0.1010487750172615\n",
      "batch: 13016, loss: 0.07251100242137909\n",
      "batch: 13017, loss: 0.07582660019397736\n",
      "batch: 13018, loss: 0.03952765464782715\n",
      "batch: 13019, loss: 0.04911104589700699\n",
      "batch: 13020, loss: 0.08602795749902725\n",
      "batch: 13021, loss: 0.04889719560742378\n",
      "batch: 13022, loss: 0.04955039545893669\n",
      "batch: 13023, loss: 0.07743032276630402\n",
      "batch: 13024, loss: 0.16200822591781616\n",
      "batch: 13025, loss: 0.13704386353492737\n",
      "batch: 13026, loss: 0.04437100887298584\n",
      "batch: 13027, loss: 0.13928933441638947\n",
      "batch: 13028, loss: 0.06760509312152863\n",
      "batch: 13029, loss: 0.08904840052127838\n",
      "batch: 13030, loss: 0.06993492692708969\n",
      "batch: 13031, loss: 0.094589963555336\n",
      "batch: 13032, loss: 0.1773190051317215\n",
      "batch: 13033, loss: 0.06493116915225983\n",
      "batch: 13034, loss: 0.10407111048698425\n",
      "batch: 13035, loss: 0.13880902528762817\n",
      "batch: 13036, loss: 0.12153936177492142\n",
      "batch: 13037, loss: 0.08567807078361511\n",
      "batch: 13038, loss: 0.10248714685440063\n",
      "batch: 13039, loss: 0.21872925758361816\n",
      "batch: 13040, loss: 0.04539404809474945\n",
      "batch: 13041, loss: 0.12792912125587463\n",
      "batch: 13042, loss: 0.085760697722435\n",
      "batch: 13043, loss: 0.23977507650852203\n",
      "batch: 13044, loss: 0.06594324111938477\n",
      "batch: 13045, loss: 0.0765606239438057\n",
      "batch: 13046, loss: 0.12333271652460098\n",
      "batch: 13047, loss: 0.08896824717521667\n",
      "batch: 13048, loss: 0.11505959928035736\n",
      "batch: 13049, loss: 0.18414953351020813\n",
      "batch: 13050, loss: 0.10831259936094284\n",
      "batch: 13051, loss: 0.1441764235496521\n",
      "batch: 13052, loss: 0.09145455062389374\n",
      "batch: 13053, loss: 0.09964104741811752\n",
      "batch: 13054, loss: 0.09657622873783112\n",
      "batch: 13055, loss: 0.1249544620513916\n",
      "batch: 13056, loss: 0.051713183522224426\n",
      "batch: 13057, loss: 0.07031206786632538\n",
      "batch: 13058, loss: 0.04499494284391403\n",
      "batch: 13059, loss: 0.09665187448263168\n",
      "batch: 13060, loss: 0.21779556572437286\n",
      "batch: 13061, loss: 0.1297379434108734\n",
      "batch: 13062, loss: 0.2020999640226364\n",
      "batch: 13063, loss: 0.19540174305438995\n",
      "batch: 13064, loss: 0.13163772225379944\n",
      "batch: 13065, loss: 0.16558197140693665\n",
      "batch: 13066, loss: 0.05158987641334534\n",
      "batch: 13067, loss: 0.07919800281524658\n",
      "batch: 13068, loss: 0.1057182252407074\n",
      "batch: 13069, loss: 0.10252001881599426\n",
      "batch: 13070, loss: 0.2057456076145172\n",
      "batch: 13071, loss: 0.26056110858917236\n",
      "batch: 13072, loss: 0.11480752378702164\n",
      "batch: 13073, loss: 0.03635755926370621\n",
      "batch: 13074, loss: 0.259518563747406\n",
      "batch: 13075, loss: 0.0901881530880928\n",
      "batch: 13076, loss: 0.09393619000911713\n",
      "batch: 13077, loss: 0.1331625133752823\n",
      "batch: 13078, loss: 0.10652630776166916\n",
      "batch: 13079, loss: 0.21790811419487\n",
      "batch: 13080, loss: 0.08534331619739532\n",
      "batch: 13081, loss: 0.05957706645131111\n",
      "batch: 13082, loss: 0.18346214294433594\n",
      "batch: 13083, loss: 0.0842907577753067\n",
      "batch: 13084, loss: 0.06547088921070099\n",
      "batch: 13085, loss: 0.19466793537139893\n",
      "batch: 13086, loss: 0.22321507334709167\n",
      "batch: 13087, loss: 0.04810776934027672\n",
      "batch: 13088, loss: 0.08095259964466095\n",
      "batch: 13089, loss: 0.06555987894535065\n",
      "batch: 13090, loss: 0.10826876014471054\n",
      "batch: 13091, loss: 0.090140201151371\n",
      "batch: 13092, loss: 0.0866163894534111\n",
      "batch: 13093, loss: 0.10860533267259598\n",
      "batch: 13094, loss: 0.16349269449710846\n",
      "batch: 13095, loss: 0.17295360565185547\n",
      "batch: 13096, loss: 0.045844029635190964\n",
      "batch: 13097, loss: 0.15346759557724\n",
      "batch: 13098, loss: 0.080905482172966\n",
      "batch: 13099, loss: 0.1708335280418396\n",
      "batch: 13100, loss: 0.18883654475212097\n",
      "model saved to ./saving/model.ckpt-131\n",
      "batch: 13101, loss: 0.11673892289400101\n",
      "batch: 13102, loss: 0.11763870716094971\n",
      "batch: 13103, loss: 0.14122384786605835\n",
      "batch: 13104, loss: 0.13702377676963806\n",
      "batch: 13105, loss: 0.09401001781225204\n",
      "batch: 13106, loss: 0.08275856077671051\n",
      "batch: 13107, loss: 0.06614628434181213\n",
      "batch: 13108, loss: 0.05789673328399658\n",
      "batch: 13109, loss: 0.052544645965099335\n",
      "batch: 13110, loss: 0.1821572482585907\n",
      "batch: 13111, loss: 0.1963399350643158\n",
      "batch: 13112, loss: 0.1074153259396553\n",
      "batch: 13113, loss: 0.05274827033281326\n",
      "batch: 13114, loss: 0.16450121998786926\n",
      "batch: 13115, loss: 0.035334810614585876\n",
      "batch: 13116, loss: 0.04891493171453476\n",
      "batch: 13117, loss: 0.1284342259168625\n",
      "batch: 13118, loss: 0.03007359802722931\n",
      "batch: 13119, loss: 0.1494927704334259\n",
      "batch: 13120, loss: 0.10286732017993927\n",
      "batch: 13121, loss: 0.23457331955432892\n",
      "batch: 13122, loss: 0.062346093356609344\n",
      "batch: 13123, loss: 0.11538149416446686\n",
      "batch: 13124, loss: 0.0739114060997963\n",
      "batch: 13125, loss: 0.10226720571517944\n",
      "batch: 13126, loss: 0.145842045545578\n",
      "batch: 13127, loss: 0.1542501002550125\n",
      "batch: 13128, loss: 0.15131528675556183\n",
      "batch: 13129, loss: 0.033843956887722015\n",
      "batch: 13130, loss: 0.20684048533439636\n",
      "batch: 13131, loss: 0.14041581749916077\n",
      "batch: 13132, loss: 0.07444879412651062\n",
      "batch: 13133, loss: 0.10221254080533981\n",
      "batch: 13134, loss: 0.059391945600509644\n",
      "batch: 13135, loss: 0.187669038772583\n",
      "batch: 13136, loss: 0.1988513171672821\n",
      "batch: 13137, loss: 0.1988028883934021\n",
      "batch: 13138, loss: 0.28569161891937256\n",
      "batch: 13139, loss: 0.12641854584217072\n",
      "batch: 13140, loss: 0.07418513298034668\n",
      "batch: 13141, loss: 0.08139236271381378\n",
      "batch: 13142, loss: 0.1317950189113617\n",
      "batch: 13143, loss: 0.10652223974466324\n",
      "batch: 13144, loss: 0.18200895190238953\n",
      "batch: 13145, loss: 0.15630510449409485\n",
      "batch: 13146, loss: 0.09093117713928223\n",
      "batch: 13147, loss: 0.10374796390533447\n",
      "batch: 13148, loss: 0.09547706693410873\n",
      "batch: 13149, loss: 0.043714940547943115\n",
      "batch: 13150, loss: 0.12066930532455444\n",
      "batch: 13151, loss: 0.09722638130187988\n",
      "batch: 13152, loss: 0.11796681582927704\n",
      "batch: 13153, loss: 0.14975206553936005\n",
      "batch: 13154, loss: 0.11692805588245392\n",
      "batch: 13155, loss: 0.10052824765443802\n",
      "batch: 13156, loss: 0.14758610725402832\n",
      "batch: 13157, loss: 0.0659034326672554\n",
      "batch: 13158, loss: 0.1377200186252594\n",
      "batch: 13159, loss: 0.08432041108608246\n",
      "batch: 13160, loss: 0.09082984924316406\n",
      "batch: 13161, loss: 0.11090807616710663\n",
      "batch: 13162, loss: 0.08114263415336609\n",
      "batch: 13163, loss: 0.07262808829545975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 13164, loss: 0.15526562929153442\n",
      "batch: 13165, loss: 0.08713940531015396\n",
      "batch: 13166, loss: 0.21881093084812164\n",
      "batch: 13167, loss: 0.12566545605659485\n",
      "batch: 13168, loss: 0.2820695638656616\n",
      "batch: 13169, loss: 0.1507617086172104\n",
      "batch: 13170, loss: 0.07481245696544647\n",
      "batch: 13171, loss: 0.08553189784288406\n",
      "batch: 13172, loss: 0.2681457996368408\n",
      "batch: 13173, loss: 0.033215105533599854\n",
      "batch: 13174, loss: 0.14708857238292694\n",
      "batch: 13175, loss: 0.27629297971725464\n",
      "batch: 13176, loss: 0.09333030879497528\n",
      "batch: 13177, loss: 0.09676841646432877\n",
      "batch: 13178, loss: 0.22215747833251953\n",
      "batch: 13179, loss: 0.21424973011016846\n",
      "batch: 13180, loss: 0.0647822767496109\n",
      "batch: 13181, loss: 0.10349725186824799\n",
      "batch: 13182, loss: 0.08401046693325043\n",
      "batch: 13183, loss: 0.03847531974315643\n",
      "batch: 13184, loss: 0.06317155808210373\n",
      "batch: 13185, loss: 0.05047685652971268\n",
      "batch: 13186, loss: 0.17339077591896057\n",
      "batch: 13187, loss: 0.12107739597558975\n",
      "batch: 13188, loss: 0.209395170211792\n",
      "batch: 13189, loss: 0.08654998242855072\n",
      "batch: 13190, loss: 0.08283708244562149\n",
      "batch: 13191, loss: 0.10908830165863037\n",
      "batch: 13192, loss: 0.10749465227127075\n",
      "batch: 13193, loss: 0.07424226403236389\n",
      "batch: 13194, loss: 0.1302317976951599\n",
      "batch: 13195, loss: 0.08020810782909393\n",
      "batch: 13196, loss: 0.09927986562252045\n",
      "batch: 13197, loss: 0.0473753921687603\n",
      "batch: 13198, loss: 0.07856966555118561\n",
      "batch: 13199, loss: 0.12380677461624146\n",
      "batch: 13200, loss: 0.05725424736738205\n",
      "model saved to ./saving/model.ckpt-132\n",
      "batch: 13201, loss: 0.15524128079414368\n",
      "batch: 13202, loss: 0.13628345727920532\n",
      "batch: 13203, loss: 0.049377184361219406\n",
      "batch: 13204, loss: 0.049325428903102875\n",
      "batch: 13205, loss: 0.11781981587409973\n",
      "batch: 13206, loss: 0.04740779101848602\n",
      "batch: 13207, loss: 0.18909774720668793\n",
      "batch: 13208, loss: 0.16526544094085693\n",
      "batch: 13209, loss: 0.08209626376628876\n",
      "batch: 13210, loss: 0.17735597491264343\n",
      "batch: 13211, loss: 0.13689327239990234\n",
      "batch: 13212, loss: 0.14858655631542206\n",
      "batch: 13213, loss: 0.0843571275472641\n",
      "batch: 13214, loss: 0.029103968292474747\n",
      "batch: 13215, loss: 0.07457706332206726\n",
      "batch: 13216, loss: 0.10056257247924805\n",
      "batch: 13217, loss: 0.12354348599910736\n",
      "batch: 13218, loss: 0.05448292940855026\n",
      "batch: 13219, loss: 0.11552026122808456\n",
      "batch: 13220, loss: 0.05456531047821045\n",
      "batch: 13221, loss: 0.09058302640914917\n",
      "batch: 13222, loss: 0.23041972517967224\n",
      "batch: 13223, loss: 0.1509055495262146\n",
      "batch: 13224, loss: 0.038884878158569336\n",
      "batch: 13225, loss: 0.15503911674022675\n",
      "batch: 13226, loss: 0.12133638560771942\n",
      "batch: 13227, loss: 0.10126059502363205\n",
      "batch: 13228, loss: 0.20694953203201294\n",
      "batch: 13229, loss: 0.05991249904036522\n",
      "batch: 13230, loss: 0.20823681354522705\n",
      "batch: 13231, loss: 0.1334618330001831\n",
      "batch: 13232, loss: 0.09966331720352173\n",
      "batch: 13233, loss: 0.08830731362104416\n",
      "batch: 13234, loss: 0.09098932147026062\n",
      "batch: 13235, loss: 0.053110167384147644\n",
      "batch: 13236, loss: 0.04541471600532532\n",
      "batch: 13237, loss: 0.06780374050140381\n",
      "batch: 13238, loss: 0.19180937111377716\n",
      "batch: 13239, loss: 0.08074397593736649\n",
      "batch: 13240, loss: 0.13577917218208313\n",
      "batch: 13241, loss: 0.08293063938617706\n",
      "batch: 13242, loss: 0.14124669134616852\n",
      "batch: 13243, loss: 0.048552315682172775\n",
      "batch: 13244, loss: 0.05882813036441803\n",
      "batch: 13245, loss: 0.11598535627126694\n",
      "batch: 13246, loss: 0.08895927667617798\n",
      "batch: 13247, loss: 0.045172713696956635\n",
      "batch: 13248, loss: 0.336698442697525\n",
      "batch: 13249, loss: 0.0547388419508934\n",
      "batch: 13250, loss: 0.10303054004907608\n",
      "batch: 13251, loss: 0.0922367051243782\n",
      "batch: 13252, loss: 0.03125113248825073\n",
      "batch: 13253, loss: 0.08084169030189514\n",
      "batch: 13254, loss: 0.17779862880706787\n",
      "batch: 13255, loss: 0.157042995095253\n",
      "batch: 13256, loss: 0.06054311245679855\n",
      "batch: 13257, loss: 0.11337094008922577\n",
      "batch: 13258, loss: 0.04924619942903519\n",
      "batch: 13259, loss: 0.10710544884204865\n",
      "batch: 13260, loss: 0.07977639883756638\n",
      "batch: 13261, loss: 0.13803789019584656\n",
      "batch: 13262, loss: 0.0982804000377655\n",
      "batch: 13263, loss: 0.10217069089412689\n",
      "batch: 13264, loss: 0.20773541927337646\n",
      "batch: 13265, loss: 0.07118753343820572\n",
      "batch: 13266, loss: 0.11742980778217316\n",
      "batch: 13267, loss: 0.1117657870054245\n",
      "batch: 13268, loss: 0.14615172147750854\n",
      "batch: 13269, loss: 0.09095875918865204\n",
      "batch: 13270, loss: 0.07610523700714111\n",
      "batch: 13271, loss: 0.04757062718272209\n",
      "batch: 13272, loss: 0.0923057347536087\n",
      "batch: 13273, loss: 0.16130009293556213\n",
      "batch: 13274, loss: 0.08164626359939575\n",
      "batch: 13275, loss: 0.09426648914813995\n",
      "batch: 13276, loss: 0.285548597574234\n",
      "batch: 13277, loss: 0.14125636219978333\n",
      "batch: 13278, loss: 0.08491570502519608\n",
      "batch: 13279, loss: 0.08807548880577087\n",
      "batch: 13280, loss: 0.247401162981987\n",
      "batch: 13281, loss: 0.1513463705778122\n",
      "batch: 13282, loss: 0.16902920603752136\n",
      "batch: 13283, loss: 0.2266218364238739\n",
      "batch: 13284, loss: 0.0755176916718483\n",
      "batch: 13285, loss: 0.08624346554279327\n",
      "batch: 13286, loss: 0.19839191436767578\n",
      "batch: 13287, loss: 0.11497630923986435\n",
      "batch: 13288, loss: 0.09306256473064423\n",
      "batch: 13289, loss: 0.08577381074428558\n",
      "batch: 13290, loss: 0.10312400758266449\n",
      "batch: 13291, loss: 0.0549631342291832\n",
      "batch: 13292, loss: 0.10255232453346252\n",
      "batch: 13293, loss: 0.0928359106183052\n",
      "batch: 13294, loss: 0.10526353865861893\n",
      "batch: 13295, loss: 0.16666698455810547\n",
      "batch: 13296, loss: 0.05736567825078964\n",
      "batch: 13297, loss: 0.0783858597278595\n",
      "batch: 13298, loss: 0.08602315187454224\n",
      "batch: 13299, loss: 0.08731396496295929\n",
      "batch: 13300, loss: 0.06994552910327911\n",
      "model saved to ./saving/model.ckpt-133\n",
      "batch: 13301, loss: 0.2101440280675888\n",
      "batch: 13302, loss: 0.2593289017677307\n",
      "batch: 13303, loss: 0.09837029874324799\n",
      "batch: 13304, loss: 0.10838556289672852\n",
      "batch: 13305, loss: 0.1864994913339615\n",
      "batch: 13306, loss: 0.10401654988527298\n",
      "batch: 13307, loss: 0.13441145420074463\n",
      "batch: 13308, loss: 0.23063130676746368\n",
      "batch: 13309, loss: 0.13183853030204773\n",
      "batch: 13310, loss: 0.08450767397880554\n",
      "batch: 13311, loss: 0.13238735496997833\n",
      "batch: 13312, loss: 0.12793712317943573\n",
      "batch: 13313, loss: 0.12911294400691986\n",
      "batch: 13314, loss: 0.0916573703289032\n",
      "batch: 13315, loss: 0.09122226387262344\n",
      "batch: 13316, loss: 0.11775349080562592\n",
      "batch: 13317, loss: 0.10496815294027328\n",
      "batch: 13318, loss: 0.05692262202501297\n",
      "batch: 13319, loss: 0.14811892807483673\n",
      "batch: 13320, loss: 0.0948658362030983\n",
      "batch: 13321, loss: 0.19837592542171478\n",
      "batch: 13322, loss: 0.1361600011587143\n",
      "batch: 13323, loss: 0.09474184364080429\n",
      "batch: 13324, loss: 0.07734459638595581\n",
      "batch: 13325, loss: 0.13527792692184448\n",
      "batch: 13326, loss: 0.2746770977973938\n",
      "batch: 13327, loss: 0.11342547833919525\n",
      "batch: 13328, loss: 0.06856994330883026\n",
      "batch: 13329, loss: 0.11337937414646149\n",
      "batch: 13330, loss: 0.16715015470981598\n",
      "batch: 13331, loss: 0.11905718594789505\n",
      "batch: 13332, loss: 0.13831262290477753\n",
      "batch: 13333, loss: 0.1297624111175537\n",
      "batch: 13334, loss: 0.14232781529426575\n",
      "batch: 13335, loss: 0.04525098204612732\n",
      "batch: 13336, loss: 0.16330823302268982\n",
      "batch: 13337, loss: 0.11133114993572235\n",
      "batch: 13338, loss: 0.11752337217330933\n",
      "batch: 13339, loss: 0.1669132262468338\n",
      "batch: 13340, loss: 0.07663178443908691\n",
      "batch: 13341, loss: 0.16376346349716187\n",
      "batch: 13342, loss: 0.04189823940396309\n",
      "batch: 13343, loss: 0.16888362169265747\n",
      "batch: 13344, loss: 0.06892343610525131\n",
      "batch: 13345, loss: 0.09889477491378784\n",
      "batch: 13346, loss: 0.17728811502456665\n",
      "batch: 13347, loss: 0.09222131967544556\n",
      "batch: 13348, loss: 0.10520225763320923\n",
      "batch: 13349, loss: 0.2212504893541336\n",
      "batch: 13350, loss: 0.08165597170591354\n",
      "batch: 13351, loss: 0.20224551856517792\n",
      "batch: 13352, loss: 0.05152766406536102\n",
      "batch: 13353, loss: 0.2424609512090683\n",
      "batch: 13354, loss: 0.04540927708148956\n",
      "batch: 13355, loss: 0.23018507659435272\n",
      "batch: 13356, loss: 0.09782622009515762\n",
      "batch: 13357, loss: 0.10990560054779053\n",
      "batch: 13358, loss: 0.12824535369873047\n",
      "batch: 13359, loss: 0.2071475237607956\n",
      "batch: 13360, loss: 0.12261970341205597\n",
      "batch: 13361, loss: 0.1493876427412033\n",
      "batch: 13362, loss: 0.11131101846694946\n",
      "batch: 13363, loss: 0.10570627450942993\n",
      "batch: 13364, loss: 0.03382723778486252\n",
      "batch: 13365, loss: 0.08449843525886536\n",
      "batch: 13366, loss: 0.11294901371002197\n",
      "batch: 13367, loss: 0.13910415768623352\n",
      "batch: 13368, loss: 0.1363253891468048\n",
      "batch: 13369, loss: 0.09860207140445709\n",
      "batch: 13370, loss: 0.08476855605840683\n",
      "batch: 13371, loss: 0.17203091084957123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 13372, loss: 0.04211776331067085\n",
      "batch: 13373, loss: 0.14318107068538666\n",
      "batch: 13374, loss: 0.12138395011425018\n",
      "batch: 13375, loss: 0.1942245215177536\n",
      "batch: 13376, loss: 0.05971633642911911\n",
      "batch: 13377, loss: 0.06968272477388382\n",
      "batch: 13378, loss: 0.138579323887825\n",
      "batch: 13379, loss: 0.06826110184192657\n",
      "batch: 13380, loss: 0.3225753605365753\n",
      "batch: 13381, loss: 0.09320007264614105\n",
      "batch: 13382, loss: 0.06667454540729523\n",
      "batch: 13383, loss: 0.1297023445367813\n",
      "batch: 13384, loss: 0.10951153934001923\n",
      "batch: 13385, loss: 0.09068776667118073\n",
      "batch: 13386, loss: 0.15672267973423004\n",
      "batch: 13387, loss: 0.1549292504787445\n",
      "batch: 13388, loss: 0.054753344506025314\n",
      "batch: 13389, loss: 0.049217820167541504\n",
      "batch: 13390, loss: 0.10364566743373871\n",
      "batch: 13391, loss: 0.06146934628486633\n",
      "batch: 13392, loss: 0.16839008033275604\n",
      "batch: 13393, loss: 0.10794682800769806\n",
      "batch: 13394, loss: 0.07818973064422607\n",
      "batch: 13395, loss: 0.049769725650548935\n",
      "batch: 13396, loss: 0.05486873537302017\n",
      "batch: 13397, loss: 0.06698121130466461\n",
      "batch: 13398, loss: 0.1284169852733612\n",
      "batch: 13399, loss: 0.05895835906267166\n",
      "batch: 13400, loss: 0.049995534121990204\n",
      "model saved to ./saving/model.ckpt-134\n",
      "batch: 13401, loss: 0.08490265905857086\n",
      "batch: 13402, loss: 0.19577091932296753\n",
      "batch: 13403, loss: 0.07338733971118927\n",
      "batch: 13404, loss: 0.0535929799079895\n",
      "batch: 13405, loss: 0.28406259417533875\n",
      "batch: 13406, loss: 0.045323118567466736\n",
      "batch: 13407, loss: 0.06868470460176468\n",
      "batch: 13408, loss: 0.09192822873592377\n",
      "batch: 13409, loss: 0.12057559937238693\n",
      "batch: 13410, loss: 0.17305731773376465\n",
      "batch: 13411, loss: 0.08253583312034607\n",
      "batch: 13412, loss: 0.23761168122291565\n",
      "batch: 13413, loss: 0.2040199339389801\n",
      "batch: 13414, loss: 0.12114453315734863\n",
      "batch: 13415, loss: 0.24296483397483826\n",
      "batch: 13416, loss: 0.028929397463798523\n",
      "batch: 13417, loss: 0.0851929634809494\n",
      "batch: 13418, loss: 0.07781781256198883\n",
      "batch: 13419, loss: 0.07705018669366837\n",
      "batch: 13420, loss: 0.07571665942668915\n",
      "batch: 13421, loss: 0.11667855829000473\n",
      "batch: 13422, loss: 0.23191873729228973\n",
      "batch: 13423, loss: 0.03991388902068138\n",
      "batch: 13424, loss: 0.07297778129577637\n",
      "batch: 13425, loss: 0.18163123726844788\n",
      "batch: 13426, loss: 0.08003153651952744\n",
      "batch: 13427, loss: 0.26163148880004883\n",
      "batch: 13428, loss: 0.11078864336013794\n",
      "batch: 13429, loss: 0.06503474712371826\n",
      "batch: 13430, loss: 0.21931593120098114\n",
      "batch: 13431, loss: 0.10216953605413437\n",
      "batch: 13432, loss: 0.1194533258676529\n",
      "batch: 13433, loss: 0.3142295181751251\n",
      "batch: 13434, loss: 0.16840873658657074\n",
      "batch: 13435, loss: 0.026440829038619995\n",
      "batch: 13436, loss: 0.0788159966468811\n",
      "batch: 13437, loss: 0.16016486287117004\n",
      "batch: 13438, loss: 0.0737060084939003\n",
      "batch: 13439, loss: 0.10008768737316132\n",
      "batch: 13440, loss: 0.10634920001029968\n",
      "batch: 13441, loss: 0.06596383452415466\n",
      "batch: 13442, loss: 0.05741431564092636\n",
      "batch: 13443, loss: 0.2729243338108063\n",
      "batch: 13444, loss: 0.11482147872447968\n",
      "batch: 13445, loss: 0.13450433313846588\n",
      "batch: 13446, loss: 0.15462946891784668\n",
      "batch: 13447, loss: 0.049659259617328644\n",
      "batch: 13448, loss: 0.04354345053434372\n",
      "batch: 13449, loss: 0.2109658420085907\n",
      "batch: 13450, loss: 0.16996054351329803\n",
      "batch: 13451, loss: 0.13568025827407837\n",
      "batch: 13452, loss: 0.18341973423957825\n",
      "batch: 13453, loss: 0.030186612159013748\n",
      "batch: 13454, loss: 0.0860188752412796\n",
      "batch: 13455, loss: 0.07441121339797974\n",
      "batch: 13456, loss: 0.19039621949195862\n",
      "batch: 13457, loss: 0.050725795328617096\n",
      "batch: 13458, loss: 0.11340057104825974\n",
      "batch: 13459, loss: 0.08546573668718338\n",
      "batch: 13460, loss: 0.09346512705087662\n",
      "batch: 13461, loss: 0.12976589798927307\n",
      "batch: 13462, loss: 0.19257837533950806\n",
      "batch: 13463, loss: 0.09289630502462387\n",
      "batch: 13464, loss: 0.050497960299253464\n",
      "batch: 13465, loss: 0.030149055644869804\n",
      "batch: 13466, loss: 0.19833144545555115\n",
      "batch: 13467, loss: 0.10600680112838745\n",
      "batch: 13468, loss: 0.23060506582260132\n",
      "batch: 13469, loss: 0.12918543815612793\n",
      "batch: 13470, loss: 0.060696739703416824\n",
      "batch: 13471, loss: 0.07205362617969513\n",
      "batch: 13472, loss: 0.09684932976961136\n",
      "batch: 13473, loss: 0.17617034912109375\n",
      "batch: 13474, loss: 0.056428663432598114\n",
      "batch: 13475, loss: 0.09083439409732819\n",
      "batch: 13476, loss: 0.12864625453948975\n",
      "batch: 13477, loss: 0.19901303946971893\n",
      "batch: 13478, loss: 0.06794314831495285\n",
      "batch: 13479, loss: 0.15130949020385742\n",
      "batch: 13480, loss: 0.08106079697608948\n",
      "batch: 13481, loss: 0.03525156527757645\n",
      "batch: 13482, loss: 0.16902291774749756\n",
      "batch: 13483, loss: 0.2503272294998169\n",
      "batch: 13484, loss: 0.04514769837260246\n",
      "batch: 13485, loss: 0.2415473759174347\n",
      "batch: 13486, loss: 0.08422507345676422\n",
      "batch: 13487, loss: 0.07725484669208527\n",
      "batch: 13488, loss: 0.07613001763820648\n",
      "batch: 13489, loss: 0.22908037900924683\n",
      "batch: 13490, loss: 0.0665639266371727\n",
      "batch: 13491, loss: 0.10567042976617813\n",
      "batch: 13492, loss: 0.07504095137119293\n",
      "batch: 13493, loss: 0.1129017174243927\n",
      "batch: 13494, loss: 0.04156023636460304\n",
      "batch: 13495, loss: 0.11130504310131073\n",
      "batch: 13496, loss: 0.07243794202804565\n",
      "batch: 13497, loss: 0.17086680233478546\n",
      "batch: 13498, loss: 0.050684552639722824\n",
      "batch: 13499, loss: 0.22885333001613617\n",
      "batch: 13500, loss: 0.11296520382165909\n",
      "model saved to ./saving/model.ckpt-135\n",
      "batch: 13501, loss: 0.05970481410622597\n",
      "batch: 13502, loss: 0.05112192779779434\n",
      "batch: 13503, loss: 0.06392907351255417\n",
      "batch: 13504, loss: 0.18015353381633759\n",
      "batch: 13505, loss: 0.10614223033189774\n",
      "batch: 13506, loss: 0.11712940037250519\n",
      "batch: 13507, loss: 0.125025674700737\n",
      "batch: 13508, loss: 0.03635881841182709\n",
      "batch: 13509, loss: 0.06124131381511688\n",
      "batch: 13510, loss: 0.12770454585552216\n",
      "batch: 13511, loss: 0.2269512414932251\n",
      "batch: 13512, loss: 0.15789970755577087\n",
      "batch: 13513, loss: 0.12815943360328674\n",
      "batch: 13514, loss: 0.12347660958766937\n",
      "batch: 13515, loss: 0.14526784420013428\n",
      "batch: 13516, loss: 0.032519832253456116\n",
      "batch: 13517, loss: 0.08264853060245514\n",
      "batch: 13518, loss: 0.20982608199119568\n",
      "batch: 13519, loss: 0.22094962000846863\n",
      "batch: 13520, loss: 0.16105350852012634\n",
      "batch: 13521, loss: 0.13714812695980072\n",
      "batch: 13522, loss: 0.09236237406730652\n",
      "batch: 13523, loss: 0.08194497227668762\n",
      "batch: 13524, loss: 0.12384150922298431\n",
      "batch: 13525, loss: 0.0998518168926239\n",
      "batch: 13526, loss: 0.11718817055225372\n",
      "batch: 13527, loss: 0.13283857703208923\n",
      "batch: 13528, loss: 0.06868308782577515\n",
      "batch: 13529, loss: 0.0996788814663887\n",
      "batch: 13530, loss: 0.11471093446016312\n",
      "batch: 13531, loss: 0.05067857727408409\n",
      "batch: 13532, loss: 0.09394210577011108\n",
      "batch: 13533, loss: 0.12497758865356445\n",
      "batch: 13534, loss: 0.08840245008468628\n",
      "batch: 13535, loss: 0.09567993134260178\n",
      "batch: 13536, loss: 0.08172214031219482\n",
      "batch: 13537, loss: 0.1443639099597931\n",
      "batch: 13538, loss: 0.1260625422000885\n",
      "batch: 13539, loss: 0.1005394458770752\n",
      "batch: 13540, loss: 0.0660630315542221\n",
      "batch: 13541, loss: 0.21368730068206787\n",
      "batch: 13542, loss: 0.09578706324100494\n",
      "batch: 13543, loss: 0.19068627059459686\n",
      "batch: 13544, loss: 0.08158628642559052\n",
      "batch: 13545, loss: 0.047662846744060516\n",
      "batch: 13546, loss: 0.08267907798290253\n",
      "batch: 13547, loss: 0.04290322959423065\n",
      "batch: 13548, loss: 0.22174042463302612\n",
      "batch: 13549, loss: 0.08831977099180222\n",
      "batch: 13550, loss: 0.032448314130306244\n",
      "batch: 13551, loss: 0.08905801177024841\n",
      "batch: 13552, loss: 0.06933224201202393\n",
      "batch: 13553, loss: 0.155174121260643\n",
      "batch: 13554, loss: 0.1686183512210846\n",
      "batch: 13555, loss: 0.08338063955307007\n",
      "batch: 13556, loss: 0.08347538113594055\n",
      "batch: 13557, loss: 0.045305751264095306\n",
      "batch: 13558, loss: 0.14748115837574005\n",
      "batch: 13559, loss: 0.1258402317762375\n",
      "batch: 13560, loss: 0.027534471824765205\n",
      "batch: 13561, loss: 0.041499845683574677\n",
      "batch: 13562, loss: 0.06413479894399643\n",
      "batch: 13563, loss: 0.08990397304296494\n",
      "batch: 13564, loss: 0.11395105719566345\n",
      "batch: 13565, loss: 0.1553662270307541\n",
      "batch: 13566, loss: 0.15967059135437012\n",
      "batch: 13567, loss: 0.07055479288101196\n",
      "batch: 13568, loss: 0.09796199202537537\n",
      "batch: 13569, loss: 0.05914098024368286\n",
      "batch: 13570, loss: 0.09218902885913849\n",
      "batch: 13571, loss: 0.1499376744031906\n",
      "batch: 13572, loss: 0.23184271156787872\n",
      "batch: 13573, loss: 0.11863822489976883\n",
      "batch: 13574, loss: 0.04366424307227135\n",
      "batch: 13575, loss: 0.03114420175552368\n",
      "batch: 13576, loss: 0.08573436737060547\n",
      "batch: 13577, loss: 0.1587548851966858\n",
      "batch: 13578, loss: 0.1498252898454666\n",
      "batch: 13579, loss: 0.026903832331299782\n",
      "batch: 13580, loss: 0.08593575656414032\n",
      "batch: 13581, loss: 0.22986286878585815\n",
      "batch: 13582, loss: 0.20335139334201813\n",
      "batch: 13583, loss: 0.09316869080066681\n",
      "batch: 13584, loss: 0.15693846344947815\n",
      "batch: 13585, loss: 0.0674315094947815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 13586, loss: 0.285078763961792\n",
      "batch: 13587, loss: 0.18792572617530823\n",
      "batch: 13588, loss: 0.059014033526182175\n",
      "batch: 13589, loss: 0.0720747783780098\n",
      "batch: 13590, loss: 0.0849447101354599\n",
      "batch: 13591, loss: 0.20652371644973755\n",
      "batch: 13592, loss: 0.07090452313423157\n",
      "batch: 13593, loss: 0.2831558883190155\n",
      "batch: 13594, loss: 0.10077539086341858\n",
      "batch: 13595, loss: 0.04990927875041962\n",
      "batch: 13596, loss: 0.2264922708272934\n",
      "batch: 13597, loss: 0.09164485335350037\n",
      "batch: 13598, loss: 0.20386093854904175\n",
      "batch: 13599, loss: 0.3679088354110718\n",
      "batch: 13600, loss: 0.12670987844467163\n",
      "model saved to ./saving/model.ckpt-136\n",
      "batch: 13601, loss: 0.22882907092571259\n",
      "batch: 13602, loss: 0.16204512119293213\n",
      "batch: 13603, loss: 0.24988290667533875\n",
      "batch: 13604, loss: 0.17414584755897522\n",
      "batch: 13605, loss: 0.08932308852672577\n",
      "batch: 13606, loss: 0.14090648293495178\n",
      "batch: 13607, loss: 0.07356208562850952\n",
      "batch: 13608, loss: 0.16560912132263184\n",
      "batch: 13609, loss: 0.1032983660697937\n",
      "batch: 13610, loss: 0.10235953330993652\n",
      "batch: 13611, loss: 0.23589828610420227\n",
      "batch: 13612, loss: 0.11348041892051697\n",
      "batch: 13613, loss: 0.17464891076087952\n",
      "batch: 13614, loss: 0.13349682092666626\n",
      "batch: 13615, loss: 0.05068916454911232\n",
      "batch: 13616, loss: 0.05158676952123642\n",
      "batch: 13617, loss: 0.1313258707523346\n",
      "batch: 13618, loss: 0.07321689277887344\n",
      "batch: 13619, loss: 0.08463677763938904\n",
      "batch: 13620, loss: 0.04153670370578766\n",
      "batch: 13621, loss: 0.054204061627388\n",
      "batch: 13622, loss: 0.24101048707962036\n",
      "batch: 13623, loss: 0.19682636857032776\n",
      "batch: 13624, loss: 0.11585663259029388\n",
      "batch: 13625, loss: 0.12578655779361725\n",
      "batch: 13626, loss: 0.13327698409557343\n",
      "batch: 13627, loss: 0.14475715160369873\n",
      "batch: 13628, loss: 0.05511472374200821\n",
      "batch: 13629, loss: 0.07120160013437271\n",
      "batch: 13630, loss: 0.0476173534989357\n",
      "batch: 13631, loss: 0.1529196798801422\n",
      "batch: 13632, loss: 0.12132923305034637\n",
      "batch: 13633, loss: 0.23582088947296143\n",
      "batch: 13634, loss: 0.1525973230600357\n",
      "batch: 13635, loss: 0.06338426470756531\n",
      "batch: 13636, loss: 0.36696475744247437\n",
      "batch: 13637, loss: 0.10913408547639847\n",
      "batch: 13638, loss: 0.07756244391202927\n",
      "batch: 13639, loss: 0.13631756603717804\n",
      "batch: 13640, loss: 0.052665047347545624\n",
      "batch: 13641, loss: 0.06169695034623146\n",
      "batch: 13642, loss: 0.08826562762260437\n",
      "batch: 13643, loss: 0.048874277621507645\n",
      "batch: 13644, loss: 0.09535212814807892\n",
      "batch: 13645, loss: 0.1048555076122284\n",
      "batch: 13646, loss: 0.10369345545768738\n",
      "batch: 13647, loss: 0.07236826419830322\n",
      "batch: 13648, loss: 0.11508885025978088\n",
      "batch: 13649, loss: 0.17163458466529846\n",
      "batch: 13650, loss: 0.07535581290721893\n",
      "batch: 13651, loss: 0.04106035456061363\n",
      "batch: 13652, loss: 0.16512846946716309\n",
      "batch: 13653, loss: 0.11512183398008347\n",
      "batch: 13654, loss: 0.18334853649139404\n",
      "batch: 13655, loss: 0.09927327185869217\n",
      "batch: 13656, loss: 0.15975651144981384\n",
      "batch: 13657, loss: 0.060086023062467575\n",
      "batch: 13658, loss: 0.060414642095565796\n",
      "batch: 13659, loss: 0.0438695065677166\n",
      "batch: 13660, loss: 0.05899379402399063\n",
      "batch: 13661, loss: 0.055288106203079224\n",
      "batch: 13662, loss: 0.07537148892879486\n",
      "batch: 13663, loss: 0.18283608555793762\n",
      "batch: 13664, loss: 0.13056468963623047\n",
      "batch: 13665, loss: 0.08280248939990997\n",
      "batch: 13666, loss: 0.14857029914855957\n",
      "batch: 13667, loss: 0.0613609179854393\n",
      "batch: 13668, loss: 0.06985621154308319\n",
      "batch: 13669, loss: 0.08405065536499023\n",
      "batch: 13670, loss: 0.24907824397087097\n",
      "batch: 13671, loss: 0.11630350351333618\n",
      "batch: 13672, loss: 0.1383155882358551\n",
      "batch: 13673, loss: 0.10978527367115021\n",
      "batch: 13674, loss: 0.10791242122650146\n",
      "batch: 13675, loss: 0.06368092447519302\n",
      "batch: 13676, loss: 0.0609564371407032\n",
      "batch: 13677, loss: 0.11253128945827484\n",
      "batch: 13678, loss: 0.12315201759338379\n",
      "batch: 13679, loss: 0.2329258918762207\n",
      "batch: 13680, loss: 0.19535675644874573\n",
      "batch: 13681, loss: 0.10455653071403503\n",
      "batch: 13682, loss: 0.10408569872379303\n",
      "batch: 13683, loss: 0.09553870558738708\n",
      "batch: 13684, loss: 0.049479320645332336\n",
      "batch: 13685, loss: 0.06985986232757568\n",
      "batch: 13686, loss: 0.1591784656047821\n",
      "batch: 13687, loss: 0.07690902799367905\n",
      "batch: 13688, loss: 0.05707656592130661\n",
      "batch: 13689, loss: 0.14470915496349335\n",
      "batch: 13690, loss: 0.06808377802371979\n",
      "batch: 13691, loss: 0.12789228558540344\n",
      "batch: 13692, loss: 0.18146741390228271\n",
      "batch: 13693, loss: 0.13224828243255615\n",
      "batch: 13694, loss: 0.07719848304986954\n",
      "batch: 13695, loss: 0.12795627117156982\n",
      "batch: 13696, loss: 0.10189114511013031\n",
      "batch: 13697, loss: 0.1023988127708435\n",
      "batch: 13698, loss: 0.0797140821814537\n",
      "batch: 13699, loss: 0.1601385772228241\n",
      "batch: 13700, loss: 0.0972970575094223\n",
      "model saved to ./saving/model.ckpt-137\n",
      "batch: 13701, loss: 0.10635755956172943\n",
      "batch: 13702, loss: 0.13642385601997375\n",
      "batch: 13703, loss: 0.07816363871097565\n",
      "batch: 13704, loss: 0.06334346532821655\n",
      "batch: 13705, loss: 0.29933202266693115\n",
      "batch: 13706, loss: 0.07718661427497864\n",
      "batch: 13707, loss: 0.16863174736499786\n",
      "batch: 13708, loss: 0.1716345250606537\n",
      "batch: 13709, loss: 0.06653863191604614\n",
      "batch: 13710, loss: 0.10148413479328156\n",
      "batch: 13711, loss: 0.09864410758018494\n",
      "batch: 13712, loss: 0.0837014764547348\n",
      "batch: 13713, loss: 0.121578648686409\n",
      "batch: 13714, loss: 0.15303125977516174\n",
      "batch: 13715, loss: 0.228177011013031\n",
      "batch: 13716, loss: 0.18768121302127838\n",
      "batch: 13717, loss: 0.04922777786850929\n",
      "batch: 13718, loss: 0.13391289114952087\n",
      "batch: 13719, loss: 0.052858054637908936\n",
      "batch: 13720, loss: 0.07059100270271301\n",
      "batch: 13721, loss: 0.06126230210065842\n",
      "batch: 13722, loss: 0.07467314600944519\n",
      "batch: 13723, loss: 0.12866391241550446\n",
      "batch: 13724, loss: 0.07880473136901855\n",
      "batch: 13725, loss: 0.06732578575611115\n",
      "batch: 13726, loss: 0.0922001451253891\n",
      "batch: 13727, loss: 0.08332160115242004\n",
      "batch: 13728, loss: 0.10225671529769897\n",
      "batch: 13729, loss: 0.17973540723323822\n",
      "batch: 13730, loss: 0.06312814354896545\n",
      "batch: 13731, loss: 0.10776084661483765\n",
      "batch: 13732, loss: 0.05084477365016937\n",
      "batch: 13733, loss: 0.04786217585206032\n",
      "batch: 13734, loss: 0.2630612850189209\n",
      "batch: 13735, loss: 0.07052697241306305\n",
      "batch: 13736, loss: 0.09712319821119308\n",
      "batch: 13737, loss: 0.08862379193305969\n",
      "batch: 13738, loss: 0.07749159634113312\n",
      "batch: 13739, loss: 0.15249095857143402\n",
      "batch: 13740, loss: 0.2170075923204422\n",
      "batch: 13741, loss: 0.06424739211797714\n",
      "batch: 13742, loss: 0.071112722158432\n",
      "batch: 13743, loss: 0.11722283065319061\n",
      "batch: 13744, loss: 0.12525087594985962\n",
      "batch: 13745, loss: 0.060500890016555786\n",
      "batch: 13746, loss: 0.25048357248306274\n",
      "batch: 13747, loss: 0.1217842698097229\n",
      "batch: 13748, loss: 0.16585397720336914\n",
      "batch: 13749, loss: 0.03907240554690361\n",
      "batch: 13750, loss: 0.04490574449300766\n",
      "batch: 13751, loss: 0.09721200913190842\n",
      "batch: 13752, loss: 0.13140413165092468\n",
      "batch: 13753, loss: 0.04674831032752991\n",
      "batch: 13754, loss: 0.0936061441898346\n",
      "batch: 13755, loss: 0.11648336797952652\n",
      "batch: 13756, loss: 0.17640843987464905\n",
      "batch: 13757, loss: 0.2046447992324829\n",
      "batch: 13758, loss: 0.11310240626335144\n",
      "batch: 13759, loss: 0.052485108375549316\n",
      "batch: 13760, loss: 0.1687159389257431\n",
      "batch: 13761, loss: 0.15886029601097107\n",
      "batch: 13762, loss: 0.19186091423034668\n",
      "batch: 13763, loss: 0.12176922708749771\n",
      "batch: 13764, loss: 0.16227403283119202\n",
      "batch: 13765, loss: 0.08435685932636261\n",
      "batch: 13766, loss: 0.10422029346227646\n",
      "batch: 13767, loss: 0.1435764878988266\n",
      "batch: 13768, loss: 0.06542374193668365\n",
      "batch: 13769, loss: 0.12030372768640518\n",
      "batch: 13770, loss: 0.06970175355672836\n",
      "batch: 13771, loss: 0.07795852422714233\n",
      "batch: 13772, loss: 0.10855308175086975\n",
      "batch: 13773, loss: 0.10818401724100113\n",
      "batch: 13774, loss: 0.08355434983968735\n",
      "batch: 13775, loss: 0.07826050370931625\n",
      "batch: 13776, loss: 0.22360694408416748\n",
      "batch: 13777, loss: 0.11199469119310379\n",
      "batch: 13778, loss: 0.08672894537448883\n",
      "batch: 13779, loss: 0.09000639617443085\n",
      "batch: 13780, loss: 0.09507390856742859\n",
      "batch: 13781, loss: 0.09102101624011993\n",
      "batch: 13782, loss: 0.06033337861299515\n",
      "batch: 13783, loss: 0.06377062201499939\n",
      "batch: 13784, loss: 0.14989131689071655\n",
      "batch: 13785, loss: 0.05934055149555206\n",
      "batch: 13786, loss: 0.049570631235837936\n",
      "batch: 13787, loss: 0.04976784437894821\n",
      "batch: 13788, loss: 0.13755640387535095\n",
      "batch: 13789, loss: 0.1307828575372696\n",
      "batch: 13790, loss: 0.216084286570549\n",
      "batch: 13791, loss: 0.08451376855373383\n",
      "batch: 13792, loss: 0.2038586288690567\n",
      "batch: 13793, loss: 0.12832441926002502\n",
      "batch: 13794, loss: 0.15259304642677307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 13795, loss: 0.04801619052886963\n",
      "batch: 13796, loss: 0.19472309947013855\n",
      "batch: 13797, loss: 0.2863757014274597\n",
      "batch: 13798, loss: 0.2017810046672821\n",
      "batch: 13799, loss: 0.060291022062301636\n",
      "batch: 13800, loss: 0.15377017855644226\n",
      "model saved to ./saving/model.ckpt-138\n",
      "batch: 13801, loss: 0.11585725843906403\n",
      "batch: 13802, loss: 0.1187971830368042\n",
      "batch: 13803, loss: 0.1450469195842743\n",
      "batch: 13804, loss: 0.17663252353668213\n",
      "batch: 13805, loss: 0.10213227570056915\n",
      "batch: 13806, loss: 0.0965704470872879\n",
      "batch: 13807, loss: 0.1444624960422516\n",
      "batch: 13808, loss: 0.15794244408607483\n",
      "batch: 13809, loss: 0.21180875599384308\n",
      "batch: 13810, loss: 0.06959090381860733\n",
      "batch: 13811, loss: 0.1666737049818039\n",
      "batch: 13812, loss: 0.0844670832157135\n",
      "batch: 13813, loss: 0.08856627345085144\n",
      "batch: 13814, loss: 0.05189269036054611\n",
      "batch: 13815, loss: 0.08737145364284515\n",
      "batch: 13816, loss: 0.10197935253381729\n",
      "batch: 13817, loss: 0.08948809653520584\n",
      "batch: 13818, loss: 0.09395826607942581\n",
      "batch: 13819, loss: 0.11059499531984329\n",
      "batch: 13820, loss: 0.08840078115463257\n",
      "batch: 13821, loss: 0.09854340553283691\n",
      "batch: 13822, loss: 0.06328043341636658\n",
      "batch: 13823, loss: 0.0579381138086319\n",
      "batch: 13824, loss: 0.0890737771987915\n",
      "batch: 13825, loss: 0.07143820822238922\n",
      "batch: 13826, loss: 0.09704530984163284\n",
      "batch: 13827, loss: 0.08157993108034134\n",
      "batch: 13828, loss: 0.08352106809616089\n",
      "batch: 13829, loss: 0.1839631199836731\n",
      "batch: 13830, loss: 0.08397282660007477\n",
      "batch: 13831, loss: 0.05829017236828804\n",
      "batch: 13832, loss: 0.05940762162208557\n",
      "batch: 13833, loss: 0.07122284173965454\n",
      "batch: 13834, loss: 0.11225444078445435\n",
      "batch: 13835, loss: 0.06748238950967789\n",
      "batch: 13836, loss: 0.05196848511695862\n",
      "batch: 13837, loss: 0.07088982313871384\n",
      "batch: 13838, loss: 0.06922145932912827\n",
      "batch: 13839, loss: 0.12431513518095016\n",
      "batch: 13840, loss: 0.09270577132701874\n",
      "batch: 13841, loss: 0.06685513257980347\n",
      "batch: 13842, loss: 0.10029114782810211\n",
      "batch: 13843, loss: 0.20883303880691528\n",
      "batch: 13844, loss: 0.3496081531047821\n",
      "batch: 13845, loss: 0.08292767405509949\n",
      "batch: 13846, loss: 0.03938153386116028\n",
      "batch: 13847, loss: 0.07162043452262878\n",
      "batch: 13848, loss: 0.13567772507667542\n",
      "batch: 13849, loss: 0.21622221171855927\n",
      "batch: 13850, loss: 0.08670308440923691\n",
      "batch: 13851, loss: 0.16800151765346527\n",
      "batch: 13852, loss: 0.15194179117679596\n",
      "batch: 13853, loss: 0.05417124554514885\n",
      "batch: 13854, loss: 0.3041933476924896\n",
      "batch: 13855, loss: 0.21772536635398865\n",
      "batch: 13856, loss: 0.172963485121727\n",
      "batch: 13857, loss: 0.16870439052581787\n",
      "batch: 13858, loss: 0.061045072972774506\n",
      "batch: 13859, loss: 0.11233233660459518\n",
      "batch: 13860, loss: 0.16625140607357025\n",
      "batch: 13861, loss: 0.10002288222312927\n",
      "batch: 13862, loss: 0.04592194780707359\n",
      "batch: 13863, loss: 0.061438027769327164\n",
      "batch: 13864, loss: 0.15047749876976013\n",
      "batch: 13865, loss: 0.17694562673568726\n",
      "batch: 13866, loss: 0.1568644940853119\n",
      "batch: 13867, loss: 0.17484340071678162\n",
      "batch: 13868, loss: 0.11969394981861115\n",
      "batch: 13869, loss: 0.20055225491523743\n",
      "batch: 13870, loss: 0.10196863114833832\n",
      "batch: 13871, loss: 0.05066213011741638\n",
      "batch: 13872, loss: 0.13612371683120728\n",
      "batch: 13873, loss: 0.0982307568192482\n",
      "batch: 13874, loss: 0.06898711621761322\n",
      "batch: 13875, loss: 0.11663475632667542\n",
      "batch: 13876, loss: 0.05693953484296799\n",
      "batch: 13877, loss: 0.10702913999557495\n",
      "batch: 13878, loss: 0.09120786190032959\n",
      "batch: 13879, loss: 0.08108402788639069\n",
      "batch: 13880, loss: 0.0872628465294838\n",
      "batch: 13881, loss: 0.08004429936408997\n",
      "batch: 13882, loss: 0.06292624771595001\n",
      "batch: 13883, loss: 0.09385746717453003\n",
      "batch: 13884, loss: 0.09481656551361084\n",
      "batch: 13885, loss: 0.05173180252313614\n",
      "batch: 13886, loss: 0.2476978600025177\n",
      "batch: 13887, loss: 0.08495654910802841\n",
      "batch: 13888, loss: 0.07645036280155182\n",
      "batch: 13889, loss: 0.14131338894367218\n",
      "batch: 13890, loss: 0.12944471836090088\n",
      "batch: 13891, loss: 0.12457210570573807\n",
      "batch: 13892, loss: 0.2132105827331543\n",
      "batch: 13893, loss: 0.07723682373762131\n",
      "batch: 13894, loss: 0.12486756592988968\n",
      "batch: 13895, loss: 0.07234588265419006\n",
      "batch: 13896, loss: 0.1274784356355667\n",
      "batch: 13897, loss: 0.1269209384918213\n",
      "batch: 13898, loss: 0.11112037301063538\n",
      "batch: 13899, loss: 0.17868317663669586\n",
      "batch: 13900, loss: 0.059657417237758636\n",
      "model saved to ./saving/model.ckpt-139\n",
      "batch: 13901, loss: 0.06186686456203461\n",
      "batch: 13902, loss: 0.13148723542690277\n",
      "batch: 13903, loss: 0.09527307748794556\n",
      "batch: 13904, loss: 0.1607157289981842\n",
      "batch: 13905, loss: 0.04397355765104294\n",
      "batch: 13906, loss: 0.16333433985710144\n",
      "batch: 13907, loss: 0.12444606423377991\n",
      "batch: 13908, loss: 0.07591591775417328\n",
      "batch: 13909, loss: 0.11826220899820328\n",
      "batch: 13910, loss: 0.14127716422080994\n",
      "batch: 13911, loss: 0.07700976729393005\n",
      "batch: 13912, loss: 0.07080898433923721\n",
      "batch: 13913, loss: 0.11454756557941437\n",
      "batch: 13914, loss: 0.0396575927734375\n",
      "batch: 13915, loss: 0.08331689983606339\n",
      "batch: 13916, loss: 0.13152150809764862\n",
      "batch: 13917, loss: 0.09614776074886322\n",
      "batch: 13918, loss: 0.10856287181377411\n",
      "batch: 13919, loss: 0.10579639673233032\n",
      "batch: 13920, loss: 0.15528273582458496\n",
      "batch: 13921, loss: 0.07799914479255676\n",
      "batch: 13922, loss: 0.03574179485440254\n",
      "batch: 13923, loss: 0.21183814108371735\n",
      "batch: 13924, loss: 0.04295419156551361\n",
      "batch: 13925, loss: 0.03791483864188194\n",
      "batch: 13926, loss: 0.05478984862565994\n",
      "batch: 13927, loss: 0.05916755646467209\n",
      "batch: 13928, loss: 0.09194357693195343\n",
      "batch: 13929, loss: 0.10144492238759995\n",
      "batch: 13930, loss: 0.11390969157218933\n",
      "batch: 13931, loss: 0.20991834998130798\n",
      "batch: 13932, loss: 0.08655758947134018\n",
      "batch: 13933, loss: 0.23472246527671814\n",
      "batch: 13934, loss: 0.14172063767910004\n",
      "batch: 13935, loss: 0.062119245529174805\n",
      "batch: 13936, loss: 0.052723199129104614\n",
      "batch: 13937, loss: 0.10229073464870453\n",
      "batch: 13938, loss: 0.13771715760231018\n",
      "batch: 13939, loss: 0.08160983771085739\n",
      "batch: 13940, loss: 0.044939517974853516\n",
      "batch: 13941, loss: 0.14929693937301636\n",
      "batch: 13942, loss: 0.049585774540901184\n",
      "batch: 13943, loss: 0.08003342151641846\n",
      "batch: 13944, loss: 0.09470108151435852\n",
      "batch: 13945, loss: 0.13029205799102783\n",
      "batch: 13946, loss: 0.06666827201843262\n",
      "batch: 13947, loss: 0.18594594299793243\n",
      "batch: 13948, loss: 0.07106469571590424\n",
      "batch: 13949, loss: 0.16029515862464905\n",
      "batch: 13950, loss: 0.0460299551486969\n",
      "batch: 13951, loss: 0.09699761867523193\n",
      "batch: 13952, loss: 0.09170518815517426\n",
      "batch: 13953, loss: 0.05393385887145996\n",
      "batch: 13954, loss: 0.10632967203855515\n",
      "batch: 13955, loss: 0.16470572352409363\n",
      "batch: 13956, loss: 0.14788389205932617\n",
      "batch: 13957, loss: 0.12345802783966064\n",
      "batch: 13958, loss: 0.11377310752868652\n",
      "batch: 13959, loss: 0.14627377688884735\n",
      "batch: 13960, loss: 0.10301105678081512\n",
      "batch: 13961, loss: 0.0624886192381382\n",
      "batch: 13962, loss: 0.0646364688873291\n",
      "batch: 13963, loss: 0.035677481442689896\n",
      "batch: 13964, loss: 0.040335364639759064\n",
      "batch: 13965, loss: 0.12541529536247253\n",
      "batch: 13966, loss: 0.08328516781330109\n",
      "batch: 13967, loss: 0.050117287784814835\n",
      "batch: 13968, loss: 0.09388744831085205\n",
      "batch: 13969, loss: 0.08081115037202835\n",
      "batch: 13970, loss: 0.061188071966171265\n",
      "batch: 13971, loss: 0.07524871826171875\n",
      "batch: 13972, loss: 0.07310818135738373\n",
      "batch: 13973, loss: 0.055835604667663574\n",
      "batch: 13974, loss: 0.10311540961265564\n",
      "batch: 13975, loss: 0.10949482768774033\n",
      "batch: 13976, loss: 0.20453257858753204\n",
      "batch: 13977, loss: 0.13605919480323792\n",
      "batch: 13978, loss: 0.15143153071403503\n",
      "batch: 13979, loss: 0.08930205553770065\n",
      "batch: 13980, loss: 0.09992849826812744\n",
      "batch: 13981, loss: 0.0771862119436264\n",
      "batch: 13982, loss: 0.14196228981018066\n",
      "batch: 13983, loss: 0.13123337924480438\n",
      "batch: 13984, loss: 0.11653497070074081\n",
      "batch: 13985, loss: 0.048025354743003845\n",
      "batch: 13986, loss: 0.2537439167499542\n",
      "batch: 13987, loss: 0.13201652467250824\n",
      "batch: 13988, loss: 0.09346282482147217\n",
      "batch: 13989, loss: 0.2046263962984085\n",
      "batch: 13990, loss: 0.12998291850090027\n",
      "batch: 13991, loss: 0.08536987751722336\n",
      "batch: 13992, loss: 0.2999995946884155\n",
      "batch: 13993, loss: 0.1580250859260559\n",
      "batch: 13994, loss: 0.21246741712093353\n",
      "batch: 13995, loss: 0.23598837852478027\n",
      "batch: 13996, loss: 0.35074925422668457\n",
      "batch: 13997, loss: 0.3259878158569336\n",
      "batch: 13998, loss: 0.10870639979839325\n",
      "batch: 13999, loss: 0.08692354708909988\n",
      "batch: 14000, loss: 0.03490550071001053\n",
      "model saved to ./saving/model.ckpt-140\n",
      "batch: 14001, loss: 0.10965421795845032\n",
      "batch: 14002, loss: 0.2337547242641449\n",
      "batch: 14003, loss: 0.20478445291519165\n",
      "batch: 14004, loss: 0.11063001304864883\n",
      "batch: 14005, loss: 0.04870697110891342\n",
      "batch: 14006, loss: 0.23922261595726013\n",
      "batch: 14007, loss: 0.057767219841480255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 14008, loss: 0.07682324945926666\n",
      "batch: 14009, loss: 0.21694369614124298\n",
      "batch: 14010, loss: 0.2955225110054016\n",
      "batch: 14011, loss: 0.07583940029144287\n",
      "batch: 14012, loss: 0.19047686457633972\n",
      "batch: 14013, loss: 0.053060077130794525\n",
      "batch: 14014, loss: 0.027782337740063667\n",
      "batch: 14015, loss: 0.09329023212194443\n",
      "batch: 14016, loss: 0.060320109128952026\n",
      "batch: 14017, loss: 0.09157680720090866\n",
      "batch: 14018, loss: 0.27472972869873047\n",
      "batch: 14019, loss: 0.09662967920303345\n",
      "batch: 14020, loss: 0.07710307091474533\n",
      "batch: 14021, loss: 0.03945140168070793\n",
      "batch: 14022, loss: 0.09469185769557953\n",
      "batch: 14023, loss: 0.13190540671348572\n",
      "batch: 14024, loss: 0.04953749477863312\n",
      "batch: 14025, loss: 0.027634382247924805\n",
      "batch: 14026, loss: 0.0772576630115509\n",
      "batch: 14027, loss: 0.06671929359436035\n",
      "batch: 14028, loss: 0.09754041582345963\n",
      "batch: 14029, loss: 0.2525622248649597\n",
      "batch: 14030, loss: 0.12145517766475677\n",
      "batch: 14031, loss: 0.08871753513813019\n",
      "batch: 14032, loss: 0.16459468007087708\n",
      "batch: 14033, loss: 0.03979583829641342\n",
      "batch: 14034, loss: 0.16747698187828064\n",
      "batch: 14035, loss: 0.06809858977794647\n",
      "batch: 14036, loss: 0.08773376047611237\n",
      "batch: 14037, loss: 0.11548560857772827\n",
      "batch: 14038, loss: 0.172108992934227\n",
      "batch: 14039, loss: 0.12818443775177002\n",
      "batch: 14040, loss: 0.13672928512096405\n",
      "batch: 14041, loss: 0.10753054171800613\n",
      "batch: 14042, loss: 0.05095886439085007\n",
      "batch: 14043, loss: 0.10706023871898651\n",
      "batch: 14044, loss: 0.2773120403289795\n",
      "batch: 14045, loss: 0.06440745294094086\n",
      "batch: 14046, loss: 0.2715827226638794\n",
      "batch: 14047, loss: 0.12231384217739105\n",
      "batch: 14048, loss: 0.09499341249465942\n",
      "batch: 14049, loss: 0.057507727295160294\n",
      "batch: 14050, loss: 0.073345847427845\n",
      "batch: 14051, loss: 0.08088766038417816\n",
      "batch: 14052, loss: 0.16461709141731262\n",
      "batch: 14053, loss: 0.10104788839817047\n",
      "batch: 14054, loss: 0.09725601971149445\n",
      "batch: 14055, loss: 0.10923749208450317\n",
      "batch: 14056, loss: 0.11840075254440308\n",
      "batch: 14057, loss: 0.10187577456235886\n",
      "batch: 14058, loss: 0.05997787415981293\n",
      "batch: 14059, loss: 0.07460547238588333\n",
      "batch: 14060, loss: 0.04809028282761574\n",
      "batch: 14061, loss: 0.22014659643173218\n",
      "batch: 14062, loss: 0.09527820348739624\n",
      "batch: 14063, loss: 0.24494826793670654\n",
      "batch: 14064, loss: 0.12486506253480911\n",
      "batch: 14065, loss: 0.10320256650447845\n",
      "batch: 14066, loss: 0.0797443613409996\n",
      "batch: 14067, loss: 0.12333524227142334\n",
      "batch: 14068, loss: 0.15045632421970367\n",
      "batch: 14069, loss: 0.15056823194026947\n",
      "batch: 14070, loss: 0.07224313914775848\n",
      "batch: 14071, loss: 0.12305015325546265\n",
      "batch: 14072, loss: 0.07609141618013382\n",
      "batch: 14073, loss: 0.14327317476272583\n",
      "batch: 14074, loss: 0.14493539929389954\n",
      "batch: 14075, loss: 0.12997967004776\n",
      "batch: 14076, loss: 0.10638174414634705\n",
      "batch: 14077, loss: 0.13848289847373962\n",
      "batch: 14078, loss: 0.1282333880662918\n",
      "batch: 14079, loss: 0.07643754780292511\n",
      "batch: 14080, loss: 0.10429130494594574\n",
      "batch: 14081, loss: 0.17094412446022034\n",
      "batch: 14082, loss: 0.08709630370140076\n",
      "batch: 14083, loss: 0.06591090559959412\n",
      "batch: 14084, loss: 0.07729992270469666\n",
      "batch: 14085, loss: 0.05001291260123253\n",
      "batch: 14086, loss: 0.09496931731700897\n",
      "batch: 14087, loss: 0.17508697509765625\n",
      "batch: 14088, loss: 0.06667161732912064\n",
      "batch: 14089, loss: 0.14921274781227112\n",
      "batch: 14090, loss: 0.08699028193950653\n",
      "batch: 14091, loss: 0.08408458530902863\n",
      "batch: 14092, loss: 0.07649124413728714\n",
      "batch: 14093, loss: 0.08603581041097641\n",
      "batch: 14094, loss: 0.1088852733373642\n",
      "batch: 14095, loss: 0.13392756879329681\n",
      "batch: 14096, loss: 0.1251818984746933\n",
      "batch: 14097, loss: 0.19371584057807922\n",
      "batch: 14098, loss: 0.038050614297389984\n",
      "batch: 14099, loss: 0.11915913224220276\n",
      "batch: 14100, loss: 0.11027642339468002\n",
      "model saved to ./saving/model.ckpt-141\n",
      "batch: 14101, loss: 0.11998588591814041\n",
      "batch: 14102, loss: 0.061061792075634\n",
      "batch: 14103, loss: 0.09833264350891113\n",
      "batch: 14104, loss: 0.11076392978429794\n",
      "batch: 14105, loss: 0.10638239234685898\n",
      "batch: 14106, loss: 0.034870728850364685\n",
      "batch: 14107, loss: 0.1282561719417572\n",
      "batch: 14108, loss: 0.11431675404310226\n",
      "batch: 14109, loss: 0.10074205696582794\n",
      "batch: 14110, loss: 0.12200363725423813\n",
      "batch: 14111, loss: 0.04653323441743851\n",
      "batch: 14112, loss: 0.07661962509155273\n",
      "batch: 14113, loss: 0.14006143808364868\n",
      "batch: 14114, loss: 0.14574246108531952\n",
      "batch: 14115, loss: 0.09707629680633545\n",
      "batch: 14116, loss: 0.07501630485057831\n",
      "batch: 14117, loss: 0.11968323588371277\n",
      "batch: 14118, loss: 0.09531936049461365\n",
      "batch: 14119, loss: 0.09090755134820938\n",
      "batch: 14120, loss: 0.2641471028327942\n",
      "batch: 14121, loss: 0.12648504972457886\n",
      "batch: 14122, loss: 0.08455564081668854\n",
      "batch: 14123, loss: 0.10796672850847244\n",
      "batch: 14124, loss: 0.08697634935379028\n",
      "batch: 14125, loss: 0.13426536321640015\n",
      "batch: 14126, loss: 0.030386226251721382\n",
      "batch: 14127, loss: 0.18390893936157227\n",
      "batch: 14128, loss: 0.24164007604122162\n",
      "batch: 14129, loss: 0.08375103026628494\n",
      "batch: 14130, loss: 0.049145400524139404\n",
      "batch: 14131, loss: 0.07649755477905273\n",
      "batch: 14132, loss: 0.1385779231786728\n",
      "batch: 14133, loss: 0.12275572121143341\n",
      "batch: 14134, loss: 0.08961080014705658\n",
      "batch: 14135, loss: 0.12310783565044403\n",
      "batch: 14136, loss: 0.07021786272525787\n",
      "batch: 14137, loss: 0.0641091912984848\n",
      "batch: 14138, loss: 0.08308172225952148\n",
      "batch: 14139, loss: 0.05309116095304489\n",
      "batch: 14140, loss: 0.2718144655227661\n",
      "batch: 14141, loss: 0.1854727566242218\n",
      "batch: 14142, loss: 0.08242380619049072\n",
      "batch: 14143, loss: 0.06748808175325394\n",
      "batch: 14144, loss: 0.08764879405498505\n",
      "batch: 14145, loss: 0.051277514547109604\n",
      "batch: 14146, loss: 0.05827175825834274\n",
      "batch: 14147, loss: 0.1193007081747055\n",
      "batch: 14148, loss: 0.06665080785751343\n",
      "batch: 14149, loss: 0.15171213448047638\n",
      "batch: 14150, loss: 0.10979383438825607\n",
      "batch: 14151, loss: 0.1479226052761078\n",
      "batch: 14152, loss: 0.09195457398891449\n",
      "batch: 14153, loss: 0.08831730484962463\n",
      "batch: 14154, loss: 0.08318852633237839\n",
      "batch: 14155, loss: 0.1133921816945076\n",
      "batch: 14156, loss: 0.10590167343616486\n",
      "batch: 14157, loss: 0.1756460815668106\n",
      "batch: 14158, loss: 0.04602276533842087\n",
      "batch: 14159, loss: 0.14093101024627686\n",
      "batch: 14160, loss: 0.15662357211112976\n",
      "batch: 14161, loss: 0.08674806356430054\n",
      "batch: 14162, loss: 0.1064479798078537\n",
      "batch: 14163, loss: 0.12854400277137756\n",
      "batch: 14164, loss: 0.06294336169958115\n",
      "batch: 14165, loss: 0.053121648728847504\n",
      "batch: 14166, loss: 0.20939257740974426\n",
      "batch: 14167, loss: 0.04462359473109245\n",
      "batch: 14168, loss: 0.09898722171783447\n",
      "batch: 14169, loss: 0.06259560585021973\n",
      "batch: 14170, loss: 0.05711118131875992\n",
      "batch: 14171, loss: 0.09407007694244385\n",
      "batch: 14172, loss: 0.07213981449604034\n",
      "batch: 14173, loss: 0.1020226776599884\n",
      "batch: 14174, loss: 0.1051359623670578\n",
      "batch: 14175, loss: 0.08546534925699234\n",
      "batch: 14176, loss: 0.2170582264661789\n",
      "batch: 14177, loss: 0.06891602277755737\n",
      "batch: 14178, loss: 0.036603622138500214\n",
      "batch: 14179, loss: 0.045471563935279846\n",
      "batch: 14180, loss: 0.0538606159389019\n",
      "batch: 14181, loss: 0.09887640923261642\n",
      "batch: 14182, loss: 0.019736364483833313\n",
      "batch: 14183, loss: 0.06614091247320175\n",
      "batch: 14184, loss: 0.1969001740217209\n",
      "batch: 14185, loss: 0.23809212446212769\n",
      "batch: 14186, loss: 0.10891597718000412\n",
      "batch: 14187, loss: 0.12253937125205994\n",
      "batch: 14188, loss: 0.14915762841701508\n",
      "batch: 14189, loss: 0.07121546566486359\n",
      "batch: 14190, loss: 0.1721164882183075\n",
      "batch: 14191, loss: 0.11377635598182678\n",
      "batch: 14192, loss: 0.18247830867767334\n",
      "batch: 14193, loss: 0.07803025841712952\n",
      "batch: 14194, loss: 0.10052700340747833\n",
      "batch: 14195, loss: 0.13509047031402588\n",
      "batch: 14196, loss: 0.06007957458496094\n",
      "batch: 14197, loss: 0.14930453896522522\n",
      "batch: 14198, loss: 0.05060569569468498\n",
      "batch: 14199, loss: 0.06612692028284073\n",
      "batch: 14200, loss: 0.06149512529373169\n",
      "model saved to ./saving/model.ckpt-142\n",
      "batch: 14201, loss: 0.1102372407913208\n",
      "batch: 14202, loss: 0.1498042494058609\n",
      "batch: 14203, loss: 0.0888981968164444\n",
      "batch: 14204, loss: 0.23796308040618896\n",
      "batch: 14205, loss: 0.12805476784706116\n",
      "batch: 14206, loss: 0.056297022849321365\n",
      "batch: 14207, loss: 0.09701387584209442\n",
      "batch: 14208, loss: 0.08504988998174667\n",
      "batch: 14209, loss: 0.03645165264606476\n",
      "batch: 14210, loss: 0.20978771150112152\n",
      "batch: 14211, loss: 0.06173288822174072\n",
      "batch: 14212, loss: 0.102183997631073\n",
      "batch: 14213, loss: 0.05817332863807678\n",
      "batch: 14214, loss: 0.0641978532075882\n",
      "batch: 14215, loss: 0.0908733531832695\n",
      "batch: 14216, loss: 0.032661959528923035\n",
      "batch: 14217, loss: 0.04581595957279205\n",
      "batch: 14218, loss: 0.09006033837795258\n",
      "batch: 14219, loss: 0.033622823655605316\n",
      "batch: 14220, loss: 0.12684938311576843\n",
      "batch: 14221, loss: 0.060977667570114136\n",
      "batch: 14222, loss: 0.10266296565532684\n",
      "batch: 14223, loss: 0.16408781707286835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 14224, loss: 0.052707355469465256\n",
      "batch: 14225, loss: 0.0296323262155056\n",
      "batch: 14226, loss: 0.08448877930641174\n",
      "batch: 14227, loss: 0.24603919684886932\n",
      "batch: 14228, loss: 0.11753712594509125\n",
      "batch: 14229, loss: 0.13520951569080353\n",
      "batch: 14230, loss: 0.18021318316459656\n",
      "batch: 14231, loss: 0.06371717900037766\n",
      "batch: 14232, loss: 0.07558814436197281\n",
      "batch: 14233, loss: 0.11752711236476898\n",
      "batch: 14234, loss: 0.12360601872205734\n",
      "batch: 14235, loss: 0.09421143680810928\n",
      "batch: 14236, loss: 0.08568695932626724\n",
      "batch: 14237, loss: 0.1546902060508728\n",
      "batch: 14238, loss: 0.08418163657188416\n",
      "batch: 14239, loss: 0.09170228987932205\n",
      "batch: 14240, loss: 0.08776462078094482\n",
      "batch: 14241, loss: 0.11770094186067581\n",
      "batch: 14242, loss: 0.12712633609771729\n",
      "batch: 14243, loss: 0.12520907819271088\n",
      "batch: 14244, loss: 0.11166118085384369\n",
      "batch: 14245, loss: 0.11974810808897018\n",
      "batch: 14246, loss: 0.12561005353927612\n",
      "batch: 14247, loss: 0.0881362184882164\n",
      "batch: 14248, loss: 0.08134778589010239\n",
      "batch: 14249, loss: 0.13793431222438812\n",
      "batch: 14250, loss: 0.05728535354137421\n",
      "batch: 14251, loss: 0.10978614538908005\n",
      "batch: 14252, loss: 0.061424411833286285\n",
      "batch: 14253, loss: 0.12523376941680908\n",
      "batch: 14254, loss: 0.11028813570737839\n",
      "batch: 14255, loss: 0.07959380745887756\n",
      "batch: 14256, loss: 0.19846028089523315\n",
      "batch: 14257, loss: 0.07467617094516754\n",
      "batch: 14258, loss: 0.0637558102607727\n",
      "batch: 14259, loss: 0.11714614927768707\n",
      "batch: 14260, loss: 0.1357133835554123\n",
      "batch: 14261, loss: 0.12946511805057526\n",
      "batch: 14262, loss: 0.037765298038721085\n",
      "batch: 14263, loss: 0.04576587304472923\n",
      "batch: 14264, loss: 0.05227113515138626\n",
      "batch: 14265, loss: 0.10818177461624146\n",
      "batch: 14266, loss: 0.19153691828250885\n",
      "batch: 14267, loss: 0.3684757649898529\n",
      "batch: 14268, loss: 0.05781438946723938\n",
      "batch: 14269, loss: 0.03829260170459747\n",
      "batch: 14270, loss: 0.09295117110013962\n",
      "batch: 14271, loss: 0.23757712543010712\n",
      "batch: 14272, loss: 0.1440565288066864\n",
      "batch: 14273, loss: 0.1312864124774933\n",
      "batch: 14274, loss: 0.0870627835392952\n",
      "batch: 14275, loss: 0.03502204641699791\n",
      "batch: 14276, loss: 0.11288284510374069\n",
      "batch: 14277, loss: 0.05271771550178528\n",
      "batch: 14278, loss: 0.16480955481529236\n",
      "batch: 14279, loss: 0.12239686399698257\n",
      "batch: 14280, loss: 0.04181881248950958\n",
      "batch: 14281, loss: 0.12461011856794357\n",
      "batch: 14282, loss: 0.08874987065792084\n",
      "batch: 14283, loss: 0.06117641553282738\n",
      "batch: 14284, loss: 0.11768750846385956\n",
      "batch: 14285, loss: 0.11820101737976074\n",
      "batch: 14286, loss: 0.06640123575925827\n",
      "batch: 14287, loss: 0.08231516927480698\n",
      "batch: 14288, loss: 0.19079871475696564\n",
      "batch: 14289, loss: 0.04087566211819649\n",
      "batch: 14290, loss: 0.0836997926235199\n",
      "batch: 14291, loss: 0.07741142809391022\n",
      "batch: 14292, loss: 0.10202817618846893\n",
      "batch: 14293, loss: 0.07704213261604309\n",
      "batch: 14294, loss: 0.096930593252182\n",
      "batch: 14295, loss: 0.03794319927692413\n",
      "batch: 14296, loss: 0.10119646787643433\n",
      "batch: 14297, loss: 0.17485769093036652\n",
      "batch: 14298, loss: 0.0753859207034111\n",
      "batch: 14299, loss: 0.20791444182395935\n",
      "batch: 14300, loss: 0.09247833490371704\n",
      "model saved to ./saving/model.ckpt-143\n",
      "batch: 14301, loss: 0.13668152689933777\n",
      "batch: 14302, loss: 0.22261404991149902\n",
      "batch: 14303, loss: 0.1478791981935501\n",
      "batch: 14304, loss: 0.05688156560063362\n",
      "batch: 14305, loss: 0.05810190364718437\n",
      "batch: 14306, loss: 0.13546279072761536\n",
      "batch: 14307, loss: 0.10989465564489365\n",
      "batch: 14308, loss: 0.12687915563583374\n",
      "batch: 14309, loss: 0.05172833055257797\n",
      "batch: 14310, loss: 0.09862600266933441\n",
      "batch: 14311, loss: 0.10032802075147629\n",
      "batch: 14312, loss: 0.06935076415538788\n",
      "batch: 14313, loss: 0.09002071619033813\n",
      "batch: 14314, loss: 0.15433096885681152\n",
      "batch: 14315, loss: 0.06930973380804062\n",
      "batch: 14316, loss: 0.1788347065448761\n",
      "batch: 14317, loss: 0.04723639041185379\n",
      "batch: 14318, loss: 0.12853392958641052\n",
      "batch: 14319, loss: 0.18495160341262817\n",
      "batch: 14320, loss: 0.34605059027671814\n",
      "batch: 14321, loss: 0.13520491123199463\n",
      "batch: 14322, loss: 0.09065432101488113\n",
      "batch: 14323, loss: 0.04511867091059685\n",
      "batch: 14324, loss: 0.06963151693344116\n",
      "batch: 14325, loss: 0.13866794109344482\n",
      "batch: 14326, loss: 0.07798761129379272\n",
      "batch: 14327, loss: 0.052711568772792816\n",
      "batch: 14328, loss: 0.20685352385044098\n",
      "batch: 14329, loss: 0.11416179686784744\n",
      "batch: 14330, loss: 0.10645158588886261\n",
      "batch: 14331, loss: 0.12717056274414062\n",
      "batch: 14332, loss: 0.1152694895863533\n",
      "batch: 14333, loss: 0.05547112599015236\n",
      "batch: 14334, loss: 0.2661934494972229\n",
      "batch: 14335, loss: 0.0832681879401207\n",
      "batch: 14336, loss: 0.051712874323129654\n",
      "batch: 14337, loss: 0.2006252110004425\n",
      "batch: 14338, loss: 0.10491321980953217\n",
      "batch: 14339, loss: 0.0805680900812149\n",
      "batch: 14340, loss: 0.14410635828971863\n",
      "batch: 14341, loss: 0.11366839706897736\n",
      "batch: 14342, loss: 0.051870208233594894\n",
      "batch: 14343, loss: 0.15273526310920715\n",
      "batch: 14344, loss: 0.2105010598897934\n",
      "batch: 14345, loss: 0.06271965801715851\n",
      "batch: 14346, loss: 0.07691055536270142\n",
      "batch: 14347, loss: 0.09537765383720398\n",
      "batch: 14348, loss: 0.035726286470890045\n",
      "batch: 14349, loss: 0.1394188404083252\n",
      "batch: 14350, loss: 0.15318255126476288\n",
      "batch: 14351, loss: 0.06675796955823898\n",
      "batch: 14352, loss: 0.1423184871673584\n",
      "batch: 14353, loss: 0.02476499229669571\n",
      "batch: 14354, loss: 0.09542173147201538\n",
      "batch: 14355, loss: 0.16773954033851624\n",
      "batch: 14356, loss: 0.11787848174571991\n",
      "batch: 14357, loss: 0.10295094549655914\n",
      "batch: 14358, loss: 0.19942674040794373\n",
      "batch: 14359, loss: 0.10866294056177139\n",
      "batch: 14360, loss: 0.02259097248315811\n",
      "batch: 14361, loss: 0.05596765875816345\n",
      "batch: 14362, loss: 0.10843989253044128\n",
      "batch: 14363, loss: 0.03695368766784668\n",
      "batch: 14364, loss: 0.07296404242515564\n",
      "batch: 14365, loss: 0.15133719146251678\n",
      "batch: 14366, loss: 0.128290057182312\n",
      "batch: 14367, loss: 0.09330002963542938\n",
      "batch: 14368, loss: 0.10246013104915619\n",
      "batch: 14369, loss: 0.09853075444698334\n",
      "batch: 14370, loss: 0.07179245352745056\n",
      "batch: 14371, loss: 0.12351703643798828\n",
      "batch: 14372, loss: 0.05611756816506386\n",
      "batch: 14373, loss: 0.11122266203165054\n",
      "batch: 14374, loss: 0.13617607951164246\n",
      "batch: 14375, loss: 0.10220769047737122\n",
      "batch: 14376, loss: 0.06180315092206001\n",
      "batch: 14377, loss: 0.1929616630077362\n",
      "batch: 14378, loss: 0.06548197567462921\n",
      "batch: 14379, loss: 0.13128654658794403\n",
      "batch: 14380, loss: 0.07053527235984802\n",
      "batch: 14381, loss: 0.12146678566932678\n",
      "batch: 14382, loss: 0.11162932217121124\n",
      "batch: 14383, loss: 0.09233218431472778\n",
      "batch: 14384, loss: 0.11058364808559418\n",
      "batch: 14385, loss: 0.0711679458618164\n",
      "batch: 14386, loss: 0.13759154081344604\n",
      "batch: 14387, loss: 0.20247116684913635\n",
      "batch: 14388, loss: 0.06812471151351929\n",
      "batch: 14389, loss: 0.11090309917926788\n",
      "batch: 14390, loss: 0.1550731360912323\n",
      "batch: 14391, loss: 0.10691075772047043\n",
      "batch: 14392, loss: 0.1509210467338562\n",
      "batch: 14393, loss: 0.10073114931583405\n",
      "batch: 14394, loss: 0.14683696627616882\n",
      "batch: 14395, loss: 0.11669866740703583\n",
      "batch: 14396, loss: 0.16510967910289764\n",
      "batch: 14397, loss: 0.32679638266563416\n",
      "batch: 14398, loss: 0.13021382689476013\n",
      "batch: 14399, loss: 0.06082439795136452\n",
      "batch: 14400, loss: 0.17068973183631897\n",
      "model saved to ./saving/model.ckpt-144\n",
      "batch: 14401, loss: 0.10367006063461304\n",
      "batch: 14402, loss: 0.09816372394561768\n",
      "batch: 14403, loss: 0.15429949760437012\n",
      "batch: 14404, loss: 0.0728548914194107\n",
      "batch: 14405, loss: 0.06324148178100586\n",
      "batch: 14406, loss: 0.0621253065764904\n",
      "batch: 14407, loss: 0.1138220727443695\n",
      "batch: 14408, loss: 0.15644580125808716\n",
      "batch: 14409, loss: 0.027490366250276566\n",
      "batch: 14410, loss: 0.10985248535871506\n",
      "batch: 14411, loss: 0.08737976849079132\n",
      "batch: 14412, loss: 0.06030482053756714\n",
      "batch: 14413, loss: 0.0760391503572464\n",
      "batch: 14414, loss: 0.13293983042240143\n",
      "batch: 14415, loss: 0.15259766578674316\n",
      "batch: 14416, loss: 0.14566227793693542\n",
      "batch: 14417, loss: 0.07099714130163193\n",
      "batch: 14418, loss: 0.06368765980005264\n",
      "batch: 14419, loss: 0.06703908741474152\n",
      "batch: 14420, loss: 0.09197785705327988\n",
      "batch: 14421, loss: 0.0815778523683548\n",
      "batch: 14422, loss: 0.045541707426309586\n",
      "batch: 14423, loss: 0.10635273158550262\n",
      "batch: 14424, loss: 0.0869055986404419\n",
      "batch: 14425, loss: 0.06995255500078201\n",
      "batch: 14426, loss: 0.03735337406396866\n",
      "batch: 14427, loss: 0.07103866338729858\n",
      "batch: 14428, loss: 0.037455543875694275\n",
      "batch: 14429, loss: 0.07648167014122009\n",
      "batch: 14430, loss: 0.11891968548297882\n",
      "batch: 14431, loss: 0.06172364950180054\n",
      "batch: 14432, loss: 0.18470756709575653\n",
      "batch: 14433, loss: 0.07208885252475739\n",
      "batch: 14434, loss: 0.10397981852293015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 14435, loss: 0.07828298211097717\n",
      "batch: 14436, loss: 0.06608482450246811\n",
      "batch: 14437, loss: 0.18733006715774536\n",
      "batch: 14438, loss: 0.09287112206220627\n",
      "batch: 14439, loss: 0.0462481826543808\n",
      "batch: 14440, loss: 0.034736402332782745\n",
      "batch: 14441, loss: 0.10619688034057617\n",
      "batch: 14442, loss: 0.11834614723920822\n",
      "batch: 14443, loss: 0.0738210678100586\n",
      "batch: 14444, loss: 0.07750527560710907\n",
      "batch: 14445, loss: 0.20448487997055054\n",
      "batch: 14446, loss: 0.07659830152988434\n",
      "batch: 14447, loss: 0.2863156199455261\n",
      "batch: 14448, loss: 0.06552494317293167\n",
      "batch: 14449, loss: 0.0557333342730999\n",
      "batch: 14450, loss: 0.05694803595542908\n",
      "batch: 14451, loss: 0.1428217589855194\n",
      "batch: 14452, loss: 0.09314113855361938\n",
      "batch: 14453, loss: 0.1972872018814087\n",
      "batch: 14454, loss: 0.07374516129493713\n",
      "batch: 14455, loss: 0.10836508870124817\n",
      "batch: 14456, loss: 0.198011115193367\n",
      "batch: 14457, loss: 0.21393641829490662\n",
      "batch: 14458, loss: 0.16450592875480652\n",
      "batch: 14459, loss: 0.10216191411018372\n",
      "batch: 14460, loss: 0.07383497059345245\n",
      "batch: 14461, loss: 0.10809731483459473\n",
      "batch: 14462, loss: 0.05966709926724434\n",
      "batch: 14463, loss: 0.07299480587244034\n",
      "batch: 14464, loss: 0.07532548159360886\n",
      "batch: 14465, loss: 0.09552229940891266\n",
      "batch: 14466, loss: 0.09544077515602112\n",
      "batch: 14467, loss: 0.15276291966438293\n",
      "batch: 14468, loss: 0.07767631858587265\n",
      "batch: 14469, loss: 0.20921680331230164\n",
      "batch: 14470, loss: 0.24345362186431885\n",
      "batch: 14471, loss: 0.29119873046875\n",
      "batch: 14472, loss: 0.24911263585090637\n",
      "batch: 14473, loss: 0.04783026874065399\n",
      "batch: 14474, loss: 0.16497552394866943\n",
      "batch: 14475, loss: 0.063810333609581\n",
      "batch: 14476, loss: 0.10386894643306732\n",
      "batch: 14477, loss: 0.0359591469168663\n",
      "batch: 14478, loss: 0.09984370321035385\n",
      "batch: 14479, loss: 0.3871490955352783\n",
      "batch: 14480, loss: 0.14406320452690125\n",
      "batch: 14481, loss: 0.07824742794036865\n",
      "batch: 14482, loss: 0.11528115719556808\n",
      "batch: 14483, loss: 0.06148165464401245\n",
      "batch: 14484, loss: 0.048287272453308105\n",
      "batch: 14485, loss: 0.1848185956478119\n",
      "batch: 14486, loss: 0.11724240332841873\n",
      "batch: 14487, loss: 0.06949863582849503\n",
      "batch: 14488, loss: 0.09688478708267212\n",
      "batch: 14489, loss: 0.0827813670039177\n",
      "batch: 14490, loss: 0.08553522825241089\n",
      "batch: 14491, loss: 0.12092724442481995\n",
      "batch: 14492, loss: 0.02668960951268673\n",
      "batch: 14493, loss: 0.0817033052444458\n",
      "batch: 14494, loss: 0.08919978141784668\n",
      "batch: 14495, loss: 0.07337554544210434\n",
      "batch: 14496, loss: 0.15459981560707092\n",
      "batch: 14497, loss: 0.03749324381351471\n",
      "batch: 14498, loss: 0.1280767321586609\n",
      "batch: 14499, loss: 0.07351525127887726\n",
      "batch: 14500, loss: 0.06319154053926468\n",
      "model saved to ./saving/model.ckpt-145\n",
      "batch: 14501, loss: 0.06730850785970688\n",
      "batch: 14502, loss: 0.12205983698368073\n",
      "batch: 14503, loss: 0.030485298484563828\n",
      "batch: 14504, loss: 0.06287433207035065\n",
      "batch: 14505, loss: 0.07284056395292282\n",
      "batch: 14506, loss: 0.06754160672426224\n",
      "batch: 14507, loss: 0.29403117299079895\n",
      "batch: 14508, loss: 0.07832323759794235\n",
      "batch: 14509, loss: 0.2997976541519165\n",
      "batch: 14510, loss: 0.040251441299915314\n",
      "batch: 14511, loss: 0.08551933616399765\n",
      "batch: 14512, loss: 0.05700770393013954\n",
      "batch: 14513, loss: 0.11715810000896454\n",
      "batch: 14514, loss: 0.08625546097755432\n",
      "batch: 14515, loss: 0.07955124974250793\n",
      "batch: 14516, loss: 0.0545467808842659\n",
      "batch: 14517, loss: 0.11114318668842316\n",
      "batch: 14518, loss: 0.1125311553478241\n",
      "batch: 14519, loss: 0.1492680460214615\n",
      "batch: 14520, loss: 0.17045161128044128\n",
      "batch: 14521, loss: 0.03728723153471947\n",
      "batch: 14522, loss: 0.10127332806587219\n",
      "batch: 14523, loss: 0.031201861798763275\n",
      "batch: 14524, loss: 0.15010777115821838\n",
      "batch: 14525, loss: 0.13361625373363495\n",
      "batch: 14526, loss: 0.04540544003248215\n",
      "batch: 14527, loss: 0.12103871256113052\n",
      "batch: 14528, loss: 0.08877681940793991\n",
      "batch: 14529, loss: 0.06317456811666489\n",
      "batch: 14530, loss: 0.1312578320503235\n",
      "batch: 14531, loss: 0.16927959024906158\n",
      "batch: 14532, loss: 0.037402067333459854\n",
      "batch: 14533, loss: 0.14499804377555847\n",
      "batch: 14534, loss: 0.29285651445388794\n",
      "batch: 14535, loss: 0.13360801339149475\n",
      "batch: 14536, loss: 0.0586906373500824\n",
      "batch: 14537, loss: 0.09846454113721848\n",
      "batch: 14538, loss: 0.1303117275238037\n",
      "batch: 14539, loss: 0.11670698970556259\n",
      "batch: 14540, loss: 0.05424140393733978\n",
      "batch: 14541, loss: 0.21267908811569214\n",
      "batch: 14542, loss: 0.04125466197729111\n",
      "batch: 14543, loss: 0.07347367703914642\n",
      "batch: 14544, loss: 0.20581334829330444\n",
      "batch: 14545, loss: 0.08666576445102692\n",
      "batch: 14546, loss: 0.08942879736423492\n",
      "batch: 14547, loss: 0.07831990718841553\n",
      "batch: 14548, loss: 0.14644905924797058\n",
      "batch: 14549, loss: 0.04977510869503021\n",
      "batch: 14550, loss: 0.13916075229644775\n",
      "batch: 14551, loss: 0.12414122372865677\n",
      "batch: 14552, loss: 0.11107002943754196\n",
      "batch: 14553, loss: 0.17434313893318176\n",
      "batch: 14554, loss: 0.07253953814506531\n",
      "batch: 14555, loss: 0.055173587054014206\n",
      "batch: 14556, loss: 0.12577299773693085\n",
      "batch: 14557, loss: 0.13865555822849274\n",
      "batch: 14558, loss: 0.0748620331287384\n",
      "batch: 14559, loss: 0.08960415422916412\n",
      "batch: 14560, loss: 0.19127216935157776\n",
      "batch: 14561, loss: 0.06976064294576645\n",
      "batch: 14562, loss: 0.1538214534521103\n",
      "batch: 14563, loss: 0.24934601783752441\n",
      "batch: 14564, loss: 0.111073337495327\n",
      "batch: 14565, loss: 0.12034855782985687\n",
      "batch: 14566, loss: 0.09300021082162857\n",
      "batch: 14567, loss: 0.11677414923906326\n",
      "batch: 14568, loss: 0.14608940482139587\n",
      "batch: 14569, loss: 0.15305952727794647\n",
      "batch: 14570, loss: 0.12384219467639923\n",
      "batch: 14571, loss: 0.11242389678955078\n",
      "batch: 14572, loss: 0.1343638300895691\n",
      "batch: 14573, loss: 0.04252823442220688\n",
      "batch: 14574, loss: 0.10644400864839554\n",
      "batch: 14575, loss: 0.26230090856552124\n",
      "batch: 14576, loss: 0.10650089383125305\n",
      "batch: 14577, loss: 0.14904077351093292\n",
      "batch: 14578, loss: 0.1118631511926651\n",
      "batch: 14579, loss: 0.09563429653644562\n",
      "batch: 14580, loss: 0.13604098558425903\n",
      "batch: 14581, loss: 0.11653411388397217\n",
      "batch: 14582, loss: 0.14820405840873718\n",
      "batch: 14583, loss: 0.09557058662176132\n",
      "batch: 14584, loss: 0.13931308686733246\n",
      "batch: 14585, loss: 0.2115909457206726\n",
      "batch: 14586, loss: 0.17238324880599976\n",
      "batch: 14587, loss: 0.05953717976808548\n",
      "batch: 14588, loss: 0.07546992599964142\n",
      "batch: 14589, loss: 0.10953838378190994\n",
      "batch: 14590, loss: 0.14550697803497314\n",
      "batch: 14591, loss: 0.14228299260139465\n",
      "batch: 14592, loss: 0.0686134546995163\n",
      "batch: 14593, loss: 0.0854538232088089\n",
      "batch: 14594, loss: 0.26657789945602417\n",
      "batch: 14595, loss: 0.09972269088029861\n",
      "batch: 14596, loss: 0.04841979593038559\n",
      "batch: 14597, loss: 0.06993862241506577\n",
      "batch: 14598, loss: 0.08951342105865479\n",
      "batch: 14599, loss: 0.10560593754053116\n",
      "batch: 14600, loss: 0.25621384382247925\n",
      "model saved to ./saving/model.ckpt-146\n",
      "batch: 14601, loss: 0.19526685774326324\n",
      "batch: 14602, loss: 0.10043313354253769\n",
      "batch: 14603, loss: 0.14494001865386963\n",
      "batch: 14604, loss: 0.16192759573459625\n",
      "batch: 14605, loss: 0.08925600349903107\n",
      "batch: 14606, loss: 0.1141432374715805\n",
      "batch: 14607, loss: 0.1479034423828125\n",
      "batch: 14608, loss: 0.0803658738732338\n",
      "batch: 14609, loss: 0.1301375925540924\n",
      "batch: 14610, loss: 0.09119999408721924\n",
      "batch: 14611, loss: 0.05331368371844292\n",
      "batch: 14612, loss: 0.05694648623466492\n",
      "batch: 14613, loss: 0.08514688163995743\n",
      "batch: 14614, loss: 0.14099159836769104\n",
      "batch: 14615, loss: 0.0351690798997879\n",
      "batch: 14616, loss: 0.07857351750135422\n",
      "batch: 14617, loss: 0.21554699540138245\n",
      "batch: 14618, loss: 0.2235368937253952\n",
      "batch: 14619, loss: 0.2174009531736374\n",
      "batch: 14620, loss: 0.0702529177069664\n",
      "batch: 14621, loss: 0.13055112957954407\n",
      "batch: 14622, loss: 0.08629948645830154\n",
      "batch: 14623, loss: 0.13523897528648376\n",
      "batch: 14624, loss: 0.08355457335710526\n",
      "batch: 14625, loss: 0.18136507272720337\n",
      "batch: 14626, loss: 0.04592834413051605\n",
      "batch: 14627, loss: 0.2676129639148712\n",
      "batch: 14628, loss: 0.0889677107334137\n",
      "batch: 14629, loss: 0.0721379742026329\n",
      "batch: 14630, loss: 0.17381587624549866\n",
      "batch: 14631, loss: 0.07500304281711578\n",
      "batch: 14632, loss: 0.11494462192058563\n",
      "batch: 14633, loss: 0.2157593071460724\n",
      "batch: 14634, loss: 0.0547407791018486\n",
      "batch: 14635, loss: 0.13466572761535645\n",
      "batch: 14636, loss: 0.13689449429512024\n",
      "batch: 14637, loss: 0.16495230793952942\n",
      "batch: 14638, loss: 0.1388261616230011\n",
      "batch: 14639, loss: 0.04651012644171715\n",
      "batch: 14640, loss: 0.11675113439559937\n",
      "batch: 14641, loss: 0.12015250325202942\n",
      "batch: 14642, loss: 0.10468555986881256\n",
      "batch: 14643, loss: 0.05734000355005264\n",
      "batch: 14644, loss: 0.1694554090499878\n",
      "batch: 14645, loss: 0.027900876477360725\n",
      "batch: 14646, loss: 0.09250388294458389\n",
      "batch: 14647, loss: 0.0864793211221695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 14648, loss: 0.16737565398216248\n",
      "batch: 14649, loss: 0.12899255752563477\n",
      "batch: 14650, loss: 0.04635578393936157\n",
      "batch: 14651, loss: 0.05749725177884102\n",
      "batch: 14652, loss: 0.052422359585762024\n",
      "batch: 14653, loss: 0.060978058725595474\n",
      "batch: 14654, loss: 0.0451931394636631\n",
      "batch: 14655, loss: 0.024635162204504013\n",
      "batch: 14656, loss: 0.21357831358909607\n",
      "batch: 14657, loss: 0.0479266420006752\n",
      "batch: 14658, loss: 0.16617685556411743\n",
      "batch: 14659, loss: 0.19932174682617188\n",
      "batch: 14660, loss: 0.0817042738199234\n",
      "batch: 14661, loss: 0.036194488406181335\n",
      "batch: 14662, loss: 0.07517470419406891\n",
      "batch: 14663, loss: 0.07974684238433838\n",
      "batch: 14664, loss: 0.0925162062048912\n",
      "batch: 14665, loss: 0.07861645519733429\n",
      "batch: 14666, loss: 0.11677302420139313\n",
      "batch: 14667, loss: 0.23416855931282043\n",
      "batch: 14668, loss: 0.1651327759027481\n",
      "batch: 14669, loss: 0.08306805789470673\n",
      "batch: 14670, loss: 0.10503167659044266\n",
      "batch: 14671, loss: 0.11621391773223877\n",
      "batch: 14672, loss: 0.09402496367692947\n",
      "batch: 14673, loss: 0.08685123920440674\n",
      "batch: 14674, loss: 0.10075502097606659\n",
      "batch: 14675, loss: 0.12977713346481323\n",
      "batch: 14676, loss: 0.20365990698337555\n",
      "batch: 14677, loss: 0.13529148697853088\n",
      "batch: 14678, loss: 0.12166023254394531\n",
      "batch: 14679, loss: 0.0891139954328537\n",
      "batch: 14680, loss: 0.06776545941829681\n",
      "batch: 14681, loss: 0.0422978401184082\n",
      "batch: 14682, loss: 0.0471150279045105\n",
      "batch: 14683, loss: 0.04743700847029686\n",
      "batch: 14684, loss: 0.0775340348482132\n",
      "batch: 14685, loss: 0.047237932682037354\n",
      "batch: 14686, loss: 0.1107410117983818\n",
      "batch: 14687, loss: 0.16104833781719208\n",
      "batch: 14688, loss: 0.10323827713727951\n",
      "batch: 14689, loss: 0.10432710498571396\n",
      "batch: 14690, loss: 0.06394445151090622\n",
      "batch: 14691, loss: 0.12664896249771118\n",
      "batch: 14692, loss: 0.0725623294711113\n",
      "batch: 14693, loss: 0.09792795777320862\n",
      "batch: 14694, loss: 0.09119316935539246\n",
      "batch: 14695, loss: 0.2382349967956543\n",
      "batch: 14696, loss: 0.11837984621524811\n",
      "batch: 14697, loss: 0.34190961718559265\n",
      "batch: 14698, loss: 0.10916831344366074\n",
      "batch: 14699, loss: 0.1157083809375763\n",
      "batch: 14700, loss: 0.1181403324007988\n",
      "model saved to ./saving/model.ckpt-147\n",
      "batch: 14701, loss: 0.09482232481241226\n",
      "batch: 14702, loss: 0.039607927203178406\n",
      "batch: 14703, loss: 0.11035919934511185\n",
      "batch: 14704, loss: 0.09103953093290329\n",
      "batch: 14705, loss: 0.13612987101078033\n",
      "batch: 14706, loss: 0.1480826586484909\n",
      "batch: 14707, loss: 0.1945769190788269\n",
      "batch: 14708, loss: 0.06828854233026505\n",
      "batch: 14709, loss: 0.21291127800941467\n",
      "batch: 14710, loss: 0.07387610524892807\n",
      "batch: 14711, loss: 0.12004344165325165\n",
      "batch: 14712, loss: 0.09754584729671478\n",
      "batch: 14713, loss: 0.04325956106185913\n",
      "batch: 14714, loss: 0.1827850341796875\n",
      "batch: 14715, loss: 0.2217073142528534\n",
      "batch: 14716, loss: 0.10266393423080444\n",
      "batch: 14717, loss: 0.1410476267337799\n",
      "batch: 14718, loss: 0.06578280031681061\n",
      "batch: 14719, loss: 0.12254897505044937\n",
      "batch: 14720, loss: 0.12637746334075928\n",
      "batch: 14721, loss: 0.10386060178279877\n",
      "batch: 14722, loss: 0.0750090628862381\n",
      "batch: 14723, loss: 0.07475025206804276\n",
      "batch: 14724, loss: 0.11338205635547638\n",
      "batch: 14725, loss: 0.09380917251110077\n",
      "batch: 14726, loss: 0.13581494987010956\n",
      "batch: 14727, loss: 0.10063007473945618\n",
      "batch: 14728, loss: 0.05220084637403488\n",
      "batch: 14729, loss: 0.04202882945537567\n",
      "batch: 14730, loss: 0.11277115345001221\n",
      "batch: 14731, loss: 0.10821305215358734\n",
      "batch: 14732, loss: 0.16926220059394836\n",
      "batch: 14733, loss: 0.10903199762105942\n",
      "batch: 14734, loss: 0.17036396265029907\n",
      "batch: 14735, loss: 0.13491013646125793\n",
      "batch: 14736, loss: 0.12546567618846893\n",
      "batch: 14737, loss: 0.22134485840797424\n",
      "batch: 14738, loss: 0.09911808371543884\n",
      "batch: 14739, loss: 0.1416335552930832\n",
      "batch: 14740, loss: 0.11508387327194214\n",
      "batch: 14741, loss: 0.05788712203502655\n",
      "batch: 14742, loss: 0.12778297066688538\n",
      "batch: 14743, loss: 0.1908239871263504\n",
      "batch: 14744, loss: 0.038281865417957306\n",
      "batch: 14745, loss: 0.062366701662540436\n",
      "batch: 14746, loss: 0.08361990749835968\n",
      "batch: 14747, loss: 0.11932934820652008\n",
      "batch: 14748, loss: 0.09009666740894318\n",
      "batch: 14749, loss: 0.15320849418640137\n",
      "batch: 14750, loss: 0.11811178922653198\n",
      "batch: 14751, loss: 0.1463238000869751\n",
      "batch: 14752, loss: 0.22417299449443817\n",
      "batch: 14753, loss: 0.08794979751110077\n",
      "batch: 14754, loss: 0.058571625500917435\n",
      "batch: 14755, loss: 0.12559369206428528\n",
      "batch: 14756, loss: 0.1654892861843109\n",
      "batch: 14757, loss: 0.05517439544200897\n",
      "batch: 14758, loss: 0.18476958572864532\n",
      "batch: 14759, loss: 0.1691017597913742\n",
      "batch: 14760, loss: 0.06617124378681183\n",
      "batch: 14761, loss: 0.061677515506744385\n",
      "batch: 14762, loss: 0.1471409797668457\n",
      "batch: 14763, loss: 0.08267930150032043\n",
      "batch: 14764, loss: 0.14672517776489258\n",
      "batch: 14765, loss: 0.12721458077430725\n",
      "batch: 14766, loss: 0.10792689770460129\n",
      "batch: 14767, loss: 0.09094595909118652\n",
      "batch: 14768, loss: 0.23454032838344574\n",
      "batch: 14769, loss: 0.19364404678344727\n",
      "batch: 14770, loss: 0.07410682737827301\n",
      "batch: 14771, loss: 0.11282600462436676\n",
      "batch: 14772, loss: 0.1605750024318695\n",
      "batch: 14773, loss: 0.10811939835548401\n",
      "batch: 14774, loss: 0.17032110691070557\n",
      "batch: 14775, loss: 0.18494239449501038\n",
      "batch: 14776, loss: 0.04483458772301674\n",
      "batch: 14777, loss: 0.08993776887655258\n",
      "batch: 14778, loss: 0.2766283452510834\n",
      "batch: 14779, loss: 0.15308381617069244\n",
      "batch: 14780, loss: 0.08589252829551697\n",
      "batch: 14781, loss: 0.07115455716848373\n",
      "batch: 14782, loss: 0.0689045861363411\n",
      "batch: 14783, loss: 0.13356588780879974\n",
      "batch: 14784, loss: 0.15467995405197144\n",
      "batch: 14785, loss: 0.12204281985759735\n",
      "batch: 14786, loss: 0.19565260410308838\n",
      "batch: 14787, loss: 0.129583477973938\n",
      "batch: 14788, loss: 0.10470767319202423\n",
      "batch: 14789, loss: 0.14710071682929993\n",
      "batch: 14790, loss: 0.10872983932495117\n",
      "batch: 14791, loss: 0.15897101163864136\n",
      "batch: 14792, loss: 0.2004450410604477\n",
      "batch: 14793, loss: 0.059946633875370026\n",
      "batch: 14794, loss: 0.14366021752357483\n",
      "batch: 14795, loss: 0.1764494776725769\n",
      "batch: 14796, loss: 0.03916719928383827\n",
      "batch: 14797, loss: 0.05677416920661926\n",
      "batch: 14798, loss: 0.07351669669151306\n",
      "batch: 14799, loss: 0.1583496481180191\n",
      "batch: 14800, loss: 0.17338308691978455\n",
      "model saved to ./saving/model.ckpt-148\n",
      "batch: 14801, loss: 0.12742021679878235\n",
      "batch: 14802, loss: 0.034888315945863724\n",
      "batch: 14803, loss: 0.04337605834007263\n",
      "batch: 14804, loss: 0.09455940127372742\n",
      "batch: 14805, loss: 0.14757218956947327\n",
      "batch: 14806, loss: 0.08414094150066376\n",
      "batch: 14807, loss: 0.1703486293554306\n",
      "batch: 14808, loss: 0.04916010797023773\n",
      "batch: 14809, loss: 0.09651025384664536\n",
      "batch: 14810, loss: 0.0729818344116211\n",
      "batch: 14811, loss: 0.08253645151853561\n",
      "batch: 14812, loss: 0.06881585717201233\n",
      "batch: 14813, loss: 0.3396913409233093\n",
      "batch: 14814, loss: 0.08946457505226135\n",
      "batch: 14815, loss: 0.11705917119979858\n",
      "batch: 14816, loss: 0.3381659984588623\n",
      "batch: 14817, loss: 0.0921659916639328\n",
      "batch: 14818, loss: 0.1249571368098259\n",
      "batch: 14819, loss: 0.10495197772979736\n",
      "batch: 14820, loss: 0.0928962230682373\n",
      "batch: 14821, loss: 0.1660265177488327\n",
      "batch: 14822, loss: 0.10535493493080139\n",
      "batch: 14823, loss: 0.030901199206709862\n",
      "batch: 14824, loss: 0.1116645336151123\n",
      "batch: 14825, loss: 0.12609538435935974\n",
      "batch: 14826, loss: 0.09155067056417465\n",
      "batch: 14827, loss: 0.05358460173010826\n",
      "batch: 14828, loss: 0.055502355098724365\n",
      "batch: 14829, loss: 0.18289944529533386\n",
      "batch: 14830, loss: 0.02173497900366783\n",
      "batch: 14831, loss: 0.06642457842826843\n",
      "batch: 14832, loss: 0.1345166265964508\n",
      "batch: 14833, loss: 0.07972118258476257\n",
      "batch: 14834, loss: 0.26857292652130127\n",
      "batch: 14835, loss: 0.10828813910484314\n",
      "batch: 14836, loss: 0.07732024043798447\n",
      "batch: 14837, loss: 0.09313398599624634\n",
      "batch: 14838, loss: 0.11323338001966476\n",
      "batch: 14839, loss: 0.1530686765909195\n",
      "batch: 14840, loss: 0.10516097396612167\n",
      "batch: 14841, loss: 0.06139785796403885\n",
      "batch: 14842, loss: 0.18359534442424774\n",
      "batch: 14843, loss: 0.13986143469810486\n",
      "batch: 14844, loss: 0.20646686851978302\n",
      "batch: 14845, loss: 0.05514506250619888\n",
      "batch: 14846, loss: 0.164905846118927\n",
      "batch: 14847, loss: 0.05533338338136673\n",
      "batch: 14848, loss: 0.029381632804870605\n",
      "batch: 14849, loss: 0.12178146839141846\n",
      "batch: 14850, loss: 0.14333657920360565\n",
      "batch: 14851, loss: 0.1440221667289734\n",
      "batch: 14852, loss: 0.13645705580711365\n",
      "batch: 14853, loss: 0.11258724331855774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 14854, loss: 0.12413132935762405\n",
      "batch: 14855, loss: 0.09634754061698914\n",
      "batch: 14856, loss: 0.1422499120235443\n",
      "batch: 14857, loss: 0.0501084178686142\n",
      "batch: 14858, loss: 0.08466321974992752\n",
      "batch: 14859, loss: 0.11405888199806213\n",
      "batch: 14860, loss: 0.24285712838172913\n",
      "batch: 14861, loss: 0.0751403272151947\n",
      "batch: 14862, loss: 0.05488826334476471\n",
      "batch: 14863, loss: 0.05740167573094368\n",
      "batch: 14864, loss: 0.06037183478474617\n",
      "batch: 14865, loss: 0.061234328895807266\n",
      "batch: 14866, loss: 0.11327727138996124\n",
      "batch: 14867, loss: 0.1380167156457901\n",
      "batch: 14868, loss: 0.13276542723178864\n",
      "batch: 14869, loss: 0.06269897520542145\n",
      "batch: 14870, loss: 0.1232488602399826\n",
      "batch: 14871, loss: 0.12102171033620834\n",
      "batch: 14872, loss: 0.10560379177331924\n",
      "batch: 14873, loss: 0.15414658188819885\n",
      "batch: 14874, loss: 0.04145091772079468\n",
      "batch: 14875, loss: 0.1424787938594818\n",
      "batch: 14876, loss: 0.09350699931383133\n",
      "batch: 14877, loss: 0.12422453612089157\n",
      "batch: 14878, loss: 0.16017132997512817\n",
      "batch: 14879, loss: 0.12062225490808487\n",
      "batch: 14880, loss: 0.10439775884151459\n",
      "batch: 14881, loss: 0.06255242973566055\n",
      "batch: 14882, loss: 0.0755692794919014\n",
      "batch: 14883, loss: 0.1942015290260315\n",
      "batch: 14884, loss: 0.09774372726678848\n",
      "batch: 14885, loss: 0.23511162400245667\n",
      "batch: 14886, loss: 0.11423686146736145\n",
      "batch: 14887, loss: 0.03958690166473389\n",
      "batch: 14888, loss: 0.17002752423286438\n",
      "batch: 14889, loss: 0.3021412491798401\n",
      "batch: 14890, loss: 0.06836678087711334\n",
      "batch: 14891, loss: 0.09373478591442108\n",
      "batch: 14892, loss: 0.08239883929491043\n",
      "batch: 14893, loss: 0.05174582824110985\n",
      "batch: 14894, loss: 0.0745500922203064\n",
      "batch: 14895, loss: 0.11037064343690872\n",
      "batch: 14896, loss: 0.07877340912818909\n",
      "batch: 14897, loss: 0.04578695446252823\n",
      "batch: 14898, loss: 0.020871493965387344\n",
      "batch: 14899, loss: 0.09043420851230621\n",
      "batch: 14900, loss: 0.17624951899051666\n",
      "model saved to ./saving/model.ckpt-149\n",
      "batch: 14901, loss: 0.09767447412014008\n",
      "batch: 14902, loss: 0.07841484993696213\n",
      "batch: 14903, loss: 0.06370081752538681\n",
      "batch: 14904, loss: 0.08761642873287201\n",
      "batch: 14905, loss: 0.057493966072797775\n",
      "batch: 14906, loss: 0.08407352864742279\n",
      "batch: 14907, loss: 0.09042173624038696\n",
      "batch: 14908, loss: 0.1074458658695221\n",
      "batch: 14909, loss: 0.13468323647975922\n",
      "batch: 14910, loss: 0.17766261100769043\n",
      "batch: 14911, loss: 0.05397950857877731\n",
      "batch: 14912, loss: 0.0728422999382019\n",
      "batch: 14913, loss: 0.11030618846416473\n",
      "batch: 14914, loss: 0.23959855735301971\n",
      "batch: 14915, loss: 0.05023080110549927\n",
      "batch: 14916, loss: 0.04108543321490288\n",
      "batch: 14917, loss: 0.10939754545688629\n",
      "batch: 14918, loss: 0.11170460283756256\n",
      "batch: 14919, loss: 0.05781970173120499\n",
      "batch: 14920, loss: 0.0733402669429779\n",
      "batch: 14921, loss: 0.13610023260116577\n",
      "batch: 14922, loss: 0.1017378643155098\n",
      "batch: 14923, loss: 0.08007699251174927\n",
      "batch: 14924, loss: 0.08244702219963074\n",
      "batch: 14925, loss: 0.11824533343315125\n",
      "batch: 14926, loss: 0.14850950241088867\n",
      "batch: 14927, loss: 0.12786617875099182\n",
      "batch: 14928, loss: 0.05355574190616608\n",
      "batch: 14929, loss: 0.09920400381088257\n",
      "batch: 14930, loss: 0.1322973221540451\n",
      "batch: 14931, loss: 0.08379494398832321\n",
      "batch: 14932, loss: 0.19612962007522583\n",
      "batch: 14933, loss: 0.2431326061487198\n",
      "batch: 14934, loss: 0.07678598165512085\n",
      "batch: 14935, loss: 0.1680811643600464\n",
      "batch: 14936, loss: 0.1064474880695343\n",
      "batch: 14937, loss: 0.11274786293506622\n",
      "batch: 14938, loss: 0.08997292816638947\n",
      "batch: 14939, loss: 0.10868971049785614\n",
      "batch: 14940, loss: 0.1371154636144638\n",
      "batch: 14941, loss: 0.11520044505596161\n",
      "batch: 14942, loss: 0.04028787836432457\n",
      "batch: 14943, loss: 0.09500214457511902\n",
      "batch: 14944, loss: 0.08692590147256851\n",
      "batch: 14945, loss: 0.2064516246318817\n",
      "batch: 14946, loss: 0.09555618464946747\n",
      "batch: 14947, loss: 0.07702983170747757\n",
      "batch: 14948, loss: 0.16196677088737488\n",
      "batch: 14949, loss: 0.05445782095193863\n",
      "batch: 14950, loss: 0.24203218519687653\n",
      "batch: 14951, loss: 0.041359782218933105\n",
      "batch: 14952, loss: 0.059365708380937576\n",
      "batch: 14953, loss: 0.08800230920314789\n",
      "batch: 14954, loss: 0.09798923879861832\n",
      "batch: 14955, loss: 0.22401879727840424\n",
      "batch: 14956, loss: 0.18695706129074097\n",
      "batch: 14957, loss: 0.1402900665998459\n",
      "batch: 14958, loss: 0.08873115479946136\n",
      "batch: 14959, loss: 0.03148216754198074\n",
      "batch: 14960, loss: 0.017163101583719254\n",
      "batch: 14961, loss: 0.07347927987575531\n",
      "batch: 14962, loss: 0.12930241227149963\n",
      "batch: 14963, loss: 0.10455889999866486\n",
      "batch: 14964, loss: 0.11729902029037476\n",
      "batch: 14965, loss: 0.10324348509311676\n",
      "batch: 14966, loss: 0.07907702028751373\n",
      "batch: 14967, loss: 0.06452911347150803\n",
      "batch: 14968, loss: 0.0938165932893753\n",
      "batch: 14969, loss: 0.07186110317707062\n",
      "batch: 14970, loss: 0.19821348786354065\n",
      "batch: 14971, loss: 0.05536605417728424\n",
      "batch: 14972, loss: 0.0907154306769371\n",
      "batch: 14973, loss: 0.05200401693582535\n",
      "batch: 14974, loss: 0.06337949633598328\n",
      "batch: 14975, loss: 0.24836432933807373\n",
      "batch: 14976, loss: 0.1668996512889862\n",
      "batch: 14977, loss: 0.15012894570827484\n",
      "batch: 14978, loss: 0.05325109139084816\n",
      "batch: 14979, loss: 0.08824601769447327\n",
      "batch: 14980, loss: 0.11484924703836441\n",
      "batch: 14981, loss: 0.054402220994234085\n",
      "batch: 14982, loss: 0.04646125063300133\n",
      "batch: 14983, loss: 0.050463080406188965\n",
      "batch: 14984, loss: 0.07572656869888306\n",
      "batch: 14985, loss: 0.1008349061012268\n",
      "batch: 14986, loss: 0.12276667356491089\n",
      "batch: 14987, loss: 0.11200817674398422\n",
      "batch: 14988, loss: 0.07627710700035095\n",
      "batch: 14989, loss: 0.08116600662469864\n",
      "batch: 14990, loss: 0.04171418398618698\n",
      "batch: 14991, loss: 0.09657669067382812\n",
      "batch: 14992, loss: 0.07620029151439667\n",
      "batch: 14993, loss: 0.11501454561948776\n",
      "batch: 14994, loss: 0.16285476088523865\n",
      "batch: 14995, loss: 0.025822050869464874\n",
      "batch: 14996, loss: 0.08045534789562225\n",
      "batch: 14997, loss: 0.05018442124128342\n",
      "batch: 14998, loss: 0.11542725563049316\n",
      "batch: 14999, loss: 0.13969504833221436\n",
      "batch: 15000, loss: 0.08526722341775894\n",
      "model saved to ./saving/model.ckpt-150\n",
      "batch: 15001, loss: 0.13303270936012268\n",
      "batch: 15002, loss: 0.0696452260017395\n",
      "batch: 15003, loss: 0.019452501088380814\n",
      "batch: 15004, loss: 0.13122570514678955\n",
      "batch: 15005, loss: 0.05741168558597565\n",
      "batch: 15006, loss: 0.05967816710472107\n",
      "batch: 15007, loss: 0.18709656596183777\n",
      "batch: 15008, loss: 0.13127171993255615\n",
      "batch: 15009, loss: 0.039369307458400726\n",
      "batch: 15010, loss: 0.13970233500003815\n",
      "batch: 15011, loss: 0.1483481526374817\n",
      "batch: 15012, loss: 0.08513009548187256\n",
      "batch: 15013, loss: 0.044823456555604935\n",
      "batch: 15014, loss: 0.10092991590499878\n",
      "batch: 15015, loss: 0.08503110706806183\n",
      "batch: 15016, loss: 0.07327336072921753\n",
      "batch: 15017, loss: 0.24357543885707855\n",
      "batch: 15018, loss: 0.06214082986116409\n",
      "batch: 15019, loss: 0.05233943462371826\n",
      "batch: 15020, loss: 0.10055666416883469\n",
      "batch: 15021, loss: 0.0727977603673935\n",
      "batch: 15022, loss: 0.24923263490200043\n",
      "batch: 15023, loss: 0.06584294885396957\n",
      "batch: 15024, loss: 0.08527466654777527\n",
      "batch: 15025, loss: 0.1397942304611206\n",
      "batch: 15026, loss: 0.07609938085079193\n",
      "batch: 15027, loss: 0.13099998235702515\n",
      "batch: 15028, loss: 0.09487664699554443\n",
      "batch: 15029, loss: 0.13002237677574158\n",
      "batch: 15030, loss: 0.17154480516910553\n",
      "batch: 15031, loss: 0.05352230370044708\n",
      "batch: 15032, loss: 0.24901792407035828\n",
      "batch: 15033, loss: 0.07836871594190598\n",
      "batch: 15034, loss: 0.0899486243724823\n",
      "batch: 15035, loss: 0.12922704219818115\n",
      "batch: 15036, loss: 0.14628350734710693\n",
      "batch: 15037, loss: 0.16050291061401367\n",
      "batch: 15038, loss: 0.07863710820674896\n",
      "batch: 15039, loss: 0.049123235046863556\n",
      "batch: 15040, loss: 0.08078880608081818\n",
      "batch: 15041, loss: 0.0659884512424469\n",
      "batch: 15042, loss: 0.06425497680902481\n",
      "batch: 15043, loss: 0.05052398890256882\n",
      "batch: 15044, loss: 0.05231615528464317\n",
      "batch: 15045, loss: 0.09223540872335434\n",
      "batch: 15046, loss: 0.11430861055850983\n",
      "batch: 15047, loss: 0.17150604724884033\n",
      "batch: 15048, loss: 0.08564367890357971\n",
      "batch: 15049, loss: 0.11243975162506104\n",
      "batch: 15050, loss: 0.05257101729512215\n",
      "batch: 15051, loss: 0.18141019344329834\n",
      "batch: 15052, loss: 0.044441062957048416\n",
      "batch: 15053, loss: 0.053783953189849854\n",
      "batch: 15054, loss: 0.02502674236893654\n",
      "batch: 15055, loss: 0.08896581083536148\n",
      "batch: 15056, loss: 0.035525549203157425\n",
      "batch: 15057, loss: 0.05192701146006584\n",
      "batch: 15058, loss: 0.05668362230062485\n",
      "batch: 15059, loss: 0.19402320683002472\n",
      "batch: 15060, loss: 0.11299203336238861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 15061, loss: 0.04764028638601303\n",
      "batch: 15062, loss: 0.12529286742210388\n",
      "batch: 15063, loss: 0.09436365962028503\n",
      "batch: 15064, loss: 0.04852873831987381\n",
      "batch: 15065, loss: 0.06168033927679062\n",
      "batch: 15066, loss: 0.09347707033157349\n",
      "batch: 15067, loss: 0.0741814523935318\n",
      "batch: 15068, loss: 0.18150128424167633\n",
      "batch: 15069, loss: 0.11506225168704987\n",
      "batch: 15070, loss: 0.11433075368404388\n",
      "batch: 15071, loss: 0.04660137742757797\n",
      "batch: 15072, loss: 0.11094345152378082\n",
      "batch: 15073, loss: 0.19393722712993622\n",
      "batch: 15074, loss: 0.10659949481487274\n",
      "batch: 15075, loss: 0.1350523829460144\n",
      "batch: 15076, loss: 0.05493428558111191\n",
      "batch: 15077, loss: 0.156746968626976\n",
      "batch: 15078, loss: 0.0655604749917984\n",
      "batch: 15079, loss: 0.14865489304065704\n",
      "batch: 15080, loss: 0.09613031148910522\n",
      "batch: 15081, loss: 0.07874055206775665\n",
      "batch: 15082, loss: 0.06069022789597511\n",
      "batch: 15083, loss: 0.22128891944885254\n",
      "batch: 15084, loss: 0.0887269377708435\n",
      "batch: 15085, loss: 0.12269820272922516\n",
      "batch: 15086, loss: 0.09987837076187134\n",
      "batch: 15087, loss: 0.18448500335216522\n",
      "batch: 15088, loss: 0.06392532587051392\n",
      "batch: 15089, loss: 0.10833760350942612\n",
      "batch: 15090, loss: 0.08981384336948395\n",
      "batch: 15091, loss: 0.03892490267753601\n",
      "batch: 15092, loss: 0.06226300448179245\n",
      "batch: 15093, loss: 0.26293161511421204\n",
      "batch: 15094, loss: 0.08166706562042236\n",
      "batch: 15095, loss: 0.17914356291294098\n",
      "batch: 15096, loss: 0.13571806252002716\n",
      "batch: 15097, loss: 0.051536925137043\n",
      "batch: 15098, loss: 0.0403694286942482\n",
      "batch: 15099, loss: 0.05721057951450348\n",
      "batch: 15100, loss: 0.13439726829528809\n",
      "model saved to ./saving/model.ckpt-151\n",
      "batch: 15101, loss: 0.02566448412835598\n",
      "batch: 15102, loss: 0.025334997102618217\n",
      "batch: 15103, loss: 0.045670025050640106\n",
      "batch: 15104, loss: 0.0821981281042099\n",
      "batch: 15105, loss: 0.1982964128255844\n",
      "batch: 15106, loss: 0.10125155746936798\n",
      "batch: 15107, loss: 0.05045800283551216\n",
      "batch: 15108, loss: 0.08261114358901978\n",
      "batch: 15109, loss: 0.12787345051765442\n",
      "batch: 15110, loss: 0.20526176691055298\n",
      "batch: 15111, loss: 0.09372363984584808\n",
      "batch: 15112, loss: 0.13203473389148712\n",
      "batch: 15113, loss: 0.07057851552963257\n",
      "batch: 15114, loss: 0.11823917925357819\n",
      "batch: 15115, loss: 0.0891030877828598\n",
      "batch: 15116, loss: 0.11365043371915817\n",
      "batch: 15117, loss: 0.08872736990451813\n",
      "batch: 15118, loss: 0.05744820833206177\n",
      "batch: 15119, loss: 0.07569635659456253\n",
      "batch: 15120, loss: 0.08116891980171204\n",
      "batch: 15121, loss: 0.10884921997785568\n",
      "batch: 15122, loss: 0.224391907453537\n",
      "batch: 15123, loss: 0.25860610604286194\n",
      "batch: 15124, loss: 0.200789213180542\n",
      "batch: 15125, loss: 0.09078219532966614\n",
      "batch: 15126, loss: 0.10508071631193161\n",
      "batch: 15127, loss: 0.08406659960746765\n",
      "batch: 15128, loss: 0.037369899451732635\n",
      "batch: 15129, loss: 0.10104711353778839\n",
      "batch: 15130, loss: 0.10078448057174683\n",
      "batch: 15131, loss: 0.08983847498893738\n",
      "batch: 15132, loss: 0.227789044380188\n",
      "batch: 15133, loss: 0.03410698100924492\n",
      "batch: 15134, loss: 0.07190001010894775\n",
      "batch: 15135, loss: 0.11444644629955292\n",
      "batch: 15136, loss: 0.14576900005340576\n",
      "batch: 15137, loss: 0.07304516434669495\n",
      "batch: 15138, loss: 0.1474771499633789\n",
      "batch: 15139, loss: 0.10537445545196533\n",
      "batch: 15140, loss: 0.07526706159114838\n",
      "batch: 15141, loss: 0.20572394132614136\n",
      "batch: 15142, loss: 0.17543263733386993\n",
      "batch: 15143, loss: 0.10892895609140396\n",
      "batch: 15144, loss: 0.08071660995483398\n",
      "batch: 15145, loss: 0.07758588343858719\n",
      "batch: 15146, loss: 0.12222473323345184\n",
      "batch: 15147, loss: 0.07913520932197571\n",
      "batch: 15148, loss: 0.07213246822357178\n",
      "batch: 15149, loss: 0.16880157589912415\n",
      "batch: 15150, loss: 0.06587028503417969\n",
      "batch: 15151, loss: 0.16500604152679443\n",
      "batch: 15152, loss: 0.0673142671585083\n",
      "batch: 15153, loss: 0.11350337415933609\n",
      "batch: 15154, loss: 0.06503377854824066\n",
      "batch: 15155, loss: 0.17160391807556152\n",
      "batch: 15156, loss: 0.0479498915374279\n",
      "batch: 15157, loss: 0.0963490679860115\n",
      "batch: 15158, loss: 0.10083568096160889\n",
      "batch: 15159, loss: 0.08416837453842163\n",
      "batch: 15160, loss: 0.08632698655128479\n",
      "batch: 15161, loss: 0.11807779967784882\n",
      "batch: 15162, loss: 0.03881052881479263\n",
      "batch: 15163, loss: 0.06276550889015198\n",
      "batch: 15164, loss: 0.027550216764211655\n",
      "batch: 15165, loss: 0.13548332452774048\n",
      "batch: 15166, loss: 0.1342376470565796\n",
      "batch: 15167, loss: 0.07728693634271622\n",
      "batch: 15168, loss: 0.08399319648742676\n",
      "batch: 15169, loss: 0.09648345410823822\n",
      "batch: 15170, loss: 0.057085566222667694\n",
      "batch: 15171, loss: 0.07165795564651489\n",
      "batch: 15172, loss: 0.11474999040365219\n",
      "batch: 15173, loss: 0.1455419659614563\n",
      "batch: 15174, loss: 0.10547254979610443\n",
      "batch: 15175, loss: 0.04871455207467079\n",
      "batch: 15176, loss: 0.2283000499010086\n",
      "batch: 15177, loss: 0.07613745331764221\n",
      "batch: 15178, loss: 0.0942532867193222\n",
      "batch: 15179, loss: 0.06999926269054413\n",
      "batch: 15180, loss: 0.07367606461048126\n",
      "batch: 15181, loss: 0.06531551480293274\n",
      "batch: 15182, loss: 0.11956208944320679\n",
      "batch: 15183, loss: 0.12456589192152023\n",
      "batch: 15184, loss: 0.06395956128835678\n",
      "batch: 15185, loss: 0.09261387586593628\n",
      "batch: 15186, loss: 0.0871150940656662\n",
      "batch: 15187, loss: 0.1588682234287262\n",
      "batch: 15188, loss: 0.19869612157344818\n",
      "batch: 15189, loss: 0.1392611265182495\n",
      "batch: 15190, loss: 0.10081478208303452\n",
      "batch: 15191, loss: 0.11948417127132416\n",
      "batch: 15192, loss: 0.09899978339672089\n",
      "batch: 15193, loss: 0.03919224813580513\n",
      "batch: 15194, loss: 0.10287324339151382\n",
      "batch: 15195, loss: 0.1735822558403015\n",
      "batch: 15196, loss: 0.07156600803136826\n",
      "batch: 15197, loss: 0.1835765838623047\n",
      "batch: 15198, loss: 0.20211496949195862\n",
      "batch: 15199, loss: 0.09216991066932678\n",
      "batch: 15200, loss: 0.15548616647720337\n",
      "model saved to ./saving/model.ckpt-152\n",
      "batch: 15201, loss: 0.05959359183907509\n",
      "batch: 15202, loss: 0.10794688761234283\n",
      "batch: 15203, loss: 0.17562782764434814\n",
      "batch: 15204, loss: 0.06999521702528\n",
      "batch: 15205, loss: 0.22173964977264404\n",
      "batch: 15206, loss: 0.09850730001926422\n",
      "batch: 15207, loss: 0.06722039729356766\n",
      "batch: 15208, loss: 0.09197750687599182\n",
      "batch: 15209, loss: 0.10126444697380066\n",
      "batch: 15210, loss: 0.2277776449918747\n",
      "batch: 15211, loss: 0.09334046393632889\n",
      "batch: 15212, loss: 0.08491191267967224\n",
      "batch: 15213, loss: 0.04394663870334625\n",
      "batch: 15214, loss: 0.1698015034198761\n",
      "batch: 15215, loss: 0.13082937896251678\n",
      "batch: 15216, loss: 0.09264801442623138\n",
      "batch: 15217, loss: 0.1614716798067093\n",
      "batch: 15218, loss: 0.042622096836566925\n",
      "batch: 15219, loss: 0.03992321342229843\n",
      "batch: 15220, loss: 0.05304163321852684\n",
      "batch: 15221, loss: 0.11019699275493622\n",
      "batch: 15222, loss: 0.18311509490013123\n",
      "batch: 15223, loss: 0.08753886818885803\n",
      "batch: 15224, loss: 0.10008271038532257\n",
      "batch: 15225, loss: 0.13158589601516724\n",
      "batch: 15226, loss: 0.07800034433603287\n",
      "batch: 15227, loss: 0.19911222159862518\n",
      "batch: 15228, loss: 0.1352202296257019\n",
      "batch: 15229, loss: 0.07932792603969574\n",
      "batch: 15230, loss: 0.1770155131816864\n",
      "batch: 15231, loss: 0.05292016267776489\n",
      "batch: 15232, loss: 0.06600628793239594\n",
      "batch: 15233, loss: 0.17147819697856903\n",
      "batch: 15234, loss: 0.14743435382843018\n",
      "batch: 15235, loss: 0.08274902403354645\n",
      "batch: 15236, loss: 0.0774846225976944\n",
      "batch: 15237, loss: 0.04566187039017677\n",
      "batch: 15238, loss: 0.09227254241704941\n",
      "batch: 15239, loss: 0.1164640337228775\n",
      "batch: 15240, loss: 0.19270437955856323\n",
      "batch: 15241, loss: 0.11300195753574371\n",
      "batch: 15242, loss: 0.0332532599568367\n",
      "batch: 15243, loss: 0.11896520853042603\n",
      "batch: 15244, loss: 0.1551300287246704\n",
      "batch: 15245, loss: 0.08017605543136597\n",
      "batch: 15246, loss: 0.1552639901638031\n",
      "batch: 15247, loss: 0.08826492726802826\n",
      "batch: 15248, loss: 0.09637296199798584\n",
      "batch: 15249, loss: 0.040304556488990784\n",
      "batch: 15250, loss: 0.1480269730091095\n",
      "batch: 15251, loss: 0.09443517029285431\n",
      "batch: 15252, loss: 0.19176995754241943\n",
      "batch: 15253, loss: 0.1841171532869339\n",
      "batch: 15254, loss: 0.0985795110464096\n",
      "batch: 15255, loss: 0.08090988546609879\n",
      "batch: 15256, loss: 0.10606326162815094\n",
      "batch: 15257, loss: 0.06401731818914413\n",
      "batch: 15258, loss: 0.06418903172016144\n",
      "batch: 15259, loss: 0.12907585501670837\n",
      "batch: 15260, loss: 0.1058104857802391\n",
      "batch: 15261, loss: 0.07502967864274979\n",
      "batch: 15262, loss: 0.08140341937541962\n",
      "batch: 15263, loss: 0.10917945206165314\n",
      "batch: 15264, loss: 0.07476739585399628\n",
      "batch: 15265, loss: 0.060912832617759705\n",
      "batch: 15266, loss: 0.1539119929075241\n",
      "batch: 15267, loss: 0.09323949366807938\n",
      "batch: 15268, loss: 0.14802399277687073\n",
      "batch: 15269, loss: 0.043454986065626144\n",
      "batch: 15270, loss: 0.424416184425354\n",
      "batch: 15271, loss: 0.05883075296878815\n",
      "batch: 15272, loss: 0.08465437591075897\n",
      "batch: 15273, loss: 0.12079992890357971\n",
      "batch: 15274, loss: 0.19143550097942352\n",
      "batch: 15275, loss: 0.2603834569454193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 15276, loss: 0.05690497159957886\n",
      "batch: 15277, loss: 0.12990349531173706\n",
      "batch: 15278, loss: 0.0930912122130394\n",
      "batch: 15279, loss: 0.08778859674930573\n",
      "batch: 15280, loss: 0.1507536768913269\n",
      "batch: 15281, loss: 0.06612375378608704\n",
      "batch: 15282, loss: 0.09229148924350739\n",
      "batch: 15283, loss: 0.07255789637565613\n",
      "batch: 15284, loss: 0.07860691100358963\n",
      "batch: 15285, loss: 0.0654229074716568\n",
      "batch: 15286, loss: 0.11749515682458878\n",
      "batch: 15287, loss: 0.12187889218330383\n",
      "batch: 15288, loss: 0.08977562189102173\n",
      "batch: 15289, loss: 0.07925548404455185\n",
      "batch: 15290, loss: 0.07520724087953568\n",
      "batch: 15291, loss: 0.1565372496843338\n",
      "batch: 15292, loss: 0.21864749491214752\n",
      "batch: 15293, loss: 0.06746521592140198\n",
      "batch: 15294, loss: 0.06855611503124237\n",
      "batch: 15295, loss: 0.2214181125164032\n",
      "batch: 15296, loss: 0.11124348640441895\n",
      "batch: 15297, loss: 0.1042759045958519\n",
      "batch: 15298, loss: 0.03991350159049034\n",
      "batch: 15299, loss: 0.17938369512557983\n",
      "batch: 15300, loss: 0.07195182889699936\n",
      "model saved to ./saving/model.ckpt-153\n",
      "batch: 15301, loss: 0.04265981167554855\n",
      "batch: 15302, loss: 0.07741331309080124\n",
      "batch: 15303, loss: 0.037866778671741486\n",
      "batch: 15304, loss: 0.1339016556739807\n",
      "batch: 15305, loss: 0.06525556743144989\n",
      "batch: 15306, loss: 0.09640632569789886\n",
      "batch: 15307, loss: 0.15643900632858276\n",
      "batch: 15308, loss: 0.191111221909523\n",
      "batch: 15309, loss: 0.11416275054216385\n",
      "batch: 15310, loss: 0.14758913218975067\n",
      "batch: 15311, loss: 0.056542180478572845\n",
      "batch: 15312, loss: 0.05242444574832916\n",
      "batch: 15313, loss: 0.19926834106445312\n",
      "batch: 15314, loss: 0.15342338383197784\n",
      "batch: 15315, loss: 0.10967335104942322\n",
      "batch: 15316, loss: 0.07734804600477219\n",
      "batch: 15317, loss: 0.06940352916717529\n",
      "batch: 15318, loss: 0.11550959944725037\n",
      "batch: 15319, loss: 0.07459031790494919\n",
      "batch: 15320, loss: 0.08714094758033752\n",
      "batch: 15321, loss: 0.09878331422805786\n",
      "batch: 15322, loss: 0.10699442028999329\n",
      "batch: 15323, loss: 0.10333406925201416\n",
      "batch: 15324, loss: 0.10999509692192078\n",
      "batch: 15325, loss: 0.08105718344449997\n",
      "batch: 15326, loss: 0.21951013803482056\n",
      "batch: 15327, loss: 0.09397974610328674\n",
      "batch: 15328, loss: 0.08232347667217255\n",
      "batch: 15329, loss: 0.025907227769494057\n",
      "batch: 15330, loss: 0.2609313726425171\n",
      "batch: 15331, loss: 0.10517280548810959\n",
      "batch: 15332, loss: 0.052629560232162476\n",
      "batch: 15333, loss: 0.07403817772865295\n",
      "batch: 15334, loss: 0.04721327871084213\n",
      "batch: 15335, loss: 0.10242228209972382\n",
      "batch: 15336, loss: 0.06257022172212601\n",
      "batch: 15337, loss: 0.06935632973909378\n",
      "batch: 15338, loss: 0.050756681710481644\n",
      "batch: 15339, loss: 0.08597229421138763\n",
      "batch: 15340, loss: 0.23200705647468567\n",
      "batch: 15341, loss: 0.04077916592359543\n",
      "batch: 15342, loss: 0.10533957183361053\n",
      "batch: 15343, loss: 0.09507042169570923\n",
      "batch: 15344, loss: 0.059605613350868225\n",
      "batch: 15345, loss: 0.07855557650327682\n",
      "batch: 15346, loss: 0.11043237894773483\n",
      "batch: 15347, loss: 0.08781689405441284\n",
      "batch: 15348, loss: 0.04058362543582916\n",
      "batch: 15349, loss: 0.07848753035068512\n",
      "batch: 15350, loss: 0.10685057938098907\n",
      "batch: 15351, loss: 0.09560883790254593\n",
      "batch: 15352, loss: 0.12632712721824646\n",
      "batch: 15353, loss: 0.1115957498550415\n",
      "batch: 15354, loss: 0.32923340797424316\n",
      "batch: 15355, loss: 0.043460696935653687\n",
      "batch: 15356, loss: 0.13745254278182983\n",
      "batch: 15357, loss: 0.1262577921152115\n",
      "batch: 15358, loss: 0.04522546008229256\n",
      "batch: 15359, loss: 0.09480281174182892\n",
      "batch: 15360, loss: 0.11572153121232986\n",
      "batch: 15361, loss: 0.047633521258831024\n",
      "batch: 15362, loss: 0.09995390474796295\n",
      "batch: 15363, loss: 0.06475411355495453\n",
      "batch: 15364, loss: 0.1821199208498001\n",
      "batch: 15365, loss: 0.15763598680496216\n",
      "batch: 15366, loss: 0.0990976095199585\n",
      "batch: 15367, loss: 0.06065734475851059\n",
      "batch: 15368, loss: 0.11912471801042557\n",
      "batch: 15369, loss: 0.13784106075763702\n",
      "batch: 15370, loss: 0.1350807547569275\n",
      "batch: 15371, loss: 0.034556664526462555\n",
      "batch: 15372, loss: 0.12582090497016907\n",
      "batch: 15373, loss: 0.11434865742921829\n",
      "batch: 15374, loss: 0.05608893558382988\n",
      "batch: 15375, loss: 0.040114253759384155\n",
      "batch: 15376, loss: 0.10726894438266754\n",
      "batch: 15377, loss: 0.06133544445037842\n",
      "batch: 15378, loss: 0.026219889521598816\n",
      "batch: 15379, loss: 0.17123481631278992\n",
      "batch: 15380, loss: 0.08788388967514038\n",
      "batch: 15381, loss: 0.06100863218307495\n",
      "batch: 15382, loss: 0.19492104649543762\n",
      "batch: 15383, loss: 0.03990201652050018\n",
      "batch: 15384, loss: 0.0670413225889206\n",
      "batch: 15385, loss: 0.06125379726290703\n",
      "batch: 15386, loss: 0.05132751166820526\n",
      "batch: 15387, loss: 0.092861607670784\n",
      "batch: 15388, loss: 0.09240104258060455\n",
      "batch: 15389, loss: 0.054425112903118134\n",
      "batch: 15390, loss: 0.13920632004737854\n",
      "batch: 15391, loss: 0.04553923010826111\n",
      "batch: 15392, loss: 0.0759672299027443\n",
      "batch: 15393, loss: 0.17335394024848938\n",
      "batch: 15394, loss: 0.05093162879347801\n",
      "batch: 15395, loss: 0.17077553272247314\n",
      "batch: 15396, loss: 0.1502801775932312\n",
      "batch: 15397, loss: 0.028394779190421104\n",
      "batch: 15398, loss: 0.05797988921403885\n",
      "batch: 15399, loss: 0.17023012042045593\n",
      "batch: 15400, loss: 0.17701753973960876\n",
      "model saved to ./saving/model.ckpt-154\n",
      "batch: 15401, loss: 0.09207739681005478\n",
      "batch: 15402, loss: 0.20323163270950317\n",
      "batch: 15403, loss: 0.16152626276016235\n",
      "batch: 15404, loss: 0.05122912675142288\n",
      "batch: 15405, loss: 0.05123905837535858\n",
      "batch: 15406, loss: 0.05420216917991638\n",
      "batch: 15407, loss: 0.06508749723434448\n",
      "batch: 15408, loss: 0.0703439936041832\n",
      "batch: 15409, loss: 0.04597682133316994\n",
      "batch: 15410, loss: 0.06912513077259064\n",
      "batch: 15411, loss: 0.09785842150449753\n",
      "batch: 15412, loss: 0.028845857828855515\n",
      "batch: 15413, loss: 0.16859374940395355\n",
      "batch: 15414, loss: 0.14410193264484406\n",
      "batch: 15415, loss: 0.11338721215724945\n",
      "batch: 15416, loss: 0.15091261267662048\n",
      "batch: 15417, loss: 0.2877896726131439\n",
      "batch: 15418, loss: 0.19472141563892365\n",
      "batch: 15419, loss: 0.26343944668769836\n",
      "batch: 15420, loss: 0.18072712421417236\n",
      "batch: 15421, loss: 0.1491742730140686\n",
      "batch: 15422, loss: 0.10221569240093231\n",
      "batch: 15423, loss: 0.1976996809244156\n",
      "batch: 15424, loss: 0.07429714500904083\n",
      "batch: 15425, loss: 0.2204124480485916\n",
      "batch: 15426, loss: 0.15182863175868988\n",
      "batch: 15427, loss: 0.20742402970790863\n",
      "batch: 15428, loss: 0.12731289863586426\n",
      "batch: 15429, loss: 0.17851203680038452\n",
      "batch: 15430, loss: 0.09266383945941925\n",
      "batch: 15431, loss: 0.1187092661857605\n",
      "batch: 15432, loss: 0.139595627784729\n",
      "batch: 15433, loss: 0.13393563032150269\n",
      "batch: 15434, loss: 0.08323833346366882\n",
      "batch: 15435, loss: 0.04164845868945122\n",
      "batch: 15436, loss: 0.07049715518951416\n",
      "batch: 15437, loss: 0.3880453407764435\n",
      "batch: 15438, loss: 0.08951112627983093\n",
      "batch: 15439, loss: 0.10159103572368622\n",
      "batch: 15440, loss: 0.0980694517493248\n",
      "batch: 15441, loss: 0.09721288084983826\n",
      "batch: 15442, loss: 0.07219090312719345\n",
      "batch: 15443, loss: 0.0626419335603714\n",
      "batch: 15444, loss: 0.12658952176570892\n",
      "batch: 15445, loss: 0.12603965401649475\n",
      "batch: 15446, loss: 0.1339559555053711\n",
      "batch: 15447, loss: 0.05758672207593918\n",
      "batch: 15448, loss: 0.06533998250961304\n",
      "batch: 15449, loss: 0.06447628885507584\n",
      "batch: 15450, loss: 0.12541940808296204\n",
      "batch: 15451, loss: 0.09508219361305237\n",
      "batch: 15452, loss: 0.1294037252664566\n",
      "batch: 15453, loss: 0.07027168571949005\n",
      "batch: 15454, loss: 0.1323108673095703\n",
      "batch: 15455, loss: 0.0528850257396698\n",
      "batch: 15456, loss: 0.15367139875888824\n",
      "batch: 15457, loss: 0.09464189410209656\n",
      "batch: 15458, loss: 0.11200977861881256\n",
      "batch: 15459, loss: 0.15422359108924866\n",
      "batch: 15460, loss: 0.04973062127828598\n",
      "batch: 15461, loss: 0.09092023223638535\n",
      "batch: 15462, loss: 0.1367780566215515\n",
      "batch: 15463, loss: 0.020848659798502922\n",
      "batch: 15464, loss: 0.24788334965705872\n",
      "batch: 15465, loss: 0.125156968832016\n",
      "batch: 15466, loss: 0.06348059326410294\n",
      "batch: 15467, loss: 0.09992542862892151\n",
      "batch: 15468, loss: 0.09503789246082306\n",
      "batch: 15469, loss: 0.03941783308982849\n",
      "batch: 15470, loss: 0.14239051938056946\n",
      "batch: 15471, loss: 0.025532353669404984\n",
      "batch: 15472, loss: 0.04113750904798508\n",
      "batch: 15473, loss: 0.1205972284078598\n",
      "batch: 15474, loss: 0.09240753948688507\n",
      "batch: 15475, loss: 0.07117035239934921\n",
      "batch: 15476, loss: 0.06490613520145416\n",
      "batch: 15477, loss: 0.039619628340005875\n",
      "batch: 15478, loss: 0.13671551644802094\n",
      "batch: 15479, loss: 0.040669508278369904\n",
      "batch: 15480, loss: 0.14232909679412842\n",
      "batch: 15481, loss: 0.08166150748729706\n",
      "batch: 15482, loss: 0.08175961673259735\n",
      "batch: 15483, loss: 0.19797088205814362\n",
      "batch: 15484, loss: 0.2606804370880127\n",
      "batch: 15485, loss: 0.08609949052333832\n",
      "batch: 15486, loss: 0.051369667053222656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 15487, loss: 0.10471658408641815\n",
      "batch: 15488, loss: 0.15860551595687866\n",
      "batch: 15489, loss: 0.051651813089847565\n",
      "batch: 15490, loss: 0.09265784919261932\n",
      "batch: 15491, loss: 0.0951879620552063\n",
      "batch: 15492, loss: 0.2223561406135559\n",
      "batch: 15493, loss: 0.18122470378875732\n",
      "batch: 15494, loss: 0.06779889762401581\n",
      "batch: 15495, loss: 0.1303732842206955\n",
      "batch: 15496, loss: 0.057586465030908585\n",
      "batch: 15497, loss: 0.10775424540042877\n",
      "batch: 15498, loss: 0.09788515418767929\n",
      "batch: 15499, loss: 0.06033125892281532\n",
      "batch: 15500, loss: 0.15900135040283203\n",
      "model saved to ./saving/model.ckpt-155\n",
      "batch: 15501, loss: 0.0917375236749649\n",
      "batch: 15502, loss: 0.08421249687671661\n",
      "batch: 15503, loss: 0.06168704107403755\n",
      "batch: 15504, loss: 0.12172642350196838\n",
      "batch: 15505, loss: 0.21019087731838226\n",
      "batch: 15506, loss: 0.04309508204460144\n",
      "batch: 15507, loss: 0.0480579137802124\n",
      "batch: 15508, loss: 0.11449830234050751\n",
      "batch: 15509, loss: 0.10639271885156631\n",
      "batch: 15510, loss: 0.16076727211475372\n",
      "batch: 15511, loss: 0.05695085972547531\n",
      "batch: 15512, loss: 0.08897490799427032\n",
      "batch: 15513, loss: 0.14965510368347168\n",
      "batch: 15514, loss: 0.11834989488124847\n",
      "batch: 15515, loss: 0.15953926742076874\n",
      "batch: 15516, loss: 0.08760098367929459\n",
      "batch: 15517, loss: 0.14073145389556885\n",
      "batch: 15518, loss: 0.12009835988283157\n",
      "batch: 15519, loss: 0.04760961979627609\n",
      "batch: 15520, loss: 0.061883073300123215\n",
      "batch: 15521, loss: 0.046489737927913666\n",
      "batch: 15522, loss: 0.08184602111577988\n",
      "batch: 15523, loss: 0.13481749594211578\n",
      "batch: 15524, loss: 0.046959660947322845\n",
      "batch: 15525, loss: 0.14144259691238403\n",
      "batch: 15526, loss: 0.12323257327079773\n",
      "batch: 15527, loss: 0.07685325294733047\n",
      "batch: 15528, loss: 0.15324419736862183\n",
      "batch: 15529, loss: 0.06363161653280258\n",
      "batch: 15530, loss: 0.10211125016212463\n",
      "batch: 15531, loss: 0.057385653257369995\n",
      "batch: 15532, loss: 0.20717087388038635\n",
      "batch: 15533, loss: 0.0845591276884079\n",
      "batch: 15534, loss: 0.05847535282373428\n",
      "batch: 15535, loss: 0.1640370488166809\n",
      "batch: 15536, loss: 0.12362521141767502\n",
      "batch: 15537, loss: 0.059670452028512955\n",
      "batch: 15538, loss: 0.1353561133146286\n",
      "batch: 15539, loss: 0.34053269028663635\n",
      "batch: 15540, loss: 0.08346518129110336\n",
      "batch: 15541, loss: 0.15073882043361664\n",
      "batch: 15542, loss: 0.15112659335136414\n",
      "batch: 15543, loss: 0.07442270219326019\n",
      "batch: 15544, loss: 0.05233099311590195\n",
      "batch: 15545, loss: 0.11602713167667389\n",
      "batch: 15546, loss: 0.06822448968887329\n",
      "batch: 15547, loss: 0.08434665948152542\n",
      "batch: 15548, loss: 0.0847269743680954\n",
      "batch: 15549, loss: 0.0378323569893837\n",
      "batch: 15550, loss: 0.10559013485908508\n",
      "batch: 15551, loss: 0.1807481050491333\n",
      "batch: 15552, loss: 0.0452081598341465\n",
      "batch: 15553, loss: 0.10099643468856812\n",
      "batch: 15554, loss: 0.06733234226703644\n",
      "batch: 15555, loss: 0.2176249474287033\n",
      "batch: 15556, loss: 0.18422774970531464\n",
      "batch: 15557, loss: 0.0628158450126648\n",
      "batch: 15558, loss: 0.06999777257442474\n",
      "batch: 15559, loss: 0.082064688205719\n",
      "batch: 15560, loss: 0.06149134039878845\n",
      "batch: 15561, loss: 0.11458691209554672\n",
      "batch: 15562, loss: 0.13943594694137573\n",
      "batch: 15563, loss: 0.12760770320892334\n",
      "batch: 15564, loss: 0.09357597678899765\n",
      "batch: 15565, loss: 0.08622334152460098\n",
      "batch: 15566, loss: 0.12332163751125336\n",
      "batch: 15567, loss: 0.12442478537559509\n",
      "batch: 15568, loss: 0.10498182475566864\n",
      "batch: 15569, loss: 0.08694370836019516\n",
      "batch: 15570, loss: 0.09827137738466263\n",
      "batch: 15571, loss: 0.09448850154876709\n",
      "batch: 15572, loss: 0.052290547639131546\n",
      "batch: 15573, loss: 0.06534504890441895\n",
      "batch: 15574, loss: 0.06472446024417877\n",
      "batch: 15575, loss: 0.15732000768184662\n",
      "batch: 15576, loss: 0.06392689794301987\n",
      "batch: 15577, loss: 0.11164253205060959\n",
      "batch: 15578, loss: 0.04582666605710983\n",
      "batch: 15579, loss: 0.10805977880954742\n",
      "batch: 15580, loss: 0.1878008097410202\n",
      "batch: 15581, loss: 0.18133223056793213\n",
      "batch: 15582, loss: 0.0834745392203331\n",
      "batch: 15583, loss: 0.11515478044748306\n",
      "batch: 15584, loss: 0.12724669277668\n",
      "batch: 15585, loss: 0.05365060269832611\n",
      "batch: 15586, loss: 0.36588358879089355\n",
      "batch: 15587, loss: 0.08817625790834427\n",
      "batch: 15588, loss: 0.04728152975440025\n",
      "batch: 15589, loss: 0.1934809386730194\n",
      "batch: 15590, loss: 0.14474065601825714\n",
      "batch: 15591, loss: 0.1223621517419815\n",
      "batch: 15592, loss: 0.11867362260818481\n",
      "batch: 15593, loss: 0.1582150161266327\n",
      "batch: 15594, loss: 0.074136883020401\n",
      "batch: 15595, loss: 0.04192767292261124\n",
      "batch: 15596, loss: 0.05278942734003067\n",
      "batch: 15597, loss: 0.08827273547649384\n",
      "batch: 15598, loss: 0.08316190540790558\n",
      "batch: 15599, loss: 0.19233950972557068\n",
      "batch: 15600, loss: 0.1759556531906128\n",
      "model saved to ./saving/model.ckpt-156\n",
      "batch: 15601, loss: 0.06126804277300835\n",
      "batch: 15602, loss: 0.12069103121757507\n",
      "batch: 15603, loss: 0.09595949202775955\n",
      "batch: 15604, loss: 0.13638857007026672\n",
      "batch: 15605, loss: 0.1261475682258606\n",
      "batch: 15606, loss: 0.030853567644953728\n",
      "batch: 15607, loss: 0.10797739028930664\n",
      "batch: 15608, loss: 0.25531667470932007\n",
      "batch: 15609, loss: 0.12320201098918915\n",
      "batch: 15610, loss: 0.17946253716945648\n",
      "batch: 15611, loss: 0.11584542691707611\n",
      "batch: 15612, loss: 0.1126428097486496\n",
      "batch: 15613, loss: 0.1304125189781189\n",
      "batch: 15614, loss: 0.13390028476715088\n",
      "batch: 15615, loss: 0.06187893822789192\n",
      "batch: 15616, loss: 0.05241399258375168\n",
      "batch: 15617, loss: 0.05751872807741165\n",
      "batch: 15618, loss: 0.06425666064023972\n",
      "batch: 15619, loss: 0.042164430022239685\n",
      "batch: 15620, loss: 0.04462170600891113\n",
      "batch: 15621, loss: 0.07613548636436462\n",
      "batch: 15622, loss: 0.1492980718612671\n",
      "batch: 15623, loss: 0.09905914962291718\n",
      "batch: 15624, loss: 0.04479316994547844\n",
      "batch: 15625, loss: 0.13307029008865356\n",
      "batch: 15626, loss: 0.1771034300327301\n",
      "batch: 15627, loss: 0.09198819100856781\n",
      "batch: 15628, loss: 0.04222334176301956\n",
      "batch: 15629, loss: 0.25057923793792725\n",
      "batch: 15630, loss: 0.11786440014839172\n",
      "batch: 15631, loss: 0.07268877327442169\n",
      "batch: 15632, loss: 0.1128949373960495\n",
      "batch: 15633, loss: 0.09008466452360153\n",
      "batch: 15634, loss: 0.11878080666065216\n",
      "batch: 15635, loss: 0.0728704109787941\n",
      "batch: 15636, loss: 0.055842380970716476\n",
      "batch: 15637, loss: 0.050737932324409485\n",
      "batch: 15638, loss: 0.028117341920733452\n",
      "batch: 15639, loss: 0.08372196555137634\n",
      "batch: 15640, loss: 0.05538855865597725\n",
      "batch: 15641, loss: 0.09772041440010071\n",
      "batch: 15642, loss: 0.16249406337738037\n",
      "batch: 15643, loss: 0.08291923254728317\n",
      "batch: 15644, loss: 0.033816322684288025\n",
      "batch: 15645, loss: 0.17847272753715515\n",
      "batch: 15646, loss: 0.11437878012657166\n",
      "batch: 15647, loss: 0.1549655646085739\n",
      "batch: 15648, loss: 0.09938207268714905\n",
      "batch: 15649, loss: 0.06371526420116425\n",
      "batch: 15650, loss: 0.22649648785591125\n",
      "batch: 15651, loss: 0.08605480194091797\n",
      "batch: 15652, loss: 0.25272136926651\n",
      "batch: 15653, loss: 0.11283273994922638\n",
      "batch: 15654, loss: 0.12357322126626968\n",
      "batch: 15655, loss: 0.03160780668258667\n",
      "batch: 15656, loss: 0.09614724665880203\n",
      "batch: 15657, loss: 0.055460374802351\n",
      "batch: 15658, loss: 0.11265256255865097\n",
      "batch: 15659, loss: 0.061400990933179855\n",
      "batch: 15660, loss: 0.11666907370090485\n",
      "batch: 15661, loss: 0.08285209536552429\n",
      "batch: 15662, loss: 0.13033747673034668\n",
      "batch: 15663, loss: 0.042642682790756226\n",
      "batch: 15664, loss: 0.030326005071401596\n",
      "batch: 15665, loss: 0.16724556684494019\n",
      "batch: 15666, loss: 0.060057174414396286\n",
      "batch: 15667, loss: 0.06170211732387543\n",
      "batch: 15668, loss: 0.17613890767097473\n",
      "batch: 15669, loss: 0.10722195357084274\n",
      "batch: 15670, loss: 0.091252900660038\n",
      "batch: 15671, loss: 0.07632505893707275\n",
      "batch: 15672, loss: 0.055800486356019974\n",
      "batch: 15673, loss: 0.03932759538292885\n",
      "batch: 15674, loss: 0.1537877768278122\n",
      "batch: 15675, loss: 0.08362384140491486\n",
      "batch: 15676, loss: 0.07736711204051971\n",
      "batch: 15677, loss: 0.08841390907764435\n",
      "batch: 15678, loss: 0.10573578625917435\n",
      "batch: 15679, loss: 0.10051688551902771\n",
      "batch: 15680, loss: 0.12181484699249268\n",
      "batch: 15681, loss: 0.03648798167705536\n",
      "batch: 15682, loss: 0.03033776581287384\n",
      "batch: 15683, loss: 0.0910182073712349\n",
      "batch: 15684, loss: 0.06202370673418045\n",
      "batch: 15685, loss: 0.09297037124633789\n",
      "batch: 15686, loss: 0.10062941908836365\n",
      "batch: 15687, loss: 0.12640820443630219\n",
      "batch: 15688, loss: 0.07117097079753876\n",
      "batch: 15689, loss: 0.10299506783485413\n",
      "batch: 15690, loss: 0.05292845517396927\n",
      "batch: 15691, loss: 0.040590204298496246\n",
      "batch: 15692, loss: 0.08716197311878204\n",
      "batch: 15693, loss: 0.20068049430847168\n",
      "batch: 15694, loss: 0.1312549114227295\n",
      "batch: 15695, loss: 0.09359024465084076\n",
      "batch: 15696, loss: 0.09123966842889786\n",
      "batch: 15697, loss: 0.15912675857543945\n",
      "batch: 15698, loss: 0.03913579881191254\n",
      "batch: 15699, loss: 0.07952284812927246\n",
      "batch: 15700, loss: 0.12350626289844513\n",
      "model saved to ./saving/model.ckpt-157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 15701, loss: 0.13189257681369781\n",
      "batch: 15702, loss: 0.1364855021238327\n",
      "batch: 15703, loss: 0.0429309606552124\n",
      "batch: 15704, loss: 0.20629683136940002\n",
      "batch: 15705, loss: 0.04341084882616997\n",
      "batch: 15706, loss: 0.027465498074889183\n",
      "batch: 15707, loss: 0.08929510414600372\n",
      "batch: 15708, loss: 0.09735739231109619\n",
      "batch: 15709, loss: 0.07340161502361298\n",
      "batch: 15710, loss: 0.03150683268904686\n",
      "batch: 15711, loss: 0.12003402411937714\n",
      "batch: 15712, loss: 0.05759064108133316\n",
      "batch: 15713, loss: 0.0452897883951664\n",
      "batch: 15714, loss: 0.14770890772342682\n",
      "batch: 15715, loss: 0.11890743672847748\n",
      "batch: 15716, loss: 0.16750362515449524\n",
      "batch: 15717, loss: 0.13247862458229065\n",
      "batch: 15718, loss: 0.1033521518111229\n",
      "batch: 15719, loss: 0.07597135007381439\n",
      "batch: 15720, loss: 0.08003245294094086\n",
      "batch: 15721, loss: 0.12091898918151855\n",
      "batch: 15722, loss: 0.16284215450286865\n",
      "batch: 15723, loss: 0.1360335350036621\n",
      "batch: 15724, loss: 0.03892786055803299\n",
      "batch: 15725, loss: 0.25195345282554626\n",
      "batch: 15726, loss: 0.0865418016910553\n",
      "batch: 15727, loss: 0.14569947123527527\n",
      "batch: 15728, loss: 0.0785817876458168\n",
      "batch: 15729, loss: 0.11396520584821701\n",
      "batch: 15730, loss: 0.15972477197647095\n",
      "batch: 15731, loss: 0.08552601933479309\n",
      "batch: 15732, loss: 0.13696302473545074\n",
      "batch: 15733, loss: 0.1036502942442894\n",
      "batch: 15734, loss: 0.04293656349182129\n",
      "batch: 15735, loss: 0.1165517121553421\n",
      "batch: 15736, loss: 0.08766745030879974\n",
      "batch: 15737, loss: 0.14145296812057495\n",
      "batch: 15738, loss: 0.04959551990032196\n",
      "batch: 15739, loss: 0.12171238660812378\n",
      "batch: 15740, loss: 0.1157757043838501\n",
      "batch: 15741, loss: 0.17843371629714966\n",
      "batch: 15742, loss: 0.08104885369539261\n",
      "batch: 15743, loss: 0.05690866708755493\n",
      "batch: 15744, loss: 0.15160930156707764\n",
      "batch: 15745, loss: 0.09299641102552414\n",
      "batch: 15746, loss: 0.1431005746126175\n",
      "batch: 15747, loss: 0.12497470527887344\n",
      "batch: 15748, loss: 0.21293431520462036\n",
      "batch: 15749, loss: 0.08541977405548096\n",
      "batch: 15750, loss: 0.05628630146384239\n",
      "batch: 15751, loss: 0.1327199786901474\n",
      "batch: 15752, loss: 0.044635429978370667\n",
      "batch: 15753, loss: 0.03143494576215744\n",
      "batch: 15754, loss: 0.03964050114154816\n",
      "batch: 15755, loss: 0.027084778994321823\n",
      "batch: 15756, loss: 0.12092484533786774\n",
      "batch: 15757, loss: 0.07733649015426636\n",
      "batch: 15758, loss: 0.06863277405500412\n",
      "batch: 15759, loss: 0.04950582981109619\n",
      "batch: 15760, loss: 0.04092000424861908\n",
      "batch: 15761, loss: 0.07545112818479538\n",
      "batch: 15762, loss: 0.08941584825515747\n",
      "batch: 15763, loss: 0.08514779061079025\n",
      "batch: 15764, loss: 0.1031179279088974\n",
      "batch: 15765, loss: 0.10750909149646759\n",
      "batch: 15766, loss: 0.2152334451675415\n",
      "batch: 15767, loss: 0.10159631818532944\n",
      "batch: 15768, loss: 0.09620651602745056\n",
      "batch: 15769, loss: 0.09039796143770218\n",
      "batch: 15770, loss: 0.08921945840120316\n",
      "batch: 15771, loss: 0.07238775491714478\n",
      "batch: 15772, loss: 0.09357519447803497\n",
      "batch: 15773, loss: 0.09531151503324509\n",
      "batch: 15774, loss: 0.12852606177330017\n",
      "batch: 15775, loss: 0.053344883024692535\n",
      "batch: 15776, loss: 0.027001962065696716\n",
      "batch: 15777, loss: 0.021637745201587677\n",
      "batch: 15778, loss: 0.09931686520576477\n",
      "batch: 15779, loss: 0.05983498692512512\n",
      "batch: 15780, loss: 0.08369258791208267\n",
      "batch: 15781, loss: 0.0827900767326355\n",
      "batch: 15782, loss: 0.14525771141052246\n",
      "batch: 15783, loss: 0.0747971162199974\n",
      "batch: 15784, loss: 0.0747077465057373\n",
      "batch: 15785, loss: 0.1495952606201172\n",
      "batch: 15786, loss: 0.036971718072891235\n",
      "batch: 15787, loss: 0.08898761868476868\n",
      "batch: 15788, loss: 0.05854491516947746\n",
      "batch: 15789, loss: 0.08924319595098495\n",
      "batch: 15790, loss: 0.11680544912815094\n",
      "batch: 15791, loss: 0.12493211030960083\n",
      "batch: 15792, loss: 0.10228949040174484\n",
      "batch: 15793, loss: 0.09440266340970993\n",
      "batch: 15794, loss: 0.24886436760425568\n",
      "batch: 15795, loss: 0.05898957699537277\n",
      "batch: 15796, loss: 0.03649459406733513\n",
      "batch: 15797, loss: 0.05736623704433441\n",
      "batch: 15798, loss: 0.2589041590690613\n",
      "batch: 15799, loss: 0.05194181948900223\n",
      "batch: 15800, loss: 0.09816821664571762\n",
      "model saved to ./saving/model.ckpt-158\n",
      "batch: 15801, loss: 0.13788846135139465\n",
      "batch: 15802, loss: 0.11000627279281616\n",
      "batch: 15803, loss: 0.07234908640384674\n",
      "batch: 15804, loss: 0.05710121989250183\n",
      "batch: 15805, loss: 0.1626359075307846\n",
      "batch: 15806, loss: 0.06738235056400299\n",
      "batch: 15807, loss: 0.12432970106601715\n",
      "batch: 15808, loss: 0.05387897789478302\n",
      "batch: 15809, loss: 0.16661080718040466\n",
      "batch: 15810, loss: 0.07666073739528656\n",
      "batch: 15811, loss: 0.0599629282951355\n",
      "batch: 15812, loss: 0.06016143038868904\n",
      "batch: 15813, loss: 0.13679392635822296\n",
      "batch: 15814, loss: 0.07827582955360413\n",
      "batch: 15815, loss: 0.12395508587360382\n",
      "batch: 15816, loss: 0.0686444416642189\n",
      "batch: 15817, loss: 0.06629344820976257\n",
      "batch: 15818, loss: 0.1655413806438446\n",
      "batch: 15819, loss: 0.10547474026679993\n",
      "batch: 15820, loss: 0.060837894678115845\n",
      "batch: 15821, loss: 0.08540024608373642\n",
      "batch: 15822, loss: 0.17490698397159576\n",
      "batch: 15823, loss: 0.11081614345312119\n",
      "batch: 15824, loss: 0.11062037944793701\n",
      "batch: 15825, loss: 0.12746554613113403\n",
      "batch: 15826, loss: 0.11125701665878296\n",
      "batch: 15827, loss: 0.13828609883785248\n",
      "batch: 15828, loss: 0.08925706893205643\n",
      "batch: 15829, loss: 0.021793046966195107\n",
      "batch: 15830, loss: 0.21252833306789398\n",
      "batch: 15831, loss: 0.14063893258571625\n",
      "batch: 15832, loss: 0.14052030444145203\n",
      "batch: 15833, loss: 0.09347490966320038\n",
      "batch: 15834, loss: 0.07292836159467697\n",
      "batch: 15835, loss: 0.10272429883480072\n",
      "batch: 15836, loss: 0.11181948333978653\n",
      "batch: 15837, loss: 0.08625473082065582\n",
      "batch: 15838, loss: 0.23336493968963623\n",
      "batch: 15839, loss: 0.07794545590877533\n",
      "batch: 15840, loss: 0.027443157508969307\n",
      "batch: 15841, loss: 0.23048008978366852\n",
      "batch: 15842, loss: 0.06124250963330269\n",
      "batch: 15843, loss: 0.04013340175151825\n",
      "batch: 15844, loss: 0.07315094769001007\n",
      "batch: 15845, loss: 0.17173227667808533\n",
      "batch: 15846, loss: 0.09303059428930283\n",
      "batch: 15847, loss: 0.17783787846565247\n",
      "batch: 15848, loss: 0.12843158841133118\n",
      "batch: 15849, loss: 0.14996524155139923\n",
      "batch: 15850, loss: 0.08857496082782745\n",
      "batch: 15851, loss: 0.08109067380428314\n",
      "batch: 15852, loss: 0.222599059343338\n",
      "batch: 15853, loss: 0.11302177608013153\n",
      "batch: 15854, loss: 0.18977504968643188\n",
      "batch: 15855, loss: 0.23746739327907562\n",
      "batch: 15856, loss: 0.0946497917175293\n",
      "batch: 15857, loss: 0.11537101119756699\n",
      "batch: 15858, loss: 0.10410897433757782\n",
      "batch: 15859, loss: 0.1710197627544403\n",
      "batch: 15860, loss: 0.03913004696369171\n",
      "batch: 15861, loss: 0.04727037996053696\n",
      "batch: 15862, loss: 0.043401166796684265\n",
      "batch: 15863, loss: 0.031265124678611755\n",
      "batch: 15864, loss: 0.1185649037361145\n",
      "batch: 15865, loss: 0.056530121713876724\n",
      "batch: 15866, loss: 0.3689514398574829\n",
      "batch: 15867, loss: 0.05438465625047684\n",
      "batch: 15868, loss: 0.04604465886950493\n",
      "batch: 15869, loss: 0.13382871448993683\n",
      "batch: 15870, loss: 0.0461462140083313\n",
      "batch: 15871, loss: 0.04726457595825195\n",
      "batch: 15872, loss: 0.10861378908157349\n",
      "batch: 15873, loss: 0.07354266941547394\n",
      "batch: 15874, loss: 0.3073194622993469\n",
      "batch: 15875, loss: 0.044765982776880264\n",
      "batch: 15876, loss: 0.05307646468281746\n",
      "batch: 15877, loss: 0.03509277105331421\n",
      "batch: 15878, loss: 0.06801288574934006\n",
      "batch: 15879, loss: 0.09849429130554199\n",
      "batch: 15880, loss: 0.09378013759851456\n",
      "batch: 15881, loss: 0.19455227255821228\n",
      "batch: 15882, loss: 0.03571426495909691\n",
      "batch: 15883, loss: 0.18146152794361115\n",
      "batch: 15884, loss: 0.13740742206573486\n",
      "batch: 15885, loss: 0.11084853112697601\n",
      "batch: 15886, loss: 0.13602179288864136\n",
      "batch: 15887, loss: 0.09617764502763748\n",
      "batch: 15888, loss: 0.01806144043803215\n",
      "batch: 15889, loss: 0.11762344837188721\n",
      "batch: 15890, loss: 0.07751666754484177\n",
      "batch: 15891, loss: 0.11832177639007568\n",
      "batch: 15892, loss: 0.03107120655477047\n",
      "batch: 15893, loss: 0.06899378448724747\n",
      "batch: 15894, loss: 0.1261821985244751\n",
      "batch: 15895, loss: 0.04415912181138992\n",
      "batch: 15896, loss: 0.05162420868873596\n",
      "batch: 15897, loss: 0.10551968961954117\n",
      "batch: 15898, loss: 0.10822564363479614\n",
      "batch: 15899, loss: 0.0742771178483963\n",
      "batch: 15900, loss: 0.06637286394834518\n",
      "model saved to ./saving/model.ckpt-159\n",
      "batch: 15901, loss: 0.06432752311229706\n",
      "batch: 15902, loss: 0.03666210174560547\n",
      "batch: 15903, loss: 0.07981140166521072\n",
      "batch: 15904, loss: 0.03909050673246384\n",
      "batch: 15905, loss: 0.09557738900184631\n",
      "batch: 15906, loss: 0.10267026722431183\n",
      "batch: 15907, loss: 0.10303431749343872\n",
      "batch: 15908, loss: 0.13061882555484772\n",
      "batch: 15909, loss: 0.1373620629310608\n",
      "batch: 15910, loss: 0.14222078025341034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 15911, loss: 0.08882007002830505\n",
      "batch: 15912, loss: 0.09879308938980103\n",
      "batch: 15913, loss: 0.10841956734657288\n",
      "batch: 15914, loss: 0.07286939769983292\n",
      "batch: 15915, loss: 0.07398068904876709\n",
      "batch: 15916, loss: 0.09977489709854126\n",
      "batch: 15917, loss: 0.11178924143314362\n",
      "batch: 15918, loss: 0.059944115579128265\n",
      "batch: 15919, loss: 0.07741892337799072\n",
      "batch: 15920, loss: 0.11480490863323212\n",
      "batch: 15921, loss: 0.08937455713748932\n",
      "batch: 15922, loss: 0.07489165663719177\n",
      "batch: 15923, loss: 0.05410770699381828\n",
      "batch: 15924, loss: 0.053838953375816345\n",
      "batch: 15925, loss: 0.08143261820077896\n",
      "batch: 15926, loss: 0.16169482469558716\n",
      "batch: 15927, loss: 0.07358800619840622\n",
      "batch: 15928, loss: 0.08900105953216553\n",
      "batch: 15929, loss: 0.09985356032848358\n",
      "batch: 15930, loss: 0.10145842283964157\n",
      "batch: 15931, loss: 0.10978789627552032\n",
      "batch: 15932, loss: 0.11285438388586044\n",
      "batch: 15933, loss: 0.17754553258419037\n",
      "batch: 15934, loss: 0.11452250182628632\n",
      "batch: 15935, loss: 0.21828106045722961\n",
      "batch: 15936, loss: 0.12178629636764526\n",
      "batch: 15937, loss: 0.08698685467243195\n",
      "batch: 15938, loss: 0.18328329920768738\n",
      "batch: 15939, loss: 0.04800287261605263\n",
      "batch: 15940, loss: 0.08199211210012436\n",
      "batch: 15941, loss: 0.03485835716128349\n",
      "batch: 15942, loss: 0.11875290423631668\n",
      "batch: 15943, loss: 0.13361892104148865\n",
      "batch: 15944, loss: 0.1689339280128479\n",
      "batch: 15945, loss: 0.10479031503200531\n",
      "batch: 15946, loss: 0.07910260558128357\n",
      "batch: 15947, loss: 0.05399300903081894\n",
      "batch: 15948, loss: 0.0699998065829277\n",
      "batch: 15949, loss: 0.16495561599731445\n",
      "batch: 15950, loss: 0.09594686329364777\n",
      "batch: 15951, loss: 0.1368359923362732\n",
      "batch: 15952, loss: 0.2239437997341156\n",
      "batch: 15953, loss: 0.0891881212592125\n",
      "batch: 15954, loss: 0.15289005637168884\n",
      "batch: 15955, loss: 0.11610733717679977\n",
      "batch: 15956, loss: 0.22533631324768066\n",
      "batch: 15957, loss: 0.12637874484062195\n",
      "batch: 15958, loss: 0.060040511190891266\n",
      "batch: 15959, loss: 0.16568326950073242\n",
      "batch: 15960, loss: 0.0671309307217598\n",
      "batch: 15961, loss: 0.11307267099618912\n",
      "batch: 15962, loss: 0.18317438662052155\n",
      "batch: 15963, loss: 0.06527300179004669\n",
      "batch: 15964, loss: 0.040389880537986755\n",
      "batch: 15965, loss: 0.059071704745292664\n",
      "batch: 15966, loss: 0.09196971356868744\n",
      "batch: 15967, loss: 0.14765536785125732\n",
      "batch: 15968, loss: 0.1970677375793457\n",
      "batch: 15969, loss: 0.11598050594329834\n",
      "batch: 15970, loss: 0.07870353758335114\n",
      "batch: 15971, loss: 0.032191403210163116\n",
      "batch: 15972, loss: 0.08578206598758698\n",
      "batch: 15973, loss: 0.027480581775307655\n",
      "batch: 15974, loss: 0.05024924874305725\n",
      "batch: 15975, loss: 0.11011706292629242\n",
      "batch: 15976, loss: 0.09731470048427582\n",
      "batch: 15977, loss: 0.03610631078481674\n",
      "batch: 15978, loss: 0.07797154784202576\n",
      "batch: 15979, loss: 0.229855477809906\n",
      "batch: 15980, loss: 0.06812867522239685\n",
      "batch: 15981, loss: 0.18448400497436523\n",
      "batch: 15982, loss: 0.04903882369399071\n",
      "batch: 15983, loss: 0.21161895990371704\n",
      "batch: 15984, loss: 0.10007584095001221\n",
      "batch: 15985, loss: 0.1749318540096283\n",
      "batch: 15986, loss: 0.1863372027873993\n",
      "batch: 15987, loss: 0.05277824029326439\n",
      "batch: 15988, loss: 0.02512509934604168\n",
      "batch: 15989, loss: 0.11181404441595078\n",
      "batch: 15990, loss: 0.1706789880990982\n",
      "batch: 15991, loss: 0.042606454342603683\n",
      "batch: 15992, loss: 0.03060898184776306\n",
      "batch: 15993, loss: 0.10946955531835556\n",
      "batch: 15994, loss: 0.1277948021888733\n",
      "batch: 15995, loss: 0.0551876500248909\n",
      "batch: 15996, loss: 0.06823157519102097\n",
      "batch: 15997, loss: 0.030905166640877724\n",
      "batch: 15998, loss: 0.06350581347942352\n",
      "batch: 15999, loss: 0.06162475049495697\n",
      "batch: 16000, loss: 0.15334007143974304\n",
      "model saved to ./saving/model.ckpt-160\n",
      "batch: 16001, loss: 0.04578077793121338\n",
      "batch: 16002, loss: 0.06363274157047272\n",
      "batch: 16003, loss: 0.07032394409179688\n",
      "batch: 16004, loss: 0.23573681712150574\n",
      "batch: 16005, loss: 0.07663355022668839\n",
      "batch: 16006, loss: 0.11577299982309341\n",
      "batch: 16007, loss: 0.10309982299804688\n",
      "batch: 16008, loss: 0.1393994688987732\n",
      "batch: 16009, loss: 0.10286082327365875\n",
      "batch: 16010, loss: 0.10725373029708862\n",
      "batch: 16011, loss: 0.06977260112762451\n",
      "batch: 16012, loss: 0.12321488559246063\n",
      "batch: 16013, loss: 0.13563509285449982\n",
      "batch: 16014, loss: 0.0725296139717102\n",
      "batch: 16015, loss: 0.04948977380990982\n",
      "batch: 16016, loss: 0.10669625550508499\n",
      "batch: 16017, loss: 0.13274842500686646\n",
      "batch: 16018, loss: 0.08550438284873962\n",
      "batch: 16019, loss: 0.09878385066986084\n",
      "batch: 16020, loss: 0.0446263812482357\n",
      "batch: 16021, loss: 0.07466805726289749\n",
      "batch: 16022, loss: 0.04294341802597046\n",
      "batch: 16023, loss: 0.16044825315475464\n",
      "batch: 16024, loss: 0.17996728420257568\n",
      "batch: 16025, loss: 0.050838109105825424\n",
      "batch: 16026, loss: 0.11269645392894745\n",
      "batch: 16027, loss: 0.09937310963869095\n",
      "batch: 16028, loss: 0.2024616301059723\n",
      "batch: 16029, loss: 0.07899048924446106\n",
      "batch: 16030, loss: 0.08641614764928818\n",
      "batch: 16031, loss: 0.08481298387050629\n",
      "batch: 16032, loss: 0.12109334766864777\n",
      "batch: 16033, loss: 0.10600633919239044\n",
      "batch: 16034, loss: 0.10215314477682114\n",
      "batch: 16035, loss: 0.06850700080394745\n",
      "batch: 16036, loss: 0.09017394483089447\n",
      "batch: 16037, loss: 0.09335693717002869\n",
      "batch: 16038, loss: 0.13223981857299805\n",
      "batch: 16039, loss: 0.12999558448791504\n",
      "batch: 16040, loss: 0.06837546080350876\n",
      "batch: 16041, loss: 0.09092898666858673\n",
      "batch: 16042, loss: 0.1941240280866623\n",
      "batch: 16043, loss: 0.060088254511356354\n",
      "batch: 16044, loss: 0.033985435962677\n",
      "batch: 16045, loss: 0.05408809706568718\n",
      "batch: 16046, loss: 0.09560483694076538\n",
      "batch: 16047, loss: 0.0808594822883606\n",
      "batch: 16048, loss: 0.07529338449239731\n",
      "batch: 16049, loss: 0.1327732354402542\n",
      "batch: 16050, loss: 0.19463787972927094\n",
      "batch: 16051, loss: 0.07853514701128006\n",
      "batch: 16052, loss: 0.1933995485305786\n",
      "batch: 16053, loss: 0.06417296826839447\n",
      "batch: 16054, loss: 0.025893302634358406\n",
      "batch: 16055, loss: 0.13962331414222717\n",
      "batch: 16056, loss: 0.044085871428251266\n",
      "batch: 16057, loss: 0.057873841375112534\n",
      "batch: 16058, loss: 0.07481439411640167\n",
      "batch: 16059, loss: 0.1554897427558899\n",
      "batch: 16060, loss: 0.1068786010146141\n",
      "batch: 16061, loss: 0.15195956826210022\n",
      "batch: 16062, loss: 0.08115808665752411\n",
      "batch: 16063, loss: 0.06553459167480469\n",
      "batch: 16064, loss: 0.11068591475486755\n",
      "batch: 16065, loss: 0.04560787230730057\n",
      "batch: 16066, loss: 0.15434511005878448\n",
      "batch: 16067, loss: 0.10063646733760834\n",
      "batch: 16068, loss: 0.12776514887809753\n",
      "batch: 16069, loss: 0.19966945052146912\n",
      "batch: 16070, loss: 0.07150142639875412\n",
      "batch: 16071, loss: 0.16274042427539825\n",
      "batch: 16072, loss: 0.1128896027803421\n",
      "batch: 16073, loss: 0.03216395527124405\n",
      "batch: 16074, loss: 0.035840295255184174\n",
      "batch: 16075, loss: 0.04843996465206146\n",
      "batch: 16076, loss: 0.03639289736747742\n",
      "batch: 16077, loss: 0.13068899512290955\n",
      "batch: 16078, loss: 0.0507461279630661\n",
      "batch: 16079, loss: 0.21977181732654572\n",
      "batch: 16080, loss: 0.07744325697422028\n",
      "batch: 16081, loss: 0.13648812472820282\n",
      "batch: 16082, loss: 0.17704665660858154\n",
      "batch: 16083, loss: 0.13084977865219116\n",
      "batch: 16084, loss: 0.06195872649550438\n",
      "batch: 16085, loss: 0.1721733957529068\n",
      "batch: 16086, loss: 0.2005281001329422\n",
      "batch: 16087, loss: 0.07872740924358368\n",
      "batch: 16088, loss: 0.12701629102230072\n",
      "batch: 16089, loss: 0.061656903475522995\n",
      "batch: 16090, loss: 0.033877648413181305\n",
      "batch: 16091, loss: 0.12772433459758759\n",
      "batch: 16092, loss: 0.05302935838699341\n",
      "batch: 16093, loss: 0.2382231503725052\n",
      "batch: 16094, loss: 0.16700956225395203\n",
      "batch: 16095, loss: 0.16310103237628937\n",
      "batch: 16096, loss: 0.06760512292385101\n",
      "batch: 16097, loss: 0.055774517357349396\n",
      "batch: 16098, loss: 0.07954957336187363\n",
      "batch: 16099, loss: 0.12942491471767426\n",
      "batch: 16100, loss: 0.07831460982561111\n",
      "model saved to ./saving/model.ckpt-161\n",
      "batch: 16101, loss: 0.09479261189699173\n",
      "batch: 16102, loss: 0.09092256426811218\n",
      "batch: 16103, loss: 0.10594034940004349\n",
      "batch: 16104, loss: 0.10005282610654831\n",
      "batch: 16105, loss: 0.16319289803504944\n",
      "batch: 16106, loss: 0.10558722913265228\n",
      "batch: 16107, loss: 0.22184079885482788\n",
      "batch: 16108, loss: 0.13207639753818512\n",
      "batch: 16109, loss: 0.0414021760225296\n",
      "batch: 16110, loss: 0.21152478456497192\n",
      "batch: 16111, loss: 0.04537332057952881\n",
      "batch: 16112, loss: 0.10464482009410858\n",
      "batch: 16113, loss: 0.06035397946834564\n",
      "batch: 16114, loss: 0.09793507307767868\n",
      "batch: 16115, loss: 0.12486471980810165\n",
      "batch: 16116, loss: 0.12022146582603455\n",
      "batch: 16117, loss: 0.06019040197134018\n",
      "batch: 16118, loss: 0.1304786503314972\n",
      "batch: 16119, loss: 0.18622317910194397\n",
      "batch: 16120, loss: 0.08094625174999237\n",
      "batch: 16121, loss: 0.11185847222805023\n",
      "batch: 16122, loss: 0.05132676661014557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 16123, loss: 0.12801720201969147\n",
      "batch: 16124, loss: 0.1560162901878357\n",
      "batch: 16125, loss: 0.07510773837566376\n",
      "batch: 16126, loss: 0.14186586439609528\n",
      "batch: 16127, loss: 0.1003318578004837\n",
      "batch: 16128, loss: 0.05624959245324135\n",
      "batch: 16129, loss: 0.04077252000570297\n",
      "batch: 16130, loss: 0.23056456446647644\n",
      "batch: 16131, loss: 0.11861799657344818\n",
      "batch: 16132, loss: 0.060545969754457474\n",
      "batch: 16133, loss: 0.07347141206264496\n",
      "batch: 16134, loss: 0.036544088274240494\n",
      "batch: 16135, loss: 0.24504998326301575\n",
      "batch: 16136, loss: 0.12387961149215698\n",
      "batch: 16137, loss: 0.04759715124964714\n",
      "batch: 16138, loss: 0.20787501335144043\n",
      "batch: 16139, loss: 0.13777516782283783\n",
      "batch: 16140, loss: 0.0974886417388916\n",
      "batch: 16141, loss: 0.09694423526525497\n",
      "batch: 16142, loss: 0.037544362246990204\n",
      "batch: 16143, loss: 0.0923214852809906\n",
      "batch: 16144, loss: 0.08103849738836288\n",
      "batch: 16145, loss: 0.14598965644836426\n",
      "batch: 16146, loss: 0.12129972875118256\n",
      "batch: 16147, loss: 0.049572285264730453\n",
      "batch: 16148, loss: 0.1064518541097641\n",
      "batch: 16149, loss: 0.04323858022689819\n",
      "batch: 16150, loss: 0.027031775563955307\n",
      "batch: 16151, loss: 0.0753522515296936\n",
      "batch: 16152, loss: 0.07816051691770554\n",
      "batch: 16153, loss: 0.09076110273599625\n",
      "batch: 16154, loss: 0.12170574814081192\n",
      "batch: 16155, loss: 0.12607607245445251\n",
      "batch: 16156, loss: 0.06855373829603195\n",
      "batch: 16157, loss: 0.04597821831703186\n",
      "batch: 16158, loss: 0.0561775341629982\n",
      "batch: 16159, loss: 0.09122547507286072\n",
      "batch: 16160, loss: 0.14614292979240417\n",
      "batch: 16161, loss: 0.05809507891535759\n",
      "batch: 16162, loss: 0.18526823818683624\n",
      "batch: 16163, loss: 0.06244032829999924\n",
      "batch: 16164, loss: 0.17363709211349487\n",
      "batch: 16165, loss: 0.04023019224405289\n",
      "batch: 16166, loss: 0.047865793108940125\n",
      "batch: 16167, loss: 0.04710455238819122\n",
      "batch: 16168, loss: 0.041841112077236176\n",
      "batch: 16169, loss: 0.050919026136398315\n",
      "batch: 16170, loss: 0.08040875941514969\n",
      "batch: 16171, loss: 0.03316881135106087\n",
      "batch: 16172, loss: 0.2563573718070984\n",
      "batch: 16173, loss: 0.09159396588802338\n",
      "batch: 16174, loss: 0.11184753477573395\n",
      "batch: 16175, loss: 0.06765598058700562\n",
      "batch: 16176, loss: 0.03305335342884064\n",
      "batch: 16177, loss: 0.02010311186313629\n",
      "batch: 16178, loss: 0.1724429875612259\n",
      "batch: 16179, loss: 0.07698901742696762\n",
      "batch: 16180, loss: 0.11919255554676056\n",
      "batch: 16181, loss: 0.11480169743299484\n",
      "batch: 16182, loss: 0.09244366735219955\n",
      "batch: 16183, loss: 0.09858234971761703\n",
      "batch: 16184, loss: 0.06302249431610107\n",
      "batch: 16185, loss: 0.06663976609706879\n",
      "batch: 16186, loss: 0.0945073813199997\n",
      "batch: 16187, loss: 0.08650405704975128\n",
      "batch: 16188, loss: 0.10087351500988007\n",
      "batch: 16189, loss: 0.02757674641907215\n",
      "batch: 16190, loss: 0.04897402599453926\n",
      "batch: 16191, loss: 0.10277935862541199\n",
      "batch: 16192, loss: 0.04720938950777054\n",
      "batch: 16193, loss: 0.12189970910549164\n",
      "batch: 16194, loss: 0.05738005042076111\n",
      "batch: 16195, loss: 0.09560386091470718\n",
      "batch: 16196, loss: 0.12620511651039124\n",
      "batch: 16197, loss: 0.17246729135513306\n",
      "batch: 16198, loss: 0.11787012219429016\n",
      "batch: 16199, loss: 0.1643417328596115\n",
      "batch: 16200, loss: 0.11165009438991547\n",
      "model saved to ./saving/model.ckpt-162\n",
      "batch: 16201, loss: 0.14434081315994263\n",
      "batch: 16202, loss: 0.17513197660446167\n",
      "batch: 16203, loss: 0.10317960381507874\n",
      "batch: 16204, loss: 0.10018874704837799\n",
      "batch: 16205, loss: 0.15635299682617188\n",
      "batch: 16206, loss: 0.10327319800853729\n",
      "batch: 16207, loss: 0.08497104048728943\n",
      "batch: 16208, loss: 0.03258730098605156\n",
      "batch: 16209, loss: 0.10509775578975677\n",
      "batch: 16210, loss: 0.10759200155735016\n",
      "batch: 16211, loss: 0.19381356239318848\n",
      "batch: 16212, loss: 0.19551818072795868\n",
      "batch: 16213, loss: 0.23758256435394287\n",
      "batch: 16214, loss: 0.033264800906181335\n",
      "batch: 16215, loss: 0.11168377101421356\n",
      "batch: 16216, loss: 0.051903001964092255\n",
      "batch: 16217, loss: 0.0711151510477066\n",
      "batch: 16218, loss: 0.1649976670742035\n",
      "batch: 16219, loss: 0.15817777812480927\n",
      "batch: 16220, loss: 0.08506900072097778\n",
      "batch: 16221, loss: 0.0846962183713913\n",
      "batch: 16222, loss: 0.05005943775177002\n",
      "batch: 16223, loss: 0.10145510733127594\n",
      "batch: 16224, loss: 0.14066866040229797\n",
      "batch: 16225, loss: 0.06927227228879929\n",
      "batch: 16226, loss: 0.0643676295876503\n",
      "batch: 16227, loss: 0.13463279604911804\n",
      "batch: 16228, loss: 0.09969870746135712\n",
      "batch: 16229, loss: 0.12818020582199097\n",
      "batch: 16230, loss: 0.06386709213256836\n",
      "batch: 16231, loss: 0.2320071905851364\n",
      "batch: 16232, loss: 0.08790893852710724\n",
      "batch: 16233, loss: 0.08140937238931656\n",
      "batch: 16234, loss: 0.08878226578235626\n",
      "batch: 16235, loss: 0.06768293678760529\n",
      "batch: 16236, loss: 0.06483511626720428\n",
      "batch: 16237, loss: 0.12588191032409668\n",
      "batch: 16238, loss: 0.015141364187002182\n",
      "batch: 16239, loss: 0.09717467427253723\n",
      "batch: 16240, loss: 0.07448449730873108\n",
      "batch: 16241, loss: 0.08239005506038666\n",
      "batch: 16242, loss: 0.05635398253798485\n",
      "batch: 16243, loss: 0.08021096885204315\n",
      "batch: 16244, loss: 0.11968228220939636\n",
      "batch: 16245, loss: 0.0736003890633583\n",
      "batch: 16246, loss: 0.14852236211299896\n",
      "batch: 16247, loss: 0.16408421099185944\n",
      "batch: 16248, loss: 0.14933373034000397\n",
      "batch: 16249, loss: 0.11314725875854492\n",
      "batch: 16250, loss: 0.09206895530223846\n",
      "batch: 16251, loss: 0.12783989310264587\n",
      "batch: 16252, loss: 0.11486036330461502\n",
      "batch: 16253, loss: 0.05813328176736832\n",
      "batch: 16254, loss: 0.18095028400421143\n",
      "batch: 16255, loss: 0.09141146391630173\n",
      "batch: 16256, loss: 0.07001243531703949\n",
      "batch: 16257, loss: 0.023521002382040024\n",
      "batch: 16258, loss: 0.14108991622924805\n",
      "batch: 16259, loss: 0.04999760910868645\n",
      "batch: 16260, loss: 0.11219161748886108\n",
      "batch: 16261, loss: 0.10306763648986816\n",
      "batch: 16262, loss: 0.07291941344738007\n",
      "batch: 16263, loss: 0.18925997614860535\n",
      "batch: 16264, loss: 0.05926452577114105\n",
      "batch: 16265, loss: 0.08607229590415955\n",
      "batch: 16266, loss: 0.07550889253616333\n",
      "batch: 16267, loss: 0.13686522841453552\n",
      "batch: 16268, loss: 0.171751469373703\n",
      "batch: 16269, loss: 0.07501081377267838\n",
      "batch: 16270, loss: 0.10256143659353256\n",
      "batch: 16271, loss: 0.05815204232931137\n",
      "batch: 16272, loss: 0.03512529656291008\n",
      "batch: 16273, loss: 0.11696380376815796\n",
      "batch: 16274, loss: 0.09669697284698486\n",
      "batch: 16275, loss: 0.2010660618543625\n",
      "batch: 16276, loss: 0.1357211172580719\n",
      "batch: 16277, loss: 0.2566785514354706\n",
      "batch: 16278, loss: 0.15786346793174744\n",
      "batch: 16279, loss: 0.1689997762441635\n",
      "batch: 16280, loss: 0.03465896099805832\n",
      "batch: 16281, loss: 0.08571209013462067\n",
      "batch: 16282, loss: 0.18914389610290527\n",
      "batch: 16283, loss: 0.13513430953025818\n",
      "batch: 16284, loss: 0.08606453984975815\n",
      "batch: 16285, loss: 0.08392760157585144\n",
      "batch: 16286, loss: 0.0994112491607666\n",
      "batch: 16287, loss: 0.0723673403263092\n",
      "batch: 16288, loss: 0.1675616204738617\n",
      "batch: 16289, loss: 0.03403044492006302\n",
      "batch: 16290, loss: 0.04294572398066521\n",
      "batch: 16291, loss: 0.16808241605758667\n",
      "batch: 16292, loss: 0.0709688737988472\n",
      "batch: 16293, loss: 0.019089197739958763\n",
      "batch: 16294, loss: 0.10234154760837555\n",
      "batch: 16295, loss: 0.09998806565999985\n",
      "batch: 16296, loss: 0.11513961106538773\n",
      "batch: 16297, loss: 0.09215634316205978\n",
      "batch: 16298, loss: 0.11220645904541016\n",
      "batch: 16299, loss: 0.059494905173778534\n",
      "batch: 16300, loss: 0.1518758088350296\n",
      "model saved to ./saving/model.ckpt-163\n",
      "batch: 16301, loss: 0.059892937541007996\n",
      "batch: 16302, loss: 0.10580296069383621\n",
      "batch: 16303, loss: 0.12560734152793884\n",
      "batch: 16304, loss: 0.05830414220690727\n",
      "batch: 16305, loss: 0.03682884946465492\n",
      "batch: 16306, loss: 0.07799074053764343\n",
      "batch: 16307, loss: 0.024729115888476372\n",
      "batch: 16308, loss: 0.08249980211257935\n",
      "batch: 16309, loss: 0.054158493876457214\n",
      "batch: 16310, loss: 0.06185728311538696\n",
      "batch: 16311, loss: 0.11751149594783783\n",
      "batch: 16312, loss: 0.060988664627075195\n",
      "batch: 16313, loss: 0.08372170478105545\n",
      "batch: 16314, loss: 0.07307703793048859\n",
      "batch: 16315, loss: 0.18011799454689026\n",
      "batch: 16316, loss: 0.07407885789871216\n",
      "batch: 16317, loss: 0.09166128933429718\n",
      "batch: 16318, loss: 0.16804389655590057\n",
      "batch: 16319, loss: 0.05481770634651184\n",
      "batch: 16320, loss: 0.04894043505191803\n",
      "batch: 16321, loss: 0.14960137009620667\n",
      "batch: 16322, loss: 0.07518083602190018\n",
      "batch: 16323, loss: 0.07936851680278778\n",
      "batch: 16324, loss: 0.08491438627243042\n",
      "batch: 16325, loss: 0.08033416420221329\n",
      "batch: 16326, loss: 0.0442117415368557\n",
      "batch: 16327, loss: 0.07518219202756882\n",
      "batch: 16328, loss: 0.14082705974578857\n",
      "batch: 16329, loss: 0.15387386083602905\n",
      "batch: 16330, loss: 0.14102676510810852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 16331, loss: 0.0770532563328743\n",
      "batch: 16332, loss: 0.11271318793296814\n",
      "batch: 16333, loss: 0.10283634811639786\n",
      "batch: 16334, loss: 0.13374142348766327\n",
      "batch: 16335, loss: 0.08194156736135483\n",
      "batch: 16336, loss: 0.10021995007991791\n",
      "batch: 16337, loss: 0.20531317591667175\n",
      "batch: 16338, loss: 0.05963430926203728\n",
      "batch: 16339, loss: 0.02356889843940735\n",
      "batch: 16340, loss: 0.08773032575845718\n",
      "batch: 16341, loss: 0.144869863986969\n",
      "batch: 16342, loss: 0.027783378958702087\n",
      "batch: 16343, loss: 0.1268925815820694\n",
      "batch: 16344, loss: 0.09995894879102707\n",
      "batch: 16345, loss: 0.12113026529550552\n",
      "batch: 16346, loss: 0.11883515864610672\n",
      "batch: 16347, loss: 0.03764001280069351\n",
      "batch: 16348, loss: 0.12634915113449097\n",
      "batch: 16349, loss: 0.07079005986452103\n",
      "batch: 16350, loss: 0.04971779137849808\n",
      "batch: 16351, loss: 0.028478559106588364\n",
      "batch: 16352, loss: 0.05408691614866257\n",
      "batch: 16353, loss: 0.11175014078617096\n",
      "batch: 16354, loss: 0.29663345217704773\n",
      "batch: 16355, loss: 0.20993170142173767\n",
      "batch: 16356, loss: 0.16178663074970245\n",
      "batch: 16357, loss: 0.18990576267242432\n",
      "batch: 16358, loss: 0.07513940334320068\n",
      "batch: 16359, loss: 0.0810161828994751\n",
      "batch: 16360, loss: 0.10359451919794083\n",
      "batch: 16361, loss: 0.1743258684873581\n",
      "batch: 16362, loss: 0.08561071753501892\n",
      "batch: 16363, loss: 0.13558359444141388\n",
      "batch: 16364, loss: 0.09251689910888672\n",
      "batch: 16365, loss: 0.10971677303314209\n",
      "batch: 16366, loss: 0.05503103882074356\n",
      "batch: 16367, loss: 0.08736278116703033\n",
      "batch: 16368, loss: 0.10112398862838745\n",
      "batch: 16369, loss: 0.07621081173419952\n",
      "batch: 16370, loss: 0.050623927265405655\n",
      "batch: 16371, loss: 0.05196452885866165\n",
      "batch: 16372, loss: 0.06638685613870621\n",
      "batch: 16373, loss: 0.1640949696302414\n",
      "batch: 16374, loss: 0.02500336989760399\n",
      "batch: 16375, loss: 0.04947710782289505\n",
      "batch: 16376, loss: 0.08676306158304214\n",
      "batch: 16377, loss: 0.09238841384649277\n",
      "batch: 16378, loss: 0.030375413596630096\n",
      "batch: 16379, loss: 0.14618298411369324\n",
      "batch: 16380, loss: 0.08892090618610382\n",
      "batch: 16381, loss: 0.053019363433122635\n",
      "batch: 16382, loss: 0.055976174771785736\n",
      "batch: 16383, loss: 0.04217521846294403\n",
      "batch: 16384, loss: 0.07366111874580383\n",
      "batch: 16385, loss: 0.1474156230688095\n",
      "batch: 16386, loss: 0.056320056319236755\n",
      "batch: 16387, loss: 0.07159381359815598\n",
      "batch: 16388, loss: 0.07719290256500244\n",
      "batch: 16389, loss: 0.030237967148423195\n",
      "batch: 16390, loss: 0.039350830018520355\n",
      "batch: 16391, loss: 0.11866656690835953\n",
      "batch: 16392, loss: 0.06915934383869171\n",
      "batch: 16393, loss: 0.07238174229860306\n",
      "batch: 16394, loss: 0.07707749307155609\n",
      "batch: 16395, loss: 0.22578202188014984\n",
      "batch: 16396, loss: 0.05007593333721161\n",
      "batch: 16397, loss: 0.09080459177494049\n",
      "batch: 16398, loss: 0.23372505605220795\n",
      "batch: 16399, loss: 0.15894600749015808\n",
      "batch: 16400, loss: 0.22694584727287292\n",
      "model saved to ./saving/model.ckpt-164\n",
      "batch: 16401, loss: 0.1509823203086853\n",
      "batch: 16402, loss: 0.05123315751552582\n",
      "batch: 16403, loss: 0.050269946455955505\n",
      "batch: 16404, loss: 0.18075887858867645\n",
      "batch: 16405, loss: 0.18985748291015625\n",
      "batch: 16406, loss: 0.0855284333229065\n",
      "batch: 16407, loss: 0.14160794019699097\n",
      "batch: 16408, loss: 0.12379743158817291\n",
      "batch: 16409, loss: 0.057620566338300705\n",
      "batch: 16410, loss: 0.0697999894618988\n",
      "batch: 16411, loss: 0.0929868221282959\n",
      "batch: 16412, loss: 0.07798448204994202\n",
      "batch: 16413, loss: 0.018110089004039764\n",
      "batch: 16414, loss: 0.07595472037792206\n",
      "batch: 16415, loss: 0.08196073770523071\n",
      "batch: 16416, loss: 0.12026800215244293\n",
      "batch: 16417, loss: 0.04044536501169205\n",
      "batch: 16418, loss: 0.08764046430587769\n",
      "batch: 16419, loss: 0.04051116481423378\n",
      "batch: 16420, loss: 0.10358098149299622\n",
      "batch: 16421, loss: 0.18839243054389954\n",
      "batch: 16422, loss: 0.019497741013765335\n",
      "batch: 16423, loss: 0.07846967875957489\n",
      "batch: 16424, loss: 0.21510235965251923\n",
      "batch: 16425, loss: 0.08011798560619354\n",
      "batch: 16426, loss: 0.1764170527458191\n",
      "batch: 16427, loss: 0.04388627037405968\n",
      "batch: 16428, loss: 0.193291574716568\n",
      "batch: 16429, loss: 0.13959234952926636\n",
      "batch: 16430, loss: 0.06837330758571625\n",
      "batch: 16431, loss: 0.15414182841777802\n",
      "batch: 16432, loss: 0.03289984166622162\n",
      "batch: 16433, loss: 0.06595505774021149\n",
      "batch: 16434, loss: 0.08951713144779205\n",
      "batch: 16435, loss: 0.14199385046958923\n",
      "batch: 16436, loss: 0.046861693263053894\n",
      "batch: 16437, loss: 0.12054815143346786\n",
      "batch: 16438, loss: 0.24739637970924377\n",
      "batch: 16439, loss: 0.055106207728385925\n",
      "batch: 16440, loss: 0.06875063478946686\n",
      "batch: 16441, loss: 0.04938053712248802\n",
      "batch: 16442, loss: 0.058842506259679794\n",
      "batch: 16443, loss: 0.06484907865524292\n",
      "batch: 16444, loss: 0.05109420418739319\n",
      "batch: 16445, loss: 0.05825548619031906\n",
      "batch: 16446, loss: 0.033223964273929596\n",
      "batch: 16447, loss: 0.08124372363090515\n",
      "batch: 16448, loss: 0.05542031303048134\n",
      "batch: 16449, loss: 0.084271140396595\n",
      "batch: 16450, loss: 0.05563610792160034\n",
      "batch: 16451, loss: 0.10966867208480835\n",
      "batch: 16452, loss: 0.11217848211526871\n",
      "batch: 16453, loss: 0.07913807779550552\n",
      "batch: 16454, loss: 0.3197847902774811\n",
      "batch: 16455, loss: 0.06582159548997879\n",
      "batch: 16456, loss: 0.04124452546238899\n",
      "batch: 16457, loss: 0.07621677219867706\n",
      "batch: 16458, loss: 0.04765009507536888\n",
      "batch: 16459, loss: 0.3299579322338104\n",
      "batch: 16460, loss: 0.1286354511976242\n",
      "batch: 16461, loss: 0.15358658134937286\n",
      "batch: 16462, loss: 0.07547728717327118\n",
      "batch: 16463, loss: 0.09486724436283112\n",
      "batch: 16464, loss: 0.17415444552898407\n",
      "batch: 16465, loss: 0.13504666090011597\n",
      "batch: 16466, loss: 0.08781912922859192\n",
      "batch: 16467, loss: 0.02767823077738285\n",
      "batch: 16468, loss: 0.1528700441122055\n",
      "batch: 16469, loss: 0.2194715142250061\n",
      "batch: 16470, loss: 0.031002722680568695\n",
      "batch: 16471, loss: 0.04807765781879425\n",
      "batch: 16472, loss: 0.11641781777143478\n",
      "batch: 16473, loss: 0.037483543157577515\n",
      "batch: 16474, loss: 0.19039878249168396\n",
      "batch: 16475, loss: 0.042018771171569824\n",
      "batch: 16476, loss: 0.10257452726364136\n",
      "batch: 16477, loss: 0.08541667461395264\n",
      "batch: 16478, loss: 0.06192823126912117\n",
      "batch: 16479, loss: 0.13241395354270935\n",
      "batch: 16480, loss: 0.09950228035449982\n",
      "batch: 16481, loss: 0.07658112794160843\n",
      "batch: 16482, loss: 0.056524455547332764\n",
      "batch: 16483, loss: 0.06983453035354614\n",
      "batch: 16484, loss: 0.09538362920284271\n",
      "batch: 16485, loss: 0.1617717742919922\n",
      "batch: 16486, loss: 0.03884221613407135\n",
      "batch: 16487, loss: 0.13193771243095398\n",
      "batch: 16488, loss: 0.09980806708335876\n",
      "batch: 16489, loss: 0.09042466431856155\n",
      "batch: 16490, loss: 0.07027484476566315\n",
      "batch: 16491, loss: 0.07923918962478638\n",
      "batch: 16492, loss: 0.03905609995126724\n",
      "batch: 16493, loss: 0.04885426163673401\n",
      "batch: 16494, loss: 0.07145488262176514\n",
      "batch: 16495, loss: 0.07035435736179352\n",
      "batch: 16496, loss: 0.15619778633117676\n",
      "batch: 16497, loss: 0.09800361841917038\n",
      "batch: 16498, loss: 0.033086903393268585\n",
      "batch: 16499, loss: 0.07101660966873169\n",
      "batch: 16500, loss: 0.10788552463054657\n",
      "model saved to ./saving/model.ckpt-165\n",
      "batch: 16501, loss: 0.03740283101797104\n",
      "batch: 16502, loss: 0.1633557677268982\n",
      "batch: 16503, loss: 0.02381710708141327\n",
      "batch: 16504, loss: 0.0373014397919178\n",
      "batch: 16505, loss: 0.08681660890579224\n",
      "batch: 16506, loss: 0.07150565087795258\n",
      "batch: 16507, loss: 0.050494030117988586\n",
      "batch: 16508, loss: 0.09441731870174408\n",
      "batch: 16509, loss: 0.10508430004119873\n",
      "batch: 16510, loss: 0.11231165379285812\n",
      "batch: 16511, loss: 0.0454849973320961\n",
      "batch: 16512, loss: 0.12107877433300018\n",
      "batch: 16513, loss: 0.16963493824005127\n",
      "batch: 16514, loss: 0.19112548232078552\n",
      "batch: 16515, loss: 0.18135148286819458\n",
      "batch: 16516, loss: 0.08258472383022308\n",
      "batch: 16517, loss: 0.14957284927368164\n",
      "batch: 16518, loss: 0.11595441401004791\n",
      "batch: 16519, loss: 0.1857919692993164\n",
      "batch: 16520, loss: 0.07317589223384857\n",
      "batch: 16521, loss: 0.06434793770313263\n",
      "batch: 16522, loss: 0.06282903254032135\n",
      "batch: 16523, loss: 0.07431282848119736\n",
      "batch: 16524, loss: 0.10599540174007416\n",
      "batch: 16525, loss: 0.13717445731163025\n",
      "batch: 16526, loss: 0.12938040494918823\n",
      "batch: 16527, loss: 0.22291161119937897\n",
      "batch: 16528, loss: 0.08364474773406982\n",
      "batch: 16529, loss: 0.09675659984350204\n",
      "batch: 16530, loss: 0.1749609112739563\n",
      "batch: 16531, loss: 0.09557497501373291\n",
      "batch: 16532, loss: 0.0719129741191864\n",
      "batch: 16533, loss: 0.20484769344329834\n",
      "batch: 16534, loss: 0.18289992213249207\n",
      "batch: 16535, loss: 0.08352214097976685\n",
      "batch: 16536, loss: 0.09650138765573502\n",
      "batch: 16537, loss: 0.09218442440032959\n",
      "batch: 16538, loss: 0.048014819622039795\n",
      "batch: 16539, loss: 0.090298131108284\n",
      "batch: 16540, loss: 0.2022211253643036\n",
      "batch: 16541, loss: 0.07528214901685715\n",
      "batch: 16542, loss: 0.05213434249162674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 16543, loss: 0.048430535942316055\n",
      "batch: 16544, loss: 0.06573960930109024\n",
      "batch: 16545, loss: 0.036980438977479935\n",
      "batch: 16546, loss: 0.06068973243236542\n",
      "batch: 16547, loss: 0.11831261217594147\n",
      "batch: 16548, loss: 0.1342845857143402\n",
      "batch: 16549, loss: 0.041691698133945465\n",
      "batch: 16550, loss: 0.16485485434532166\n",
      "batch: 16551, loss: 0.04883544147014618\n",
      "batch: 16552, loss: 0.08924716711044312\n",
      "batch: 16553, loss: 0.04234666749835014\n",
      "batch: 16554, loss: 0.0523047149181366\n",
      "batch: 16555, loss: 0.1944560408592224\n",
      "batch: 16556, loss: 0.16423140466213226\n",
      "batch: 16557, loss: 0.21713782846927643\n",
      "batch: 16558, loss: 0.16986870765686035\n",
      "batch: 16559, loss: 0.031160490587353706\n",
      "batch: 16560, loss: 0.10991169512271881\n",
      "batch: 16561, loss: 0.1212160587310791\n",
      "batch: 16562, loss: 0.10292176902294159\n",
      "batch: 16563, loss: 0.09986409544944763\n",
      "batch: 16564, loss: 0.0897592231631279\n",
      "batch: 16565, loss: 0.13058722019195557\n",
      "batch: 16566, loss: 0.09883561730384827\n",
      "batch: 16567, loss: 0.2613949775695801\n",
      "batch: 16568, loss: 0.03034578077495098\n",
      "batch: 16569, loss: 0.1493016630411148\n",
      "batch: 16570, loss: 0.06883707642555237\n",
      "batch: 16571, loss: 0.04040447622537613\n",
      "batch: 16572, loss: 0.09487395733594894\n",
      "batch: 16573, loss: 0.10126262158155441\n",
      "batch: 16574, loss: 0.03780243545770645\n",
      "batch: 16575, loss: 0.12747201323509216\n",
      "batch: 16576, loss: 0.05435284227132797\n",
      "batch: 16577, loss: 0.20118437707424164\n",
      "batch: 16578, loss: 0.08863849937915802\n",
      "batch: 16579, loss: 0.19952797889709473\n",
      "batch: 16580, loss: 0.05205664783716202\n",
      "batch: 16581, loss: 0.03358909860253334\n",
      "batch: 16582, loss: 0.15575182437896729\n",
      "batch: 16583, loss: 0.11071192473173141\n",
      "batch: 16584, loss: 0.053496770560741425\n",
      "batch: 16585, loss: 0.05208032578229904\n",
      "batch: 16586, loss: 0.043449193239212036\n",
      "batch: 16587, loss: 0.2978273928165436\n",
      "batch: 16588, loss: 0.08486658334732056\n",
      "batch: 16589, loss: 0.07359534502029419\n",
      "batch: 16590, loss: 0.09926657378673553\n",
      "batch: 16591, loss: 0.12362738698720932\n",
      "batch: 16592, loss: 0.17641431093215942\n",
      "batch: 16593, loss: 0.1435021609067917\n",
      "batch: 16594, loss: 0.07800851762294769\n",
      "batch: 16595, loss: 0.0996379554271698\n",
      "batch: 16596, loss: 0.1160498633980751\n",
      "batch: 16597, loss: 0.17140325903892517\n",
      "batch: 16598, loss: 0.05032956600189209\n",
      "batch: 16599, loss: 0.05658364295959473\n",
      "batch: 16600, loss: 0.24756433069705963\n",
      "model saved to ./saving/model.ckpt-166\n",
      "batch: 16601, loss: 0.11314214766025543\n",
      "batch: 16602, loss: 0.09709871560335159\n",
      "batch: 16603, loss: 0.13485848903656006\n",
      "batch: 16604, loss: 0.04488642141222954\n",
      "batch: 16605, loss: 0.20121651887893677\n",
      "batch: 16606, loss: 0.14117774367332458\n",
      "batch: 16607, loss: 0.11844176799058914\n",
      "batch: 16608, loss: 0.07406505942344666\n",
      "batch: 16609, loss: 0.06965531408786774\n",
      "batch: 16610, loss: 0.0888003259897232\n",
      "batch: 16611, loss: 0.03329034894704819\n",
      "batch: 16612, loss: 0.1531175971031189\n",
      "batch: 16613, loss: 0.06640394777059555\n",
      "batch: 16614, loss: 0.04399440065026283\n",
      "batch: 16615, loss: 0.029232829809188843\n",
      "batch: 16616, loss: 0.0732213705778122\n",
      "batch: 16617, loss: 0.09800800681114197\n",
      "batch: 16618, loss: 0.20065703988075256\n",
      "batch: 16619, loss: 0.07826866209506989\n",
      "batch: 16620, loss: 0.12772823870182037\n",
      "batch: 16621, loss: 0.09365801513195038\n",
      "batch: 16622, loss: 0.11123307794332504\n",
      "batch: 16623, loss: 0.0915142297744751\n",
      "batch: 16624, loss: 0.10120096802711487\n",
      "batch: 16625, loss: 0.09949713945388794\n",
      "batch: 16626, loss: 0.11623822897672653\n",
      "batch: 16627, loss: 0.045224856585264206\n",
      "batch: 16628, loss: 0.09881322085857391\n",
      "batch: 16629, loss: 0.1608363687992096\n",
      "batch: 16630, loss: 0.05739245563745499\n",
      "batch: 16631, loss: 0.12183402478694916\n",
      "batch: 16632, loss: 0.11658290773630142\n",
      "batch: 16633, loss: 0.038054630160331726\n",
      "batch: 16634, loss: 0.08155477792024612\n",
      "batch: 16635, loss: 0.22360223531723022\n",
      "batch: 16636, loss: 0.10021385550498962\n",
      "batch: 16637, loss: 0.02112027257680893\n",
      "batch: 16638, loss: 0.07069012522697449\n",
      "batch: 16639, loss: 0.0446234866976738\n",
      "batch: 16640, loss: 0.08459719270467758\n",
      "batch: 16641, loss: 0.0856764018535614\n",
      "batch: 16642, loss: 0.08363842219114304\n",
      "batch: 16643, loss: 0.05703729763627052\n",
      "batch: 16644, loss: 0.06587333977222443\n",
      "batch: 16645, loss: 0.06858941912651062\n",
      "batch: 16646, loss: 0.07521028816699982\n",
      "batch: 16647, loss: 0.11705608665943146\n",
      "batch: 16648, loss: 0.13066986203193665\n",
      "batch: 16649, loss: 0.0263630710542202\n",
      "batch: 16650, loss: 0.15280906856060028\n",
      "batch: 16651, loss: 0.2083577811717987\n",
      "batch: 16652, loss: 0.08957382291555405\n",
      "batch: 16653, loss: 0.03602961450815201\n",
      "batch: 16654, loss: 0.20781365036964417\n",
      "batch: 16655, loss: 0.07642436027526855\n",
      "batch: 16656, loss: 0.04807040095329285\n",
      "batch: 16657, loss: 0.28842100501060486\n",
      "batch: 16658, loss: 0.048689305782318115\n",
      "batch: 16659, loss: 0.10099802911281586\n",
      "batch: 16660, loss: 0.11071918904781342\n",
      "batch: 16661, loss: 0.08462631702423096\n",
      "batch: 16662, loss: 0.10025100409984589\n",
      "batch: 16663, loss: 0.21276314556598663\n",
      "batch: 16664, loss: 0.14612612128257751\n",
      "batch: 16665, loss: 0.04606195539236069\n",
      "batch: 16666, loss: 0.11560886353254318\n",
      "batch: 16667, loss: 0.11144350469112396\n",
      "batch: 16668, loss: 0.08099305629730225\n",
      "batch: 16669, loss: 0.073869988322258\n",
      "batch: 16670, loss: 0.0935518890619278\n",
      "batch: 16671, loss: 0.1495051234960556\n",
      "batch: 16672, loss: 0.09011293947696686\n",
      "batch: 16673, loss: 0.0733630359172821\n",
      "batch: 16674, loss: 0.05009901896119118\n",
      "batch: 16675, loss: 0.07423542439937592\n",
      "batch: 16676, loss: 0.035310085862874985\n",
      "batch: 16677, loss: 0.07548396289348602\n",
      "batch: 16678, loss: 0.09074316918849945\n",
      "batch: 16679, loss: 0.07631298154592514\n",
      "batch: 16680, loss: 0.0898379236459732\n",
      "batch: 16681, loss: 0.15885639190673828\n",
      "batch: 16682, loss: 0.07500883936882019\n",
      "batch: 16683, loss: 0.09679855406284332\n",
      "batch: 16684, loss: 0.11008349806070328\n",
      "batch: 16685, loss: 0.08898112922906876\n",
      "batch: 16686, loss: 0.10095786303281784\n",
      "batch: 16687, loss: 0.058687999844551086\n",
      "batch: 16688, loss: 0.09915095567703247\n",
      "batch: 16689, loss: 0.08096902072429657\n",
      "batch: 16690, loss: 0.08224915713071823\n",
      "batch: 16691, loss: 0.0833706483244896\n",
      "batch: 16692, loss: 0.07349151372909546\n",
      "batch: 16693, loss: 0.1441061794757843\n",
      "batch: 16694, loss: 0.07205665856599808\n",
      "batch: 16695, loss: 0.08886867761611938\n",
      "batch: 16696, loss: 0.06003586947917938\n",
      "batch: 16697, loss: 0.10705797374248505\n",
      "batch: 16698, loss: 0.047358326613903046\n",
      "batch: 16699, loss: 0.06017496436834335\n",
      "batch: 16700, loss: 0.22111093997955322\n",
      "model saved to ./saving/model.ckpt-167\n",
      "batch: 16701, loss: 0.0881885215640068\n",
      "batch: 16702, loss: 0.08936942368745804\n",
      "batch: 16703, loss: 0.10116265714168549\n",
      "batch: 16704, loss: 0.05571343004703522\n",
      "batch: 16705, loss: 0.10321862995624542\n",
      "batch: 16706, loss: 0.08265462517738342\n",
      "batch: 16707, loss: 0.17403361201286316\n",
      "batch: 16708, loss: 0.12058906257152557\n",
      "batch: 16709, loss: 0.08337487280368805\n",
      "batch: 16710, loss: 0.11358602344989777\n",
      "batch: 16711, loss: 0.0586014986038208\n",
      "batch: 16712, loss: 0.05061768367886543\n",
      "batch: 16713, loss: 0.12140564620494843\n",
      "batch: 16714, loss: 0.07759204506874084\n",
      "batch: 16715, loss: 0.11512710154056549\n",
      "batch: 16716, loss: 0.058507390320301056\n",
      "batch: 16717, loss: 0.22683532536029816\n",
      "batch: 16718, loss: 0.07685019075870514\n",
      "batch: 16719, loss: 0.05073172226548195\n",
      "batch: 16720, loss: 0.09564375877380371\n",
      "batch: 16721, loss: 0.025909965857863426\n",
      "batch: 16722, loss: 0.02571599744260311\n",
      "batch: 16723, loss: 0.03502247482538223\n",
      "batch: 16724, loss: 0.1028866171836853\n",
      "batch: 16725, loss: 0.0658160075545311\n",
      "batch: 16726, loss: 0.026713598519563675\n",
      "batch: 16727, loss: 0.07100202143192291\n",
      "batch: 16728, loss: 0.13099050521850586\n",
      "batch: 16729, loss: 0.10362767428159714\n",
      "batch: 16730, loss: 0.10350331664085388\n",
      "batch: 16731, loss: 0.024992510676383972\n",
      "batch: 16732, loss: 0.05527246743440628\n",
      "batch: 16733, loss: 0.06855663657188416\n",
      "batch: 16734, loss: 0.037624992430210114\n",
      "batch: 16735, loss: 0.06164824217557907\n",
      "batch: 16736, loss: 0.05741828307509422\n",
      "batch: 16737, loss: 0.1779947578907013\n",
      "batch: 16738, loss: 0.10295505076646805\n",
      "batch: 16739, loss: 0.05851002410054207\n",
      "batch: 16740, loss: 0.09051252156496048\n",
      "batch: 16741, loss: 0.042818713933229446\n",
      "batch: 16742, loss: 0.11975888162851334\n",
      "batch: 16743, loss: 0.038055963814258575\n",
      "batch: 16744, loss: 0.09529095143079758\n",
      "batch: 16745, loss: 0.13759683072566986\n",
      "batch: 16746, loss: 0.04358990862965584\n",
      "batch: 16747, loss: 0.03174269571900368\n",
      "batch: 16748, loss: 0.12719467282295227\n",
      "batch: 16749, loss: 0.19542044401168823\n",
      "batch: 16750, loss: 0.05798215791583061\n",
      "batch: 16751, loss: 0.08582061529159546\n",
      "batch: 16752, loss: 0.12905628979206085\n",
      "batch: 16753, loss: 0.0918310210108757\n",
      "batch: 16754, loss: 0.22650015354156494\n",
      "batch: 16755, loss: 0.06979101896286011\n",
      "batch: 16756, loss: 0.06887876987457275\n",
      "batch: 16757, loss: 0.18374739587306976\n",
      "batch: 16758, loss: 0.11071046441793442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 16759, loss: 0.061570197343826294\n",
      "batch: 16760, loss: 0.12281922250986099\n",
      "batch: 16761, loss: 0.0657401755452156\n",
      "batch: 16762, loss: 0.08856354653835297\n",
      "batch: 16763, loss: 0.10530219972133636\n",
      "batch: 16764, loss: 0.11981216818094254\n",
      "batch: 16765, loss: 0.11481578648090363\n",
      "batch: 16766, loss: 0.1364961862564087\n",
      "batch: 16767, loss: 0.16212385892868042\n",
      "batch: 16768, loss: 0.03272957354784012\n",
      "batch: 16769, loss: 0.09914644062519073\n",
      "batch: 16770, loss: 0.043876856565475464\n",
      "batch: 16771, loss: 0.12192586809396744\n",
      "batch: 16772, loss: 0.1432906538248062\n",
      "batch: 16773, loss: 0.14183828234672546\n",
      "batch: 16774, loss: 0.08338013291358948\n",
      "batch: 16775, loss: 0.041800606995821\n",
      "batch: 16776, loss: 0.06965514272451401\n",
      "batch: 16777, loss: 0.07916810363531113\n",
      "batch: 16778, loss: 0.038136862218379974\n",
      "batch: 16779, loss: 0.058261506259441376\n",
      "batch: 16780, loss: 0.05610690638422966\n",
      "batch: 16781, loss: 0.1136297658085823\n",
      "batch: 16782, loss: 0.35295096039772034\n",
      "batch: 16783, loss: 0.15742653608322144\n",
      "batch: 16784, loss: 0.09197299927473068\n",
      "batch: 16785, loss: 0.1340312659740448\n",
      "batch: 16786, loss: 0.12427777051925659\n",
      "batch: 16787, loss: 0.060624316334724426\n",
      "batch: 16788, loss: 0.07041596621274948\n",
      "batch: 16789, loss: 0.13636764883995056\n",
      "batch: 16790, loss: 0.0399496927857399\n",
      "batch: 16791, loss: 0.0621621236205101\n",
      "batch: 16792, loss: 0.0625845342874527\n",
      "batch: 16793, loss: 0.0787610113620758\n",
      "batch: 16794, loss: 0.03058374673128128\n",
      "batch: 16795, loss: 0.03094305284321308\n",
      "batch: 16796, loss: 0.09871628880500793\n",
      "batch: 16797, loss: 0.14224465191364288\n",
      "batch: 16798, loss: 0.05834316462278366\n",
      "batch: 16799, loss: 0.09901167452335358\n",
      "batch: 16800, loss: 0.03740832209587097\n",
      "model saved to ./saving/model.ckpt-168\n",
      "batch: 16801, loss: 0.059572748839855194\n",
      "batch: 16802, loss: 0.08763401210308075\n",
      "batch: 16803, loss: 0.1038517877459526\n",
      "batch: 16804, loss: 0.058866601437330246\n",
      "batch: 16805, loss: 0.083278127014637\n",
      "batch: 16806, loss: 0.13760684430599213\n",
      "batch: 16807, loss: 0.20551195740699768\n",
      "batch: 16808, loss: 0.05076543986797333\n",
      "batch: 16809, loss: 0.2117953896522522\n",
      "batch: 16810, loss: 0.06193029135465622\n",
      "batch: 16811, loss: 0.03839368373155594\n",
      "batch: 16812, loss: 0.13231897354125977\n",
      "batch: 16813, loss: 0.049126897007226944\n",
      "batch: 16814, loss: 0.08240290731191635\n",
      "batch: 16815, loss: 0.023059025406837463\n",
      "batch: 16816, loss: 0.09700924903154373\n",
      "batch: 16817, loss: 0.07256840914487839\n",
      "batch: 16818, loss: 0.09093473851680756\n",
      "batch: 16819, loss: 0.1393299102783203\n",
      "batch: 16820, loss: 0.09173042327165604\n",
      "batch: 16821, loss: 0.05009754002094269\n",
      "batch: 16822, loss: 0.10095161199569702\n",
      "batch: 16823, loss: 0.04458905756473541\n",
      "batch: 16824, loss: 0.06631333380937576\n",
      "batch: 16825, loss: 0.09719586372375488\n",
      "batch: 16826, loss: 0.050014927983284\n",
      "batch: 16827, loss: 0.10467919707298279\n",
      "batch: 16828, loss: 0.06129876524209976\n",
      "batch: 16829, loss: 0.06826522946357727\n",
      "batch: 16830, loss: 0.09861145913600922\n",
      "batch: 16831, loss: 0.03882664814591408\n",
      "batch: 16832, loss: 0.0919894427061081\n",
      "batch: 16833, loss: 0.040707461535930634\n",
      "batch: 16834, loss: 0.11368872225284576\n",
      "batch: 16835, loss: 0.09528491646051407\n",
      "batch: 16836, loss: 0.10704634338617325\n",
      "batch: 16837, loss: 0.052556879818439484\n",
      "batch: 16838, loss: 0.03690781071782112\n",
      "batch: 16839, loss: 0.07237891852855682\n",
      "batch: 16840, loss: 0.07993634790182114\n",
      "batch: 16841, loss: 0.10483800619840622\n",
      "batch: 16842, loss: 0.045083291828632355\n",
      "batch: 16843, loss: 0.1046953946352005\n",
      "batch: 16844, loss: 0.08212612569332123\n",
      "batch: 16845, loss: 0.09041576087474823\n",
      "batch: 16846, loss: 0.07532573491334915\n",
      "batch: 16847, loss: 0.048543866723775864\n",
      "batch: 16848, loss: 0.11538716405630112\n",
      "batch: 16849, loss: 0.03945179283618927\n",
      "batch: 16850, loss: 0.054023854434490204\n",
      "batch: 16851, loss: 0.13395820558071136\n",
      "batch: 16852, loss: 0.055267781019210815\n",
      "batch: 16853, loss: 0.15358251333236694\n",
      "batch: 16854, loss: 0.24172693490982056\n",
      "batch: 16855, loss: 0.055154960602521896\n",
      "batch: 16856, loss: 0.1134583130478859\n",
      "batch: 16857, loss: 0.05188070237636566\n",
      "batch: 16858, loss: 0.17402447760105133\n",
      "batch: 16859, loss: 0.11754749715328217\n",
      "batch: 16860, loss: 0.05739385262131691\n",
      "batch: 16861, loss: 0.09789969027042389\n",
      "batch: 16862, loss: 0.16468095779418945\n",
      "batch: 16863, loss: 0.04676191508769989\n",
      "batch: 16864, loss: 0.17294874787330627\n",
      "batch: 16865, loss: 0.0633116364479065\n",
      "batch: 16866, loss: 0.07546409219503403\n",
      "batch: 16867, loss: 0.12490193545818329\n",
      "batch: 16868, loss: 0.030291855335235596\n",
      "batch: 16869, loss: 0.0377042219042778\n",
      "batch: 16870, loss: 0.1259072721004486\n",
      "batch: 16871, loss: 0.042965006083250046\n",
      "batch: 16872, loss: 0.12793613970279694\n",
      "batch: 16873, loss: 0.08332410454750061\n",
      "batch: 16874, loss: 0.09456564486026764\n",
      "batch: 16875, loss: 0.1063086986541748\n",
      "batch: 16876, loss: 0.05783093720674515\n",
      "batch: 16877, loss: 0.10075916349887848\n",
      "batch: 16878, loss: 0.035324495285749435\n",
      "batch: 16879, loss: 0.0392262302339077\n",
      "batch: 16880, loss: 0.1347205638885498\n",
      "batch: 16881, loss: 0.10167653858661652\n",
      "batch: 16882, loss: 0.1705651581287384\n",
      "batch: 16883, loss: 0.04324520006775856\n",
      "batch: 16884, loss: 0.08632822334766388\n",
      "batch: 16885, loss: 0.09343597292900085\n",
      "batch: 16886, loss: 0.14666804671287537\n",
      "batch: 16887, loss: 0.037452906370162964\n",
      "batch: 16888, loss: 0.1603592038154602\n",
      "batch: 16889, loss: 0.1449645310640335\n",
      "batch: 16890, loss: 0.08748480677604675\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9b5ce226deef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 开始训练模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-9fe1916bb59a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(lr, batch_size, epoches, model_path, data_loader)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0;31m# TF2 Keras中有两个与交叉熵相关的损失函数：\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;31m# tf.keras.losses.categorical_crossentropy和tf.keras.losses.sparse_categorical_crossentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wy.yang/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-0f14a24ab80d>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wy.yang/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wy.yang/venv/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    660\u001b[0m       \u001b[0;31m# Performance: Using `constant_op` is much faster than passing a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m       \u001b[0mflattened_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wy.yang/venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   8219\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   8220\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Reshape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8221\u001b[0;31m         tld.op_callbacks, tensor, shape)\n\u001b[0m\u001b[1;32m   8222\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8223\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 开始训练模型\n",
    "train(learning_rate, batch_size, num_epoches, model_path, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.9665\n"
     ]
    }
   ],
   "source": [
    "# 测试模型结果\n",
    "test(model_root, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# GPU设置显存使用策略\n",
    "# TF2提供两种显存使用策略，让我们能够更灵活地控制程序地显存使用方式：\n",
    "# (1)仅在需要时申请显存空间(程序初始运行时消耗很少地显存，随着程序的运行而动态申请显存)\n",
    "gpus = tf.config.list_physical_devices(device_type='GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(device=gpu, enable=True)\n",
    "\n",
    "# (2)限制消耗固定大小的显存(程序不会超出限定的显存大小，若超出则报错)\n",
    "tf.config.set_logical_device_configuration(gpus[0], [tf.config.LogicalDeviceConfiguration(memory_limit=y1024)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "num_epochs = 1\n",
    "batch_size = 50\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # MNIST中的图像默认为uint8（0-255的数字）。以下代码将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        # 从数据集中随机取出batch_size个元素并返回\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 1s 1ms/step - loss: 0.2943 - sparse_categorical_accuracy: 0.9179\n",
      "WARNING:tensorflow:From /home/opt/wy.yang/venv/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/opt/wy.yang/venv/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: saved/1/assets\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(100, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    "data_loader = MNISTLoader()\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=[tf.keras.metrics.sparse_categorical_accuracy]\n",
    ")\n",
    "model.fit(data_loader.train_data, data_loader.train_label, epochs=num_epochs, batch_size=batch_size)\n",
    "tf.saved_model.save(model, \"saved/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.949700\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "\n",
    "model = tf.saved_model.load(\"saved/1\")\n",
    "data_loader = MNISTLoader()\n",
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model(data_loader.test_data[start_index: end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]],\n",
       "\n",
       "\n",
       "       [[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]],\n",
       "\n",
       "\n",
       "       [[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.test_data[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(data_loader.test_data[0:3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
