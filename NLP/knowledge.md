## 常见知识点

1. 激活函数比较（sigmoid、tanh、RELU）

   一般tanh比sigmoid效果好一点(简单说明下，两者很类似，**tanh是rescaled的sigmoid，sigmoid输出都为正数，根据BP规则，某层的神经元的权重的梯度的符号和后层误差的一样**，也就是说，**如果后一层的误差为正，则这一层的权重全部都要降低，如果为负，则这一层梯度全部为负，权重全部增加，权重要么都增加，要么都减少，这明显是有问题的；tanh是以0为对称中心的，这会消除在权重更新时的系统偏差导致的偏向性**。当然这是启发式的，并不是说tanh一定比sigmoid的好)，ReLU也是很好的选择，最大的好处是，当tanh和sigmoid饱和时都会有梯度消失的问题，ReLU就不会有这个问题，而且计算简单，当然它会产生dead neurons。

2. Dropout的理解

   - **一方面缓解过拟合，另一方面引入的随机性，可以平缓训练过程，加速训练过程，处理outliers**。
   - **Dropout可以看做ensemble，特征采样，相当于bagging很多子网络**；训练过程中动态扩展拥有类似variation的输入数据集。（在单层网络中，类似折中Naive bayes(所有特征权重独立)和logistic regression(所有特征之间有关系)；
   - 一般对于越复杂的大规模网络，Dropout效果越好，是一个强regularizer！

3. 梯度消失和梯度爆炸

   **梯度消失**：本质上由于激活函数导致的，以sigmoid函数为例，它在两端时都趋于饱和，函数求导结果很小，向后传播时因为链式法则使得多次梯度乘积越来越小，最终产生梯度消失。

   **梯度爆炸**：出现在激活函数的激活区，而且权重W过大的情况下。相对来说梯度爆炸情况较少。

4. 参数更新方法

   ![update](/Users/yangwenyan/Documents/gitproject/Technology-Accumulation/NLP/pic/update.png)

5. 池化的作用

   卷积虽然可以大范围减少输出尺寸（特征数），但是依然较难计算并且很容易过拟合，所以利用图片的静态特性通过池化的方式进一步减少尺寸。
=======
3. Attention分类

   ![attentions](/Users/maciel/Documents/gitprojet/Technology-Accumulation/NLP/pic/attentions.png)

4. 为什么Dropout能够解决过拟合？

   * **取平均的效果**。dropout 掉不同的隐藏神经元就类似在训练不同的网络（随机删掉一半隐藏神经元导致网络结构已经不同)，整个dropout过程就相当于 对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。类似于bagging很多子网络。
   * **减少神经元之间复杂的共适应关系：** 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。（这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况）。 迫使网络去学习更加鲁棒的特征 （这些特征在其它的神经元的随机子集中也存在）。

5. dropout和Bagging有何不同？

   - 在 Bagging 的情况下，所有模型都是独立的；而在 Dropout 的情况下，所有模型**共享参数**，其中每个模型继承父神经网络参数的不同子集。
   - 在 Bagging 的情况下，每一个模型都会在其相应训练集上训练到收敛。而在 Dropout 的情况下，通常大部分模型都没有显式地被训练；取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。

