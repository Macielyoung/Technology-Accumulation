## 常见知识点

1. 激活函数比较（sigmoid、tanh、RELU）

   一般tanh比sigmoid效果好一点(简单说明下，两者很类似，**tanh是rescaled的sigmoid，sigmoid输出都为正数，根据BP规则，某层的神经元的权重的梯度的符号和后层误差的一样**，也就是说，**如果后一层的误差为正，则这一层的权重全部都要降低，如果为负，则这一层梯度全部为负，权重全部增加，权重要么都增加，要么都减少，这明显是有问题的；tanh是以0为对称中心的，这会消除在权重更新时的系统偏差导致的偏向性**。当然这是启发式的，并不是说tanh一定比sigmoid的好)，ReLU也是很好的选择，最大的好处是，当tanh和sigmoid饱和时都会有梯度消失的问题，ReLU就不会有这个问题，而且计算简单，当然它会产生dead neurons。

2. Dropout的理解

   - **一方面缓解过拟合，另一方面引入的随机性，可以平缓训练过程，加速训练过程，处理outliers**。
   - **Dropout可以看做ensemble，特征采样，相当于bagging很多子网络**；训练过程中动态扩展拥有类似variation的输入数据集。（在单层网络中，类似折中Naive bayes(所有特征权重独立)和logistic regression(所有特征之间有关系)；
   - 一般对于越复杂的大规模网络，Dropout效果越好，是一个强regularizer！

3. 梯度消失和梯度爆炸

   **梯度消失**：本质上由于激活函数导致的，以sigmoid函数为例，它在两端时都趋于饱和，函数求导结果很小，向后传播时因为链式法则使得多次梯度乘积越来越小，最终产生梯度消失。

   **梯度爆炸**：出现在激活函数的激活区，而且权重W过大的情况下。相对来说梯度爆炸情况较少。

4. 参数更新方法

   ![update](/Users/yangwenyan/Documents/gitproject/Technology-Accumulation/NLP/pic/update.png)

5. 池化的作用

   卷积虽然可以大范围减少输出尺寸（特征数），但是依然较难计算并且很容易过拟合，所以利用图片的静态特性通过池化的方式进一步减少尺寸。

3. Attention分类

   ![attentions](/Users/maciel/Documents/gitprojet/Technology-Accumulation/NLP/pic/attentions.png)

4. 为什么Dropout能够解决过拟合？

   * **取平均的效果**。dropout 掉不同的隐藏神经元就类似在训练不同的网络（随机删掉一半隐藏神经元导致网络结构已经不同)，整个dropout过程就相当于 对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。类似于bagging很多子网络。
   * **减少神经元之间复杂的共适应关系：** 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。（这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况）。 迫使网络去学习更加鲁棒的特征 （这些特征在其它的神经元的随机子集中也存在）。

5. dropout和Bagging有何不同？

   - 在 Bagging 的情况下，所有模型都是独立的；而在 Dropout 的情况下，所有模型**共享参数**，其中每个模型继承父神经网络参数的不同子集。
   - 在 Bagging 的情况下，每一个模型都会在其相应训练集上训练到收敛。而在 Dropout 的情况下，通常大部分模型都没有显式地被训练；取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。

9. 什么时候使用BN和LN？

   * 一般在深层网络中使用，比如在深层ResNet中使用BN，深层Transformer中使用LN。
   * BN与LN应用在非线性映射前效果更佳。
   * 当发现网络训练速度比较慢，梯度消失、爆炸等问题时，可以考虑BN或LN。
   * 使用BN时，对batch size的选择要求较高，且在训练前需要对数据进行shuffle。

10. 为何L1和L2正则化可以防止过拟合？

    L1和L2正则化会让模型偏向于更小的权重。一般来说，更小的权重意味着更低的模型复杂度，就是对训练数据的拟合刚刚好，不会过分拟合训练数据（比如噪声、异常值等）以提高模型的泛化能力。

    此外，添加正则化相当于为模型添加了某种限制，规定了参数的分布，从而降低了模型的复杂度。模型的复杂度降低，意味着模型对于噪声与异常点的抗干扰性的能力增强，从而提高模型的泛化能力。 -- **奥卡姆剃刀原理**

11. CNN中卷积层为什么需要padding

    卷积操作如果没有padding，这会带来两个问题：

    - 每一次做卷积操作时，你的图像就会缩小，如果这种情况发生多次，你的图像就会变得很小。
    - 边缘的像素点所受到的关注点比中心的关注点少很多。比如上图的 `1 * 1` 的像素点只进行了一次卷积计算，而中心点 `3 * 3` 却进行了9次卷积计算，这明显是不公平的。这意味着图像边缘的信息大多都丢失了。

    因此padding存在的意义在于：

    - 为了不丢弃原图信息
    - 为了保持feature map 的大小与原图一致
    - 为了让更深层的layer的 input 依旧保持有足够大的信息量
    - 为了实现上述目的，且不做多余的事情，padding出来的pixel的值都是0，不存在噪音问题。

12. 为什么卷积核设计的尺寸都是奇数？

    - 保证像素点中心位置，避免位置信息偏移
    - 填充边缘时能保证两边都能填充，原矩阵依然对称

13. 1*1卷积核的作用

    - 实现信息的跨通道交互和整合。
    - 对卷积核通道数进行降维和升维，减小参数量。

14. 卷积中不同零填充。

    假定 `m， k` 分别代表图像的宽度和卷积核的宽度：

    - Valid 卷积(有效卷积、债卷积)：不使用零填充，卷积核只允许访问那些图像中能够完全包含整个核

      的位置，输出的宽度为m − k + 1。

      - 在这种情况下，输出的所有像素都是输入中相同数量像素的函数，这使得输出像素的表示更加规范。
      - 然而，输出的大小在每一层都会缩减，这限制了网络中能够包含的卷积层的层数。（一般情况下，影响不大，除非是上百层的网络）

    - Same 卷积(相同卷积、宽卷积)：**只进行足够的零填充来**保持输出和输入具有相同的大小，即输出的宽度为m。

      - 在这种情况下，只要硬件支持，网络就能包含任意多的卷积层。
      - 然而，输入像素中靠近边界的部分相比于中间部分对于输出像素的影响更小。这可能会导致边界像素存在一定程度的欠表示。

    - **Full 卷积（全卷积）：**进行足够多的零填充使得每个像素都能被访问 k 次（非全卷积只有中间的像素能被访问 k 次），最终输出图像的宽度为m + k − 1

      - 因为 same 卷积可能导致边界像素欠表示，从而出现了 Full 卷积；
      - 但是在这种情况下，输出像素中靠近边界的部分相比于中间部分是更少像素的函数。这将导致**学得的卷积核不能再所有所有位置表现一致**。
      - 事实上，很少使用 Full 卷积

15. 