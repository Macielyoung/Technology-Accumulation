## 常见知识点

1. 激活函数比较（sigmoid、tanh、RELU）

   一般tanh比sigmoid效果好一点(简单说明下，两者很类似，**tanh是rescaled的sigmoid，sigmoid输出都为正数，根据BP规则，某层的神经元的权重的梯度的符号和后层误差的一样**，也就是说，**如果后一层的误差为正，则这一层的权重全部都要降低，如果为负，则这一层梯度全部为负，权重全部增加，权重要么都增加，要么都减少，这明显是有问题的；tanh是以0为对称中心的，这会消除在权重更新时的系统偏差导致的偏向性**。当然这是启发式的，并不是说tanh一定比sigmoid的好)，ReLU也是很好的选择，最大的好处是，当tanh和sigmoid饱和时都会有梯度消失的问题，ReLU就不会有这个问题，而且计算简单，当然它会产生dead neurons。

2. Dropout的理解

   - **一方面缓解过拟合，另一方面引入的随机性，可以平缓训练过程，加速训练过程，处理outliers**
   - **Dropout可以看做ensemble，特征采样，相当于bagging很多子网络**；训练过程中动态扩展拥有类似variation的输入数据集。（在单层网络中，类似折中Naive bayes(所有特征权重独立)和logistic regression(所有特征之间有关系)；
   - 一般对于越复杂的大规模网络，Dropout效果越好，是一个强regularizer！

3. 梯度消失和梯度爆炸

   **梯度消失**：本质上由于激活函数导致的，以sigmoid函数为例，它在两端时都趋于饱和，函数求导结果很小，向后传播时因为链式法则使得多次梯度乘积越来越小，最终产生梯度消失。

   **梯度爆炸**：出现在激活函数的激活区，而且权重W过大的情况下。相对来说梯度爆炸情况较少。

4. 参数更新方法

   ![update](/Users/yangwenyan/Documents/gitproject/Technology-Accumulation/NLP/pic/update.png)

5. 池化的作用

   卷积虽然可以大范围减少输出尺寸（特征数），但是依然较难计算并且很容易过拟合，所以利用图片的静态特性通过池化的方式进一步减少尺寸。

6. 

